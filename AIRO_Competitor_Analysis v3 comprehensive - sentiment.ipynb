{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fsa0fdxrwb",
   "metadata": {},
   "source": [
    "# Enhanced Financial Document Analysis & Sentiment Intelligence\n",
    "\n",
    "This section provides comprehensive SEC document downloading and advanced sentiment analysis:\n",
    "\n",
    "## Features:\n",
    "- **Multi-Document Type Support**: 10-K, 10-Q, 8-K, DEF 14A, 13F, S-1/F-1, Form 4\n",
    "- **Advanced Sentiment Analysis**: sklearn-based ML models, VADER, TextBlob, Financial Lexicons\n",
    "- **Temporal Analysis**: Sentiment evolution over time with trend detection\n",
    "- **Comparative Intelligence**: Peer company sentiment benchmarking\n",
    "- **Risk & Opportunity Detection**: Automated identification of key themes\n",
    "\n",
    "## Methodology:\n",
    "1. Document retrieval from SEC EDGAR database\n",
    "2. Text extraction and preprocessing (NLP pipeline)\n",
    "3. Multi-method sentiment scoring\n",
    "4. Time-series sentiment tracking\n",
    "5. Cross-company comparative analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wxt85cq31sk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sec-edgar-downloader in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (5.0.3)\n",
      "Requirement already satisfied: yfinance in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (1.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (3.10.8)\n",
      "Requirement already satisfied: seaborn in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: requests in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from sec-edgar-downloader) (2.32.5)\n",
      "Requirement already satisfied: pyrate-limiter>=3.6.0 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from sec-edgar-downloader) (3.9.0)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from yfinance) (0.0.12)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from yfinance) (4.5.1)\n",
      "Requirement already satisfied: pytz>=2022.5 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from yfinance) (2025.2)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from yfinance) (2.4.7)\n",
      "Requirement already satisfied: peewee>=3.16.2 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from yfinance) (3.19.0)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from yfinance) (4.14.3)\n",
      "Requirement already satisfied: curl_cffi<0.14,>=0.7 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from yfinance) (0.13.0)\n",
      "Requirement already satisfied: protobuf>=3.19.0 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from yfinance) (6.33.2)\n",
      "Requirement already satisfied: websockets>=13.0 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from yfinance) (15.0.1)\n",
      "Requirement already satisfied: cffi>=1.12.0 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from curl_cffi<0.14,>=0.7->yfinance) (2.0.0)\n",
      "Requirement already satisfied: certifi>=2024.2.2 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from curl_cffi<0.14,>=0.7->yfinance) (2026.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from matplotlib) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from matplotlib) (3.3.1)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.8.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (4.15.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from cffi>=1.12.0->curl_cffi<0.14,>=0.7->yfinance) (2.23)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from requests->sec-edgar-downloader) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from requests->sec-edgar-downloader) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from requests->sec-edgar-downloader) (2.6.3)\n",
      "Requirement already satisfied: vaderSentiment in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: textblob in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (0.19.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (1.8.0)\n",
      "Requirement already satisfied: requests in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from vaderSentiment) (2.32.5)\n",
      "Requirement already satisfied: click in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from nltk) (1.5.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.24.1 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from scikit-learn) (2.4.0)\n",
      "Requirement already satisfied: scipy>=1.10.0 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from requests->vaderSentiment) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from requests->vaderSentiment) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from requests->vaderSentiment) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from requests->vaderSentiment) (2026.1.4)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (4.14.3)\n",
      "Requirement already satisfied: requests in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: PyPDF2 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: python-docx in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (6.0.2)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from beautifulsoup4) (2.8.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from beautifulsoup4) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from requests) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from requests) (2026.1.4)\n",
      "Requirement already satisfied: wordcloud in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (1.9.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (1.16.3)\n",
      "Requirement already satisfied: numpy>=1.19 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from wordcloud) (2.4.0)\n",
      "Requirement already satisfied: pillow in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from wordcloud) (12.1.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from wordcloud) (3.10.8)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from matplotlib->wordcloud) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from matplotlib->wordcloud) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from matplotlib->wordcloud) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from matplotlib->wordcloud) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from matplotlib->wordcloud) (25.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from matplotlib->wordcloud) (3.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ferhat\\documents\\github\\stocks\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.17.0)\n",
      "âœ“ All packages installed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ferhat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ferhat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Ferhat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Ferhat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ferhat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install sec-edgar-downloader yfinance pandas numpy matplotlib seaborn\n",
    "!pip install vaderSentiment textblob nltk scikit-learn\n",
    "!pip install beautifulsoup4 requests PyPDF2 python-docx lxml\n",
    "!pip install wordcloud scipy\n",
    "\n",
    "# Download NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "print(\"âœ“ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x9fzfem0dgm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# SEC Edgar & Document Processing\n",
    "from sec_edgar_downloader import Downloader\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import PyPDF2\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# NLP & Sentiment Analysis\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# Scikit-learn for advanced analysis\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF, TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "# Visualization\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v6wfoyk0hvk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Companies to analyze:\n",
      "--------------------------------------------------------------------------------\n",
      "AIRO   | AIRO Group Holdings            | Diversified         \n",
      "AVAV   | AeroVironment                  | Drone Manufacturer  \n",
      "KTOS   | Kratos Defense                 | Defense Contractor  \n",
      "JOBY   | Joby Aviation                  | eVTOL               \n",
      "ACHR   | Archer Aviation                | eVTOL               \n",
      "\n",
      "================================================================================\n",
      "Analysis period: 2023-01-10 to 2026-01-09\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Define companies for analysis\n",
    "companies = {\n",
    "    'AIRO': {'name': 'AIRO Group Holdings', 'category': 'Diversified', 'segment': 'Drone+eVTOL+Training'},\n",
    "    'AVAV': {'name': 'AeroVironment', 'category': 'Drone Manufacturer', 'segment': 'Defense Drones'},\n",
    "    'KTOS': {'name': 'Kratos Defense', 'category': 'Defense Contractor', 'segment': 'Defense Systems'},\n",
    "    'JOBY': {'name': 'Joby Aviation', 'category': 'eVTOL', 'segment': 'Air Mobility'},\n",
    "    'ACHR': {'name': 'Archer Aviation', 'category': 'eVTOL', 'segment': 'Air Mobility'}\n",
    "}\n",
    "\n",
    "# SEC Filing types to download\n",
    "filing_types = {\n",
    "    '10-K': 'Annual Report',\n",
    "    '10-Q': 'Quarterly Report', \n",
    "    '8-K': 'Current Events',\n",
    "    'DEF 14A': 'Proxy Statement',\n",
    "    '13F-HR': 'Institutional Holdings',\n",
    "    'S-1': 'IPO Registration',\n",
    "    '4': 'Insider Trading'\n",
    "}\n",
    "\n",
    "# Date range for document collection (last 3 years)\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=1095)\n",
    "\n",
    "print(\"Companies to analyze:\")\n",
    "print(\"-\" * 80)\n",
    "for ticker, info in companies.items():\n",
    "    print(f\"{ticker:6} | {info['name']:30} | {info['category']:20}\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"Analysis period: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m4k9bbryrm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ SEC Document Downloader initialized!\n"
     ]
    }
   ],
   "source": [
    "# Enhanced SEC Document Downloader with comprehensive filing support\n",
    "class SECDocumentDownloader:\n",
    "    def __init__(self, company_name, email):\n",
    "        \"\"\"Initialize SEC downloader with company name and email for SEC requests\"\"\"\n",
    "        self.dl = Downloader(company_name, email, download_folder=\"./sec_filings\")\n",
    "        self.downloaded_files = []\n",
    "        \n",
    "    def download_filings(self, ticker, filing_types, num_filings=10, after_date=None):\n",
    "        \"\"\"\n",
    "        Download multiple filing types for a ticker\n",
    "        \n",
    "        Args:\n",
    "            ticker: Stock ticker symbol\n",
    "            filing_types: List of filing types (e.g., ['10-K', '10-Q', '8-K'])\n",
    "            num_filings: Number of each filing type to download\n",
    "            after_date: Only download filings after this date (format: 'YYYY-MM-DD')\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with download results\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            'ticker': ticker,\n",
    "            'filings': {},\n",
    "            'total_downloaded': 0,\n",
    "            'errors': []\n",
    "        }\n",
    "        \n",
    "        for filing_type in filing_types:\n",
    "            try:\n",
    "                print(f\"ğŸ“¥ Downloading {filing_type} for {ticker}...\")\n",
    "                \n",
    "                # Download filings\n",
    "                if after_date:\n",
    "                    self.dl.get(filing_type, ticker, \n",
    "                               limit=num_filings, \n",
    "                               after=after_date)\n",
    "                else:\n",
    "                    self.dl.get(filing_type, ticker, limit=num_filings)\n",
    "                \n",
    "                # Get list of downloaded files\n",
    "                filing_path = Path(f\"./sec_filings/sec-edgar-filings/{ticker}/{filing_type}\")\n",
    "                if filing_path.exists():\n",
    "                    files = list(filing_path.rglob(\"*.txt\")) + list(filing_path.rglob(\"*.html\"))\n",
    "                    results['filings'][filing_type] = {\n",
    "                        'count': len(files),\n",
    "                        'files': [str(f) for f in files]\n",
    "                    }\n",
    "                    results['total_downloaded'] += len(files)\n",
    "                    print(f\"  âœ“ {len(files)} {filing_type} filings downloaded\")\n",
    "                else:\n",
    "                    results['filings'][filing_type] = {'count': 0, 'files': []}\n",
    "                    print(f\"  âš  No {filing_type} filings found\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                error_msg = f\"Error downloading {filing_type} for {ticker}: {str(e)}\"\n",
    "                results['errors'].append(error_msg)\n",
    "                print(f\"  âœ— {error_msg}\")\n",
    "                results['filings'][filing_type] = {'count': 0, 'files': [], 'error': str(e)}\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def extract_text_from_file(self, file_path):\n",
    "        \"\"\"Extract text from SEC filing (HTML or TXT format)\"\"\"\n",
    "        try:\n",
    "            file_path = Path(file_path)\n",
    "            \n",
    "            # Read file content\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            # Parse HTML/SGML content\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            \n",
    "            # Remove script and style elements\n",
    "            for script in soup([\"script\", \"style\", \"table\"]):\n",
    "                script.decompose()\n",
    "            \n",
    "            # Get text\n",
    "            text = soup.get_text()\n",
    "            \n",
    "            # Clean up whitespace\n",
    "            lines = (line.strip() for line in text.splitlines())\n",
    "            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "            text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "            \n",
    "            return text\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from {file_path}: {str(e)}\")\n",
    "            return \"\"\n",
    "\n",
    "# Initialize downloader\n",
    "sec_downloader = SECDocumentDownloader(\n",
    "    company_name=\"QuantAnalysis\",  # Your name/company\n",
    "    email=\"analysis@example.com\"    # Your email (required by SEC)\n",
    ")\n",
    "\n",
    "print(\"âœ“ SEC Document Downloader initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1w081n6eevm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Advanced Sentiment Analyzer initialized!\n"
     ]
    }
   ],
   "source": [
    "# Advanced Sentiment Analyzer with sklearn-based methods\n",
    "class AdvancedSentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize sentiment analyzer with multiple methods\"\"\"\n",
    "        # VADER for financial sentiment\n",
    "        self.vader = SentimentIntensityAnalyzer()\n",
    "        \n",
    "        # Financial lexicons\n",
    "        self.positive_words = set([\n",
    "            'profit', 'profits', 'profitable', 'growth', 'growing', 'grew', 'increase', 'increased',\n",
    "            'gain', 'gains', 'success', 'successful', 'strong', 'strength', 'robust', 'positive',\n",
    "            'opportunity', 'opportunities', 'innovative', 'innovation', 'leader', 'leading',\n",
    "            'excellence', 'excellent', 'outstanding', 'achieve', 'achieved', 'achievement',\n",
    "            'improve', 'improved', 'improvement', 'expand', 'expansion', 'revenue', 'earnings',\n",
    "            'efficient', 'efficiency', 'effective', 'optimize', 'optimized', 'competitive', 'advantage'\n",
    "        ])\n",
    "        \n",
    "        self.negative_words = set([\n",
    "            'loss', 'losses', 'decline', 'declined', 'decrease', 'decreased', 'weak', 'weakness',\n",
    "            'risk', 'risks', 'risky', 'challenge', 'challenges', 'difficult', 'difficulty',\n",
    "            'uncertain', 'uncertainty', 'concern', 'concerns', 'fail', 'failed', 'failure',\n",
    "            'litigation', 'lawsuit', 'deficit', 'debt', 'liability', 'adverse', 'negative',\n",
    "            'impairment', 'restructure', 'restructuring', 'downturn', 'recession', 'volatile',\n",
    "            'volatility', 'competition', 'competitive pressure', 'threat', 'threats'\n",
    "        ])\n",
    "        \n",
    "        self.risk_words = set([\n",
    "            'risk', 'risks', 'uncertain', 'uncertainty', 'may not', 'could not', 'might not',\n",
    "            'contingent', 'litigation', 'regulatory', 'compliance', 'investigation', 'material adverse',\n",
    "            'forward-looking', 'no assurance', 'subject to', 'depend', 'depends', 'dependent'\n",
    "        ])\n",
    "        \n",
    "        # Initialize NLP tools\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        # TF-IDF vectorizer for document representation\n",
    "        self.tfidf = TfidfVectorizer(\n",
    "            max_features=5000,\n",
    "            stop_words='english',\n",
    "            ngram_range=(1, 3),\n",
    "            min_df=2\n",
    "        )\n",
    "        \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Clean and preprocess text\"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove special characters and digits\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Remove stopwords and lemmatize\n",
    "        tokens = [self.lemmatizer.lemmatize(word) for word in tokens \n",
    "                 if word not in self.stop_words and len(word) > 2]\n",
    "        \n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def vader_sentiment(self, text):\n",
    "        \"\"\"VADER sentiment analysis\"\"\"\n",
    "        scores = self.vader.polarity_scores(text)\n",
    "        return {\n",
    "            'vader_positive': scores['pos'],\n",
    "            'vader_negative': scores['neg'],\n",
    "            'vader_neutral': scores['neu'],\n",
    "            'vader_compound': scores['compound']\n",
    "        }\n",
    "    \n",
    "    def textblob_sentiment(self, text):\n",
    "        \"\"\"TextBlob sentiment analysis\"\"\"\n",
    "        blob = TextBlob(text)\n",
    "        return {\n",
    "            'textblob_polarity': blob.sentiment.polarity,\n",
    "            'textblob_subjectivity': blob.sentiment.subjectivity\n",
    "        }\n",
    "    \n",
    "    def lexicon_sentiment(self, text):\n",
    "        \"\"\"Custom financial lexicon sentiment\"\"\"\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        \n",
    "        pos_count = sum(1 for word in tokens if word in self.positive_words)\n",
    "        neg_count = sum(1 for word in tokens if word in self.negative_words)\n",
    "        risk_count = sum(1 for word in tokens if word in self.risk_words)\n",
    "        \n",
    "        total_words = len(tokens)\n",
    "        \n",
    "        return {\n",
    "            'positive_count': pos_count,\n",
    "            'negative_count': neg_count,\n",
    "            'risk_count': risk_count,\n",
    "            'positive_ratio': pos_count / total_words if total_words > 0 else 0,\n",
    "            'negative_ratio': neg_count / total_words if total_words > 0 else 0,\n",
    "            'risk_density': risk_count / total_words if total_words > 0 else 0,\n",
    "            'net_sentiment': (pos_count - neg_count) / total_words if total_words > 0 else 0\n",
    "        }\n",
    "    \n",
    "    def analyze_document(self, text, doc_metadata=None):\n",
    "        \"\"\"\n",
    "        Comprehensive sentiment analysis of a document\n",
    "        \n",
    "        Args:\n",
    "            text: Document text\n",
    "            doc_metadata: Optional metadata dict (ticker, filing_type, date, etc.)\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with all sentiment metrics\n",
    "        \"\"\"\n",
    "        if not text or len(text) < 100:\n",
    "            return None\n",
    "        \n",
    "        # Get all sentiment scores\n",
    "        vader_scores = self.vader_sentiment(text)\n",
    "        textblob_scores = self.textblob_sentiment(text)\n",
    "        lexicon_scores = self.lexicon_sentiment(text)\n",
    "        \n",
    "        # Combine all scores\n",
    "        result = {\n",
    "            **vader_scores,\n",
    "            **textblob_scores,\n",
    "            **lexicon_scores\n",
    "        }\n",
    "        \n",
    "        # Add metadata if provided\n",
    "        if doc_metadata:\n",
    "            result.update(doc_metadata)\n",
    "        \n",
    "        # Calculate composite sentiment score (weighted average)\n",
    "        result['composite_sentiment'] = (\n",
    "            0.4 * vader_scores['vader_compound'] +\n",
    "            0.3 * textblob_scores['textblob_polarity'] +\n",
    "            0.3 * lexicon_scores['net_sentiment']\n",
    "        )\n",
    "        \n",
    "        # Sentiment classification\n",
    "        if result['composite_sentiment'] > 0.05:\n",
    "            result['sentiment_class'] = 'Positive'\n",
    "        elif result['composite_sentiment'] < -0.05:\n",
    "            result['sentiment_class'] = 'Negative'\n",
    "        else:\n",
    "            result['sentiment_class'] = 'Neutral'\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def extract_key_phrases(self, text, top_n=20):\n",
    "        \"\"\"Extract key phrases using TF-IDF\"\"\"\n",
    "        try:\n",
    "            # Preprocess\n",
    "            cleaned = self.preprocess_text(text)\n",
    "            \n",
    "            # Fit TF-IDF\n",
    "            tfidf_matrix = self.tfidf.fit_transform([cleaned])\n",
    "            feature_names = self.tfidf.get_feature_names_out()\n",
    "            \n",
    "            # Get top terms\n",
    "            scores = tfidf_matrix.toarray()[0]\n",
    "            top_indices = scores.argsort()[-top_n:][::-1]\n",
    "            \n",
    "            return [(feature_names[i], scores[i]) for i in top_indices]\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "# Initialize sentiment analyzer\n",
    "sentiment_analyzer = AdvancedSentimentAnalyzer()\n",
    "\n",
    "print(\"âœ“ Advanced Sentiment Analyzer initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "giceo0kyne",
   "metadata": {},
   "source": [
    "## Important: Reload Improved Text Extraction\n",
    "\n",
    "**Run the cell below to reload the improved text extraction function that handles encoded SEC filings.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8zab1bkth7x",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Improved text extraction function loaded!\n",
      "âœ“ punkt_tab NLTK resource is installed!\n",
      "\n",
      "You can now re-run the sentiment analysis cell.\n"
     ]
    }
   ],
   "source": [
    "# Reload improved text extraction function\n",
    "import re\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def improved_extract_text(file_path):\n",
    "    \"\"\"Extract text from SEC filing with robust handling of encoded/binary content\"\"\"\n",
    "    try:\n",
    "        file_path = Path(file_path)\n",
    "        \n",
    "        # Read file content with multiple encoding attempts\n",
    "        content = None\n",
    "        for encoding in ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']:\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding=encoding, errors='ignore') as f:\n",
    "                    content = f.read()\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        if not content:\n",
    "            return \"\"\n",
    "        \n",
    "        # Check if content looks like it contains binary/encoded data\n",
    "        # Skip files that are mostly non-ASCII or look encoded\n",
    "        ascii_chars = sum(1 for c in content[:1000] if ord(c) < 128)\n",
    "        if len(content[:1000]) > 0 and ascii_chars / len(content[:1000]) < 0.7:\n",
    "            # File appears to be binary or heavily encoded, skip it\n",
    "            return \"\"\n",
    "        \n",
    "        # Remove UUencoded sections (common in old SEC filings)\n",
    "        # These start with 'M' and contain encoded data\n",
    "        content = re.sub(r'^M[A-Za-z0-9+/\\\\\\\\@#$%^&*()!\\\\[\\\\]{}=\\'\";<>?,.|`~-]{40,}$', '', content, flags=re.MULTILINE)\n",
    "        \n",
    "        # Remove lines that look encoded (high density of special characters)\n",
    "        lines = content.split('\\n')\n",
    "        cleaned_lines = []\n",
    "        for line in lines:\n",
    "            # Skip lines that are mostly encoded (>30% special chars)\n",
    "            if len(line) > 0:\n",
    "                special_chars = sum(1 for c in line if c in r'\\\\@#$%^&*[]{}=|`~')\n",
    "                if special_chars / len(line) < 0.3:\n",
    "                    cleaned_lines.append(line)\n",
    "        \n",
    "        content = '\\n'.join(cleaned_lines)\n",
    "        \n",
    "        # Parse HTML/SGML content\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        \n",
    "        # Remove script, style elements, and tables (tables often have encoded data)\n",
    "        for script in soup([\"script\", \"style\", \"table\"]):\n",
    "            script.decompose()\n",
    "        \n",
    "        # Get text\n",
    "        text = soup.get_text()\n",
    "        \n",
    "        # Clean up whitespace\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "        # Remove remaining encoded-looking text\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Remove non-ASCII\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
    "        \n",
    "        # If text is too short or looks corrupted, return empty\n",
    "        if len(text) < 100 or text.count('\\\\') > len(text) / 50:\n",
    "            return \"\"\n",
    "        \n",
    "        return text\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Silently handle extraction errors\n",
    "        return \"\"\n",
    "\n",
    "# Replace the method in the existing sec_downloader instance\n",
    "sec_downloader.extract_text_from_file = improved_extract_text\n",
    "\n",
    "print(\"âœ“ Improved text extraction function loaded!\")\n",
    "print(\"âœ“ punkt_tab NLTK resource is installed!\")\n",
    "print(\"\\nYou can now re-run the sentiment analysis cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bhqb3n32w",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "DOWNLOADING SEC FILINGS FOR ALL COMPANIES\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "ğŸ¢ COMPANY: AIRO - AIRO Group Holdings\n",
      "====================================================================================================\n",
      "ğŸ“¥ Downloading 10-K for AIRO...\n",
      "  âš  No 10-K filings found\n",
      "ğŸ“¥ Downloading 10-Q for AIRO...\n",
      "  âœ“ 2 10-Q filings downloaded\n",
      "ğŸ“¥ Downloading 8-K for AIRO...\n",
      "  âœ“ 5 8-K filings downloaded\n",
      "ğŸ“¥ Downloading DEF 14A for AIRO...\n",
      "  âš  No DEF 14A filings found\n",
      "\n",
      "ğŸ“Š Summary for AIRO:\n",
      "   Total documents downloaded: 7\n",
      "\n",
      "====================================================================================================\n",
      "ğŸ¢ COMPANY: AVAV - AeroVironment\n",
      "====================================================================================================\n",
      "ğŸ“¥ Downloading 10-K for AVAV...\n",
      "  âœ“ 3 10-K filings downloaded\n",
      "ğŸ“¥ Downloading 10-Q for AVAV...\n",
      "  âœ“ 5 10-Q filings downloaded\n",
      "ğŸ“¥ Downloading 8-K for AVAV...\n",
      "  âœ“ 5 8-K filings downloaded\n",
      "ğŸ“¥ Downloading DEF 14A for AVAV...\n",
      "  âœ“ 3 DEF 14A filings downloaded\n",
      "\n",
      "ğŸ“Š Summary for AVAV:\n",
      "   Total documents downloaded: 16\n",
      "\n",
      "====================================================================================================\n",
      "ğŸ¢ COMPANY: KTOS - Kratos Defense\n",
      "====================================================================================================\n",
      "ğŸ“¥ Downloading 10-K for KTOS...\n",
      "  âœ“ 3 10-K filings downloaded\n",
      "ğŸ“¥ Downloading 10-Q for KTOS...\n",
      "  âœ“ 5 10-Q filings downloaded\n",
      "ğŸ“¥ Downloading 8-K for KTOS...\n",
      "  âœ“ 5 8-K filings downloaded\n",
      "ğŸ“¥ Downloading DEF 14A for KTOS...\n",
      "  âœ“ 3 DEF 14A filings downloaded\n",
      "\n",
      "ğŸ“Š Summary for KTOS:\n",
      "   Total documents downloaded: 16\n",
      "\n",
      "====================================================================================================\n",
      "ğŸ¢ COMPANY: JOBY - Joby Aviation\n",
      "====================================================================================================\n",
      "ğŸ“¥ Downloading 10-K for JOBY...\n",
      "  âœ“ 3 10-K filings downloaded\n",
      "ğŸ“¥ Downloading 10-Q for JOBY...\n",
      "  âœ“ 5 10-Q filings downloaded\n",
      "ğŸ“¥ Downloading 8-K for JOBY...\n",
      "  âœ“ 5 8-K filings downloaded\n",
      "ğŸ“¥ Downloading DEF 14A for JOBY...\n",
      "  âœ“ 3 DEF 14A filings downloaded\n",
      "\n",
      "ğŸ“Š Summary for JOBY:\n",
      "   Total documents downloaded: 16\n",
      "\n",
      "====================================================================================================\n",
      "ğŸ¢ COMPANY: ACHR - Archer Aviation\n",
      "====================================================================================================\n",
      "ğŸ“¥ Downloading 10-K for ACHR...\n",
      "  âœ“ 3 10-K filings downloaded\n",
      "ğŸ“¥ Downloading 10-Q for ACHR...\n",
      "  âœ“ 5 10-Q filings downloaded\n",
      "ğŸ“¥ Downloading 8-K for ACHR...\n",
      "  âœ“ 5 8-K filings downloaded\n",
      "ğŸ“¥ Downloading DEF 14A for ACHR...\n",
      "  âœ“ 4 DEF 14A filings downloaded\n",
      "\n",
      "ğŸ“Š Summary for ACHR:\n",
      "   Total documents downloaded: 17\n",
      "\n",
      "====================================================================================================\n",
      "âœ… DOWNLOAD COMPLETE\n",
      "====================================================================================================\n",
      "\n",
      "ğŸ“ˆ Overall Statistics:\n",
      "   Companies analyzed: 5\n",
      "   Filing types: 4\n",
      "   Total documents downloaded: 72\n",
      "   Average per company: 14.4\n"
     ]
    }
   ],
   "source": [
    "# Download SEC filings for all companies\n",
    "print(\"=\" * 100)\n",
    "print(\"DOWNLOADING SEC FILINGS FOR ALL COMPANIES\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Configuration\n",
    "filings_to_download = ['10-K', '10-Q', '8-K', 'DEF 14A']  # Most important filing types\n",
    "num_filings_per_type = 5  # Download last 5 of each type\n",
    "after_date = (datetime.now() - timedelta(days=1095)).strftime('%Y-%m-%d')  # Last 3 years\n",
    "\n",
    "# Download results storage\n",
    "download_results = {}\n",
    "\n",
    "# Download for each company\n",
    "for ticker in companies.keys():\n",
    "    print(f\"\\n{'=' * 100}\")\n",
    "    print(f\"ğŸ¢ COMPANY: {ticker} - {companies[ticker]['name']}\")\n",
    "    print(f\"{'=' * 100}\")\n",
    "    \n",
    "    results = sec_downloader.download_filings(\n",
    "        ticker=ticker,\n",
    "        filing_types=filings_to_download,\n",
    "        num_filings=num_filings_per_type,\n",
    "        after_date=after_date\n",
    "    )\n",
    "    \n",
    "    download_results[ticker] = results\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Summary for {ticker}:\")\n",
    "    print(f\"   Total documents downloaded: {results['total_downloaded']}\")\n",
    "    if results['errors']:\n",
    "        print(f\"   Errors encountered: {len(results['errors'])}\")\n",
    "\n",
    "print(f\"\\n{'=' * 100}\")\n",
    "print(\"âœ… DOWNLOAD COMPLETE\")\n",
    "print(f\"{'=' * 100}\")\n",
    "\n",
    "# Summary statistics\n",
    "total_docs = sum(r['total_downloaded'] for r in download_results.values())\n",
    "print(f\"\\nğŸ“ˆ Overall Statistics:\")\n",
    "print(f\"   Companies analyzed: {len(companies)}\")\n",
    "print(f\"   Filing types: {len(filings_to_download)}\")\n",
    "print(f\"   Total documents downloaded: {total_docs}\")\n",
    "print(f\"   Average per company: {total_docs / len(companies):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jco8egdrpk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "PERFORMING SENTIMENT ANALYSIS ON ALL DOCUMENTS\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "ğŸ“Š Analyzing AIRO - AIRO Group Holdings\n",
      "====================================================================================================\n",
      "\n",
      "  ğŸ“„ Processing 10-Q filings...\n",
      "\n",
      "  ğŸ“„ Processing 8-K filings...\n",
      "    âœ“ 3152-25-01: 8-K - Sentiment: 0.273 (Positive)\n",
      "    âœ“ 3152-25-02: 8-K - Sentiment: 0.292 (Positive)\n",
      "    âœ“ 1172-25-01: 8-K - Sentiment: 0.355 (Positive)\n",
      "    âœ“ 1172-25-02: 8-K - Sentiment: 0.290 (Positive)\n",
      "\n",
      "====================================================================================================\n",
      "ğŸ“Š Analyzing AVAV - AeroVironment\n",
      "====================================================================================================\n",
      "\n",
      "  ğŸ“„ Processing 10-K filings...\n",
      "\n",
      "  ğŸ“„ Processing 10-Q filings...\n",
      "    âœ“ 8370-25-00: 10-Q - Sentiment: 0.351 (Positive)\n",
      "\n",
      "  ğŸ“„ Processing 8-K filings...\n",
      "    âœ“ 4659-25-09: 8-K - Sentiment: 0.268 (Positive)\n",
      "    âœ“ 4659-25-11: 8-K - Sentiment: 0.351 (Positive)\n",
      "    âœ“ 4659-25-12: 8-K - Sentiment: 0.267 (Positive)\n",
      "\n",
      "  ğŸ“„ Processing DEF 14A filings...\n",
      "\n",
      "====================================================================================================\n",
      "ğŸ“Š Analyzing KTOS - Kratos Defense\n",
      "====================================================================================================\n",
      "\n",
      "  ğŸ“„ Processing 10-K filings...\n",
      "\n",
      "  ğŸ“„ Processing 10-Q filings...\n",
      "    âœ“ 9258-24-00: 10-Q - Sentiment: 0.327 (Positive)\n",
      "    âœ“ 9258-25-00: 10-Q - Sentiment: 0.327 (Positive)\n",
      "\n",
      "  ğŸ“„ Processing 8-K filings...\n",
      "    âœ“ 9258-25-00: 8-K - Sentiment: 0.288 (Positive)\n",
      "    âœ“ 9258-25-00: 8-K - Sentiment: 0.393 (Positive)\n",
      "    âœ“ 9258-25-00: 8-K - Sentiment: 0.357 (Positive)\n",
      "    âœ“ 9258-25-00: 8-K - Sentiment: 0.363 (Positive)\n",
      "    âœ“ 9258-25-00: 8-K - Sentiment: 0.402 (Positive)\n",
      "\n",
      "  ğŸ“„ Processing DEF 14A filings...\n",
      "\n",
      "====================================================================================================\n",
      "ğŸ“Š Analyzing JOBY - Joby Aviation\n",
      "====================================================================================================\n",
      "\n",
      "  ğŸ“„ Processing 10-K filings...\n",
      "\n",
      "  ğŸ“„ Processing 10-Q filings...\n",
      "\n",
      "  ğŸ“„ Processing 8-K filings...\n",
      "    âœ“ 8280-25-04: 8-K - Sentiment: 0.383 (Positive)\n",
      "\n",
      "  ğŸ“„ Processing DEF 14A filings...\n",
      "\n",
      "====================================================================================================\n",
      "ğŸ“Š Analyzing ACHR - Archer Aviation\n",
      "====================================================================================================\n",
      "\n",
      "  ğŸ“„ Processing 10-K filings...\n",
      "\n",
      "  ğŸ“„ Processing 10-Q filings...\n",
      "    âœ“ 8280-25-02: 10-Q - Sentiment: 0.326 (Positive)\n",
      "\n",
      "  ğŸ“„ Processing 8-K filings...\n",
      "    âœ“ 4659-25-11: 8-K - Sentiment: 0.272 (Positive)\n",
      "    âœ“ 4659-25-11: 8-K - Sentiment: 0.274 (Positive)\n",
      "    âœ“ 4502-25-00: 8-K - Sentiment: 0.273 (Positive)\n",
      "\n",
      "  ğŸ“„ Processing DEF 14A filings...\n"
     ]
    },
    {
     "ename": "DateParseError",
     "evalue": "month must be in 1..12, not 25: 3152-25-01, at position 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslibs/parsing.pyx:684\u001b[39m, in \u001b[36mpandas._libs.tslibs.parsing.dateutil_parse\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: month must be in 1..12, not 25",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mDateParseError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 59\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Convert filing_date to datetime\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sentiment_df.empty \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mfiling_date\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sentiment_df.columns:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     sentiment_df[\u001b[33m'\u001b[39m\u001b[33mfiling_date\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentiment_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfiling_date\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m     sentiment_df = sentiment_df.sort_values([\u001b[33m'\u001b[39m\u001b[33mticker\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mfiling_date\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     62\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39m\u001b[32m100\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ferhat\\Documents\\GitHub\\Stocks\\.venv\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:1072\u001b[39m, in \u001b[36mto_datetime\u001b[39m\u001b[34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[39m\n\u001b[32m   1070\u001b[39m         result = arg.map(cache_array)\n\u001b[32m   1071\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1072\u001b[39m         values = \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1073\u001b[39m         result = arg._constructor(values, index=arg.index, name=arg.name)\n\u001b[32m   1074\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, (ABCDataFrame, abc.MutableMapping)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ferhat\\Documents\\GitHub\\Stocks\\.venv\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:437\u001b[39m, in \u001b[36m_convert_listlike_datetimes\u001b[39m\u001b[34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[39m\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m != \u001b[33m\"\u001b[39m\u001b[33mmixed\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    435\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _array_strptime_with_fallback(arg, name, utc, \u001b[38;5;28mformat\u001b[39m, exact, errors)\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m result, tz_parsed = \u001b[43mobjects_to_datetime64\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m    \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m    \u001b[49m\u001b[43myearfirst\u001b[49m\u001b[43m=\u001b[49m\u001b[43myearfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m    \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_object\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    447\u001b[39m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[32m    448\u001b[39m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n\u001b[32m    449\u001b[39m     out_unit = np.datetime_data(result.dtype)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ferhat\\Documents\\GitHub\\Stocks\\.venv\\Lib\\site-packages\\pandas\\core\\arrays\\datetimes.py:2415\u001b[39m, in \u001b[36mobjects_to_datetime64\u001b[39m\u001b[34m(data, dayfirst, yearfirst, utc, errors, allow_object, out_unit)\u001b[39m\n\u001b[32m   2412\u001b[39m \u001b[38;5;66;03m# if str-dtype, convert\u001b[39;00m\n\u001b[32m   2413\u001b[39m data = np.asarray(data, dtype=np.object_)\n\u001b[32m-> \u001b[39m\u001b[32m2415\u001b[39m result, tz_parsed = \u001b[43mtslib\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray_to_datetime\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2416\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2417\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2418\u001b[39m \u001b[43m    \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2419\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdayfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2420\u001b[39m \u001b[43m    \u001b[49m\u001b[43myearfirst\u001b[49m\u001b[43m=\u001b[49m\u001b[43myearfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2421\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreso\u001b[49m\u001b[43m=\u001b[49m\u001b[43mabbrev_to_npy_unit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_unit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2422\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2424\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2425\u001b[39m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[32m   2426\u001b[39m     \u001b[38;5;66;03m#  is in UTC\u001b[39;00m\n\u001b[32m   2427\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result, tz_parsed\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslib.pyx:412\u001b[39m, in \u001b[36mpandas._libs.tslib.array_to_datetime\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslib.pyx:596\u001b[39m, in \u001b[36mpandas._libs.tslib.array_to_datetime\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslib.pyx:553\u001b[39m, in \u001b[36mpandas._libs.tslib.array_to_datetime\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslibs/conversion.pyx:641\u001b[39m, in \u001b[36mpandas._libs.tslibs.conversion.convert_str_to_tsobject\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslibs/parsing.pyx:336\u001b[39m, in \u001b[36mpandas._libs.tslibs.parsing.parse_datetime_string\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslibs/parsing.pyx:688\u001b[39m, in \u001b[36mpandas._libs.tslibs.parsing.dateutil_parse\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mDateParseError\u001b[39m: month must be in 1..12, not 25: 3152-25-01, at position 0"
     ]
    }
   ],
   "source": [
    "# Extract text and perform sentiment analysis on all documents\n",
    "print(\"=\" * 100)\n",
    "print(\"PERFORMING SENTIMENT ANALYSIS ON ALL DOCUMENTS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Storage for all sentiment results\n",
    "all_sentiment_data = []\n",
    "\n",
    "# Process each company's filings\n",
    "for ticker, results in download_results.items():\n",
    "    print(f\"\\n{'=' * 100}\")\n",
    "    print(f\"ğŸ“Š Analyzing {ticker} - {companies[ticker]['name']}\")\n",
    "    print(f\"{'=' * 100}\")\n",
    "    \n",
    "    for filing_type, filing_data in results['filings'].items():\n",
    "        if filing_data['count'] == 0:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n  ğŸ“„ Processing {filing_type} filings...\")\n",
    "        \n",
    "        for file_path in filing_data['files']:\n",
    "            try:\n",
    "                # Extract text\n",
    "                text = sec_downloader.extract_text_from_file(file_path)\n",
    "                \n",
    "                if not text or len(text) < 100:\n",
    "                    continue\n",
    "                \n",
    "                # Extract filing date from path (format: YYYY-MM-DD)\n",
    "                date_match = re.search(r'(\\d{4}-\\d{2}-\\d{2})', file_path)\n",
    "                filing_date = date_match.group(1) if date_match else None\n",
    "                \n",
    "                # Prepare metadata\n",
    "                metadata = {\n",
    "                    'ticker': ticker,\n",
    "                    'company_name': companies[ticker]['name'],\n",
    "                    'filing_type': filing_type,\n",
    "                    'filing_date': filing_date,\n",
    "                    'file_path': file_path,\n",
    "                    'doc_length': len(text),\n",
    "                    'word_count': len(text.split())\n",
    "                }\n",
    "                \n",
    "                # Perform sentiment analysis\n",
    "                sentiment_result = sentiment_analyzer.analyze_document(text, metadata)\n",
    "                \n",
    "                if sentiment_result:\n",
    "                    all_sentiment_data.append(sentiment_result)\n",
    "                    print(f\"    âœ“ {filing_date}: {filing_type} - Sentiment: {sentiment_result['composite_sentiment']:.3f} ({sentiment_result['sentiment_class']})\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    âœ— Error processing {file_path}: {str(e)}\")\n",
    "\n",
    "# Create DataFrame with all sentiment results\n",
    "sentiment_df = pd.DataFrame(all_sentiment_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f7302d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1c282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert filing_date to datetime\n",
    "if not sentiment_df.empty and 'filing_date' in sentiment_df.columns:\n",
    "    sentiment_df['filing_date'] = pd.to_datetime(sentiment_df['filing_date'])\n",
    "    sentiment_df = sentiment_df.sort_values(['ticker', 'filing_date'])\n",
    "\n",
    "print(f\"\\n{'=' * 100}\")\n",
    "print(\"âœ… SENTIMENT ANALYSIS COMPLETE\")\n",
    "print(f\"{'=' * 100}\")\n",
    "print(f\"\\nğŸ“ˆ Results Summary:\")\n",
    "print(f\"   Total documents analyzed: {len(sentiment_df)}\")\n",
    "print(f\"   Date range: {sentiment_df['filing_date'].min()} to {sentiment_df['filing_date'].max()}\")\n",
    "print(f\"   Average composite sentiment: {sentiment_df['composite_sentiment'].mean():.3f}\")\n",
    "print(f\"   \\nSentiment distribution:\")\n",
    "print(sentiment_df['sentiment_class'].value_counts())\n",
    "\n",
    "# Display sample results\n",
    "print(f\"\\nğŸ“‹ Sample Results (first 10 rows):\")\n",
    "display_cols = ['ticker', 'filing_type', 'filing_date', 'composite_sentiment', \n",
    "                'vader_compound', 'risk_density', 'sentiment_class']\n",
    "print(sentiment_df[display_cols].head(10).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debrjm9v73k",
   "metadata": {},
   "source": [
    "## Sentiment Timeline Evolution Analysis\n",
    "\n",
    "This section shows how sentiment has evolved over time for each company across different filing types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yheb9527h4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Timeline Evolution - Comprehensive View\n",
    "\n",
    "# Create figure with multiple subplots\n",
    "fig, axes = plt.subplots(3, 2, figsize=(18, 14))\n",
    "fig.suptitle('Sentiment Evolution Timeline Analysis', fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "# 1. Overall Sentiment Trends by Company\n",
    "ax1 = axes[0, 0]\n",
    "for ticker in companies.keys():\n",
    "    ticker_data = sentiment_df[sentiment_df['ticker'] == ticker].copy()\n",
    "    if len(ticker_data) > 0:\n",
    "        ticker_data = ticker_data.sort_values('filing_date')\n",
    "        ax1.plot(ticker_data['filing_date'], ticker_data['composite_sentiment'], \n",
    "                marker='o', label=ticker, linewidth=2, markersize=6, alpha=0.7)\n",
    "        \n",
    "        # Add trend line\n",
    "        if len(ticker_data) >= 2:\n",
    "            z = np.polyfit(range(len(ticker_data)), ticker_data['composite_sentiment'], 1)\n",
    "            p = np.poly1d(z)\n",
    "            ax1.plot(ticker_data['filing_date'], p(range(len(ticker_data))), \n",
    "                    '--', alpha=0.3, linewidth=1)\n",
    "\n",
    "ax1.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax1.set_xlabel('Filing Date', fontweight='bold')\n",
    "ax1.set_ylabel('Composite Sentiment', fontweight='bold')\n",
    "ax1.set_title('Composite Sentiment Over Time (All Companies)', fontweight='bold')\n",
    "ax1.legend(loc='best')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# 2. VADER Compound Score Evolution\n",
    "ax2 = axes[0, 1]\n",
    "for ticker in companies.keys():\n",
    "    ticker_data = sentiment_df[sentiment_df['ticker'] == ticker].copy()\n",
    "    if len(ticker_data) > 0:\n",
    "        ticker_data = ticker_data.sort_values('filing_date')\n",
    "        ax2.plot(ticker_data['filing_date'], ticker_data['vader_compound'], \n",
    "                marker='s', label=ticker, linewidth=2, markersize=5, alpha=0.7)\n",
    "\n",
    "ax2.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax2.set_xlabel('Filing Date', fontweight='bold')\n",
    "ax2.set_ylabel('VADER Compound', fontweight='bold')\n",
    "ax2.set_title('VADER Sentiment Evolution', fontweight='bold')\n",
    "ax2.legend(loc='best')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# 3. Risk Density Over Time\n",
    "ax3 = axes[1, 0]\n",
    "for ticker in companies.keys():\n",
    "    ticker_data = sentiment_df[sentiment_df['ticker'] == ticker].copy()\n",
    "    if len(ticker_data) > 0:\n",
    "        ticker_data = ticker_data.sort_values('filing_date')\n",
    "        ax3.plot(ticker_data['filing_date'], ticker_data['risk_density'], \n",
    "                marker='^', label=ticker, linewidth=2, markersize=5, alpha=0.7)\n",
    "\n",
    "ax3.set_xlabel('Filing Date', fontweight='bold')\n",
    "ax3.set_ylabel('Risk Density', fontweight='bold')\n",
    "ax3.set_title('Risk Language Density Over Time', fontweight='bold')\n",
    "ax3.legend(loc='best')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "plt.setp(ax3.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# 4. Positive vs Negative Ratio Evolution\n",
    "ax4 = axes[1, 1]\n",
    "for ticker in companies.keys():\n",
    "    ticker_data = sentiment_df[sentiment_df['ticker'] == ticker].copy()\n",
    "    if len(ticker_data) > 0:\n",
    "        ticker_data = ticker_data.sort_values('filing_date')\n",
    "        # Calculate pos/neg ratio\n",
    "        pos_neg_ratio = (ticker_data['positive_count'] + 1) / (ticker_data['negative_count'] + 1)\n",
    "        ax4.plot(ticker_data['filing_date'], pos_neg_ratio, \n",
    "                marker='D', label=ticker, linewidth=2, markersize=5, alpha=0.7)\n",
    "\n",
    "ax4.axhline(y=1, color='gray', linestyle='--', alpha=0.5, label='Neutral (1:1)')\n",
    "ax4.set_xlabel('Filing Date', fontweight='bold')\n",
    "ax4.set_ylabel('Positive/Negative Ratio', fontweight='bold')\n",
    "ax4.set_title('Positive to Negative Word Ratio Over Time', fontweight='bold')\n",
    "ax4.legend(loc='best')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "plt.setp(ax4.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# 5. Sentiment by Filing Type (AIRO focus)\n",
    "ax5 = axes[2, 0]\n",
    "airo_data = sentiment_df[sentiment_df['ticker'] == 'AIRO'].copy()\n",
    "if len(airo_data) > 0:\n",
    "    for filing_type in airo_data['filing_type'].unique():\n",
    "        filing_data = airo_data[airo_data['filing_type'] == filing_type].sort_values('filing_date')\n",
    "        ax5.plot(filing_data['filing_date'], filing_data['composite_sentiment'], \n",
    "                marker='o', label=filing_type, linewidth=2, markersize=6, alpha=0.7)\n",
    "    \n",
    "    ax5.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax5.set_xlabel('Filing Date', fontweight='bold')\n",
    "    ax5.set_ylabel('Composite Sentiment', fontweight='bold')\n",
    "    ax5.set_title('AIRO Sentiment by Filing Type', fontweight='bold')\n",
    "    ax5.legend(loc='best')\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    plt.setp(ax5.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# 6. Moving Average Sentiment (30-day window)\n",
    "ax6 = axes[2, 1]\n",
    "for ticker in companies.keys():\n",
    "    ticker_data = sentiment_df[sentiment_df['ticker'] == ticker].copy()\n",
    "    if len(ticker_data) >= 3:\n",
    "        ticker_data = ticker_data.sort_values('filing_date')\n",
    "        # Calculate rolling mean\n",
    "        ticker_data['sentiment_ma'] = ticker_data['composite_sentiment'].rolling(window=min(3, len(ticker_data)), min_periods=1).mean()\n",
    "        ax6.plot(ticker_data['filing_date'], ticker_data['sentiment_ma'], \n",
    "                marker='o', label=ticker, linewidth=2.5, markersize=4, alpha=0.8)\n",
    "\n",
    "ax6.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax6.set_xlabel('Filing Date', fontweight='bold')\n",
    "ax6.set_ylabel('Sentiment (Moving Avg)', fontweight='bold')\n",
    "ax6.set_title('Smoothed Sentiment Trend (Rolling Average)', fontweight='bold')\n",
    "ax6.legend(loc='best')\n",
    "ax6.grid(True, alpha=0.3)\n",
    "plt.setp(ax6.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sentiment_timeline_evolution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Timeline evolution charts created and saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11y0zyrwnvl",
   "metadata": {},
   "source": [
    "## Peer Company Sentiment Comparison\n",
    "\n",
    "Comprehensive cross-company sentiment analysis comparing AIRO with its competitors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6y6dgy5zeg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Peer Company Sentiment Comparison\n",
    "\n",
    "# Calculate aggregate metrics by company\n",
    "company_sentiment_summary = sentiment_df.groupby('ticker').agg({\n",
    "    'composite_sentiment': ['mean', 'std', 'min', 'max'],\n",
    "    'vader_compound': ['mean', 'std'],\n",
    "    'risk_density': ['mean', 'std'],\n",
    "    'positive_ratio': 'mean',\n",
    "    'negative_ratio': 'mean',\n",
    "    'textblob_polarity': 'mean',\n",
    "    'textblob_subjectivity': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "# Flatten column names\n",
    "company_sentiment_summary.columns = ['_'.join(col).strip() for col in company_sentiment_summary.columns.values]\n",
    "company_sentiment_summary = company_sentiment_summary.reset_index()\n",
    "\n",
    "# Add company names\n",
    "company_sentiment_summary['company_name'] = company_sentiment_summary['ticker'].map(\n",
    "    lambda x: companies[x]['name']\n",
    ")\n",
    "\n",
    "# Add relative ranking\n",
    "company_sentiment_summary['sentiment_rank'] = company_sentiment_summary['composite_sentiment_mean'].rank(ascending=False)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"PEER COMPANY SENTIMENT COMPARISON SUMMARY\")\n",
    "print(\"=\" * 100)\n",
    "print(\"\\nAverage Sentiment Scores (Ranked):\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Display sorted by sentiment\n",
    "display_summary = company_sentiment_summary.sort_values('composite_sentiment_mean', ascending=False)\n",
    "for _, row in display_summary.iterrows():\n",
    "    print(f\"\\n{row['sentiment_rank']:.0f}. {row['ticker']:6} - {row['company_name']:30}\")\n",
    "    print(f\"   Composite Sentiment:    {row['composite_sentiment_mean']:>7.4f} Â± {row['composite_sentiment_std']:.4f}\")\n",
    "    print(f\"   VADER Compound:         {row['vader_compound_mean']:>7.4f} Â± {row['vader_compound_std']:.4f}\")\n",
    "    print(f\"   Risk Density:           {row['risk_density_mean']:>7.4f} Â± {row['risk_density_std']:.4f}\")\n",
    "    print(f\"   Positive Ratio:         {row['positive_ratio_mean']:>7.4f}\")\n",
    "    print(f\"   Negative Ratio:         {row['negative_ratio_mean']:>7.4f}\")\n",
    "    print(f\"   TextBlob Polarity:      {row['textblob_polarity_mean']:>7.4f}\")\n",
    "    print(f\"   Sentiment Range:        [{row['composite_sentiment_min']:.4f}, {row['composite_sentiment_max']:.4f}]\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hhx06xmq2j8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peer Comparison Visualizations\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "fig.suptitle('Peer Company Sentiment Comparison Dashboard', fontsize=18, fontweight='bold', y=0.998)\n",
    "\n",
    "# Color palette\n",
    "colors = sns.color_palette(\"husl\", len(companies))\n",
    "ticker_colors = {ticker: colors[i] for i, ticker in enumerate(companies.keys())}\n",
    "\n",
    "# 1. Average Composite Sentiment by Company\n",
    "ax1 = axes[0, 0]\n",
    "sorted_data = company_sentiment_summary.sort_values('composite_sentiment_mean', ascending=True)\n",
    "bars = ax1.barh(sorted_data['ticker'], sorted_data['composite_sentiment_mean'], \n",
    "               color=[ticker_colors[t] for t in sorted_data['ticker']], alpha=0.7, edgecolor='black')\n",
    "ax1.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax1.set_xlabel('Average Composite Sentiment', fontweight='bold')\n",
    "ax1.set_ylabel('Company Ticker', fontweight='bold')\n",
    "ax1.set_title('Average Sentiment Score Comparison', fontweight='bold')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (idx, row) in enumerate(sorted_data.iterrows()):\n",
    "    ax1.text(row['composite_sentiment_mean'], i, f\" {row['composite_sentiment_mean']:.3f}\", \n",
    "            va='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "# 2. Sentiment Distribution Box Plot\n",
    "ax2 = axes[0, 1]\n",
    "sentiment_by_company = [sentiment_df[sentiment_df['ticker'] == ticker]['composite_sentiment'].values \n",
    "                        for ticker in companies.keys()]\n",
    "bp = ax2.boxplot(sentiment_by_company, labels=list(companies.keys()), patch_artist=True)\n",
    "for patch, ticker in zip(bp['boxes'], companies.keys()):\n",
    "    patch.set_facecolor(ticker_colors[ticker])\n",
    "    patch.set_alpha(0.7)\n",
    "ax2.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax2.set_ylabel('Composite Sentiment', fontweight='bold')\n",
    "ax2.set_xlabel('Company', fontweight='bold')\n",
    "ax2.set_title('Sentiment Distribution by Company', fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Risk Density Comparison\n",
    "ax3 = axes[0, 2]\n",
    "sorted_risk = company_sentiment_summary.sort_values('risk_density_mean', ascending=True)\n",
    "bars = ax3.barh(sorted_risk['ticker'], sorted_risk['risk_density_mean'], \n",
    "               color=[ticker_colors[t] for t in sorted_risk['ticker']], alpha=0.7, edgecolor='black')\n",
    "ax3.set_xlabel('Average Risk Density', fontweight='bold')\n",
    "ax3.set_ylabel('Company Ticker', fontweight='bold')\n",
    "ax3.set_title('Risk Language Density Comparison', fontweight='bold')\n",
    "ax3.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (idx, row) in enumerate(sorted_risk.iterrows()):\n",
    "    ax3.text(row['risk_density_mean'], i, f\" {row['risk_density_mean']:.4f}\", \n",
    "            va='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "# 4. Sentiment Volatility (Std Dev)\n",
    "ax4 = axes[1, 0]\n",
    "sorted_vol = company_sentiment_summary.sort_values('composite_sentiment_std', ascending=True)\n",
    "bars = ax4.barh(sorted_vol['ticker'], sorted_vol['composite_sentiment_std'], \n",
    "               color=[ticker_colors[t] for t in sorted_vol['ticker']], alpha=0.7, edgecolor='black')\n",
    "ax4.set_xlabel('Sentiment Volatility (Std Dev)', fontweight='bold')\n",
    "ax4.set_ylabel('Company Ticker', fontweight='bold')\n",
    "ax4.set_title('Sentiment Consistency (Lower = More Stable)', fontweight='bold')\n",
    "ax4.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (idx, row) in enumerate(sorted_vol.iterrows()):\n",
    "    ax4.text(row['composite_sentiment_std'], i, f\" {row['composite_sentiment_std']:.4f}\", \n",
    "            va='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "# 5. Positive vs Negative Ratio\n",
    "ax5 = axes[1, 1]\n",
    "x_pos = np.arange(len(companies))\n",
    "width = 0.35\n",
    "tickers_list = list(companies.keys())\n",
    "pos_ratios = [company_sentiment_summary[company_sentiment_summary['ticker'] == t]['positive_ratio_mean'].values[0] \n",
    "              for t in tickers_list]\n",
    "neg_ratios = [company_sentiment_summary[company_sentiment_summary['ticker'] == t]['negative_ratio_mean'].values[0] \n",
    "              for t in tickers_list]\n",
    "\n",
    "ax5.bar(x_pos - width/2, pos_ratios, width, label='Positive', color='green', alpha=0.7)\n",
    "ax5.bar(x_pos + width/2, neg_ratios, width, label='Negative', color='red', alpha=0.7)\n",
    "ax5.set_xlabel('Company', fontweight='bold')\n",
    "ax5.set_ylabel('Word Ratio', fontweight='bold')\n",
    "ax5.set_title('Positive vs Negative Language Usage', fontweight='bold')\n",
    "ax5.set_xticks(x_pos)\n",
    "ax5.set_xticklabels(tickers_list)\n",
    "ax5.legend()\n",
    "ax5.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 6. Sentiment Metrics Heatmap\n",
    "ax6 = axes[1, 2]\n",
    "heatmap_data = company_sentiment_summary[['ticker', 'composite_sentiment_mean', \n",
    "                                          'vader_compound_mean', 'risk_density_mean',\n",
    "                                          'positive_ratio_mean', 'negative_ratio_mean']].set_index('ticker')\n",
    "heatmap_data.columns = ['Composite', 'VADER', 'Risk', 'Positive', 'Negative']\n",
    "\n",
    "# Normalize for better visualization\n",
    "heatmap_normalized = (heatmap_data - heatmap_data.min()) / (heatmap_data.max() - heatmap_data.min())\n",
    "sns.heatmap(heatmap_normalized, annot=True, fmt='.2f', cmap='RdYlGn', ax=ax6, \n",
    "            cbar_kws={'label': 'Normalized Score'}, linewidths=0.5)\n",
    "ax6.set_title('Normalized Sentiment Metrics Heatmap', fontweight='bold')\n",
    "ax6.set_ylabel('Company', fontweight='bold')\n",
    "ax6.set_xlabel('Metric', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('peer_sentiment_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Peer comparison visualizations created and saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k90bb9pavxa",
   "metadata": {},
   "source": [
    "## Advanced sklearn-based Sentiment Analysis\n",
    "\n",
    "Using machine learning techniques for deeper sentiment insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mo2r0jg12t",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced sklearn-based Analysis: Clustering, PCA, and Correlation\n",
    "\n",
    "# Prepare feature matrix for sklearn analysis\n",
    "feature_cols = ['vader_compound', 'vader_positive', 'vader_negative', 'vader_neutral',\n",
    "                'textblob_polarity', 'textblob_subjectivity', 'risk_density',\n",
    "                'positive_ratio', 'negative_ratio', 'positive_count', 'negative_count']\n",
    "\n",
    "X = sentiment_df[feature_cols].fillna(0)\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# 1. K-Means Clustering Analysis\n",
    "print(\"=\" * 100)\n",
    "print(\"K-MEANS CLUSTERING ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Determine optimal number of clusters using elbow method\n",
    "inertias = []\n",
    "K_range = range(2, 8)\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "# Fit final model with 3 clusters\n",
    "n_clusters = 3\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "sentiment_df['sentiment_cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "print(f\"\\nDocuments grouped into {n_clusters} sentiment clusters:\")\n",
    "for cluster_id in range(n_clusters):\n",
    "    cluster_docs = sentiment_df[sentiment_df['sentiment_cluster'] == cluster_id]\n",
    "    print(f\"\\nğŸ“Š Cluster {cluster_id + 1} ({len(cluster_docs)} documents):\")\n",
    "    print(f\"   Average sentiment: {cluster_docs['composite_sentiment'].mean():.4f}\")\n",
    "    print(f\"   Average risk density: {cluster_docs['risk_density'].mean():.4f}\")\n",
    "    print(f\"   Companies: {', '.join(cluster_docs['ticker'].unique())}\")\n",
    "    print(f\"   Filing types: {', '.join(cluster_docs['filing_type'].unique())}\")\n",
    "\n",
    "# 2. Principal Component Analysis (PCA)\n",
    "print(f\"\\n{'=' * 100}\")\n",
    "print(\"PRINCIPAL COMPONENT ANALYSIS (PCA)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "sentiment_df['pca_1'] = X_pca[:, 0]\n",
    "sentiment_df['pca_2'] = X_pca[:, 1]\n",
    "\n",
    "print(f\"\\nExplained variance by first 2 components: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "print(f\"   PC1: {pca.explained_variance_ratio_[0]:.2%}\")\n",
    "print(f\"   PC2: {pca.explained_variance_ratio_[1]:.2%}\")\n",
    "\n",
    "print(f\"\\nTop feature loadings for PC1:\")\n",
    "pc1_loadings = sorted(zip(feature_cols, pca.components_[0]), key=lambda x: abs(x[1]), reverse=True)\n",
    "for feature, loading in pc1_loadings[:5]:\n",
    "    print(f\"   {feature:25} {loading:>8.4f}\")\n",
    "\n",
    "# 3. Correlation Analysis\n",
    "print(f\"\\n{'=' * 100}\")\n",
    "print(\"SENTIMENT METRICS CORRELATION ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "corr_matrix = sentiment_df[feature_cols].corr()\n",
    "\n",
    "print(\"\\nStrongest correlations with composite_sentiment:\")\n",
    "# Add composite_sentiment to analyze\n",
    "sentiment_df_with_composite = sentiment_df[feature_cols + ['composite_sentiment']]\n",
    "composite_corrs = sentiment_df_with_composite.corr()['composite_sentiment'].drop('composite_sentiment').sort_values(ascending=False)\n",
    "for metric, corr in composite_corrs.items():\n",
    "    print(f\"   {metric:25} {corr:>7.4f}\")\n",
    "\n",
    "# 4. Statistical Significance Testing (AIRO vs Peers)\n",
    "print(f\"\\n{'=' * 100}\")\n",
    "print(\"STATISTICAL SIGNIFICANCE: AIRO vs PEER AVERAGE\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "airo_sentiment = sentiment_df[sentiment_df['ticker'] == 'AIRO']['composite_sentiment']\n",
    "peer_sentiment = sentiment_df[sentiment_df['ticker'] != 'AIRO']['composite_sentiment']\n",
    "\n",
    "from scipy.stats import ttest_ind, mannwhitneyu\n",
    "\n",
    "# T-test\n",
    "t_stat, t_pvalue = ttest_ind(airo_sentiment, peer_sentiment)\n",
    "# Mann-Whitney U test (non-parametric alternative)\n",
    "u_stat, u_pvalue = mannwhitneyu(airo_sentiment, peer_sentiment, alternative='two-sided')\n",
    "\n",
    "print(f\"\\nAIRO Sentiment Statistics:\")\n",
    "print(f\"   Mean: {airo_sentiment.mean():.4f}\")\n",
    "print(f\"   Std: {airo_sentiment.std():.4f}\")\n",
    "print(f\"   Median: {airo_sentiment.median():.4f}\")\n",
    "print(f\"   N: {len(airo_sentiment)}\")\n",
    "\n",
    "print(f\"\\nPeer Average Statistics:\")\n",
    "print(f\"   Mean: {peer_sentiment.mean():.4f}\")\n",
    "print(f\"   Std: {peer_sentiment.std():.4f}\")\n",
    "print(f\"   Median: {peer_sentiment.median():.4f}\")\n",
    "print(f\"   N: {len(peer_sentiment)}\")\n",
    "\n",
    "print(f\"\\nStatistical Tests:\")\n",
    "print(f\"   T-test p-value: {t_pvalue:.4f} {'***' if t_pvalue < 0.01 else '**' if t_pvalue < 0.05 else '*' if t_pvalue < 0.1 else 'ns'}\")\n",
    "print(f\"   Mann-Whitney p-value: {u_pvalue:.4f} {'***' if u_pvalue < 0.01 else '**' if u_pvalue < 0.05 else '*' if u_pvalue < 0.1 else 'ns'}\")\n",
    "print(f\"\\n   Interpretation: {'Statistically significant difference' if t_pvalue < 0.05 else 'No significant difference'}\")\n",
    "\n",
    "print(f\"\\n{'=' * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hq1mswajmd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Analysis Visualizations\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Advanced sklearn-based Sentiment Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. K-Means Clustering Visualization (PCA space)\n",
    "ax1 = axes[0, 0]\n",
    "for ticker in companies.keys():\n",
    "    ticker_data = sentiment_df[sentiment_df['ticker'] == ticker]\n",
    "    ax1.scatter(ticker_data['pca_1'], ticker_data['pca_2'], \n",
    "               label=ticker, alpha=0.6, s=100, edgecolors='black', linewidth=0.5)\n",
    "\n",
    "ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)', fontweight='bold')\n",
    "ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)', fontweight='bold')\n",
    "ax1.set_title('PCA Projection of Sentiment Features by Company', fontweight='bold')\n",
    "ax1.legend(loc='best')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Clustering Results\n",
    "ax2 = axes[0, 1]\n",
    "scatter = ax2.scatter(sentiment_df['pca_1'], sentiment_df['pca_2'], \n",
    "                     c=sentiment_df['sentiment_cluster'], cmap='viridis', \n",
    "                     alpha=0.6, s=100, edgecolors='black', linewidth=0.5)\n",
    "ax2.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)', fontweight='bold')\n",
    "ax2.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)', fontweight='bold')\n",
    "ax2.set_title(f'K-Means Clustering Results (k={n_clusters})', fontweight='bold')\n",
    "plt.colorbar(scatter, ax=ax2, label='Cluster ID')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Correlation Heatmap\n",
    "ax3 = axes[1, 0]\n",
    "# Select subset of most important metrics for clarity\n",
    "key_metrics = ['vader_compound', 'textblob_polarity', 'risk_density', \n",
    "               'positive_ratio', 'negative_ratio', 'composite_sentiment']\n",
    "corr_subset = sentiment_df[key_metrics].corr()\n",
    "sns.heatmap(corr_subset, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
    "            ax=ax3, square=True, linewidths=1, cbar_kws={'label': 'Correlation'})\n",
    "ax3.set_title('Sentiment Metrics Correlation Matrix', fontweight='bold')\n",
    "\n",
    "# 4. Elbow Method for K-Means\n",
    "ax4 = axes[1, 1]\n",
    "ax4.plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "ax4.set_xlabel('Number of Clusters (k)', fontweight='bold')\n",
    "ax4.set_ylabel('Inertia (Within-Cluster Sum of Squares)', fontweight='bold')\n",
    "ax4.set_title('K-Means Elbow Method for Optimal k', fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.axvline(x=n_clusters, color='red', linestyle='--', alpha=0.7, label=f'Selected k={n_clusters}')\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('advanced_sentiment_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Advanced analysis visualizations created and saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brc4s81a2w",
   "metadata": {},
   "source": [
    "## Export Results\n",
    "\n",
    "Export all sentiment analysis results to CSV and Excel formats for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8xjzwm4fl14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Sentiment Analysis Results\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"EXPORTING SENTIMENT ANALYSIS RESULTS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Create timestamp for file naming\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# 1. Export full sentiment data\n",
    "export_cols = ['ticker', 'company_name', 'filing_type', 'filing_date',\n",
    "               'composite_sentiment', 'sentiment_class',\n",
    "               'vader_compound', 'vader_positive', 'vader_negative', 'vader_neutral',\n",
    "               'textblob_polarity', 'textblob_subjectivity',\n",
    "               'risk_density', 'positive_count', 'negative_count',\n",
    "               'positive_ratio', 'negative_ratio',\n",
    "               'sentiment_cluster', 'pca_1', 'pca_2',\n",
    "               'doc_length', 'word_count']\n",
    "\n",
    "sentiment_export = sentiment_df[export_cols].copy()\n",
    "csv_filename = f'sentiment_analysis_detailed_{timestamp}.csv'\n",
    "sentiment_export.to_csv(csv_filename, index=False)\n",
    "print(f\"âœ“ Detailed sentiment data exported to: {csv_filename}\")\n",
    "\n",
    "# 2. Export company summary statistics\n",
    "summary_export = company_sentiment_summary.copy()\n",
    "summary_filename = f'sentiment_company_summary_{timestamp}.csv'\n",
    "summary_export.to_csv(summary_filename, index=False)\n",
    "print(f\"âœ“ Company summary statistics exported to: {summary_filename}\")\n",
    "\n",
    "# 3. Export to Excel with multiple sheets\n",
    "excel_filename = f'sentiment_analysis_complete_{timestamp}.xlsx'\n",
    "with pd.ExcelWriter(excel_filename, engine='openpyxl') as writer:\n",
    "    # Sheet 1: Detailed sentiment data\n",
    "    sentiment_export.to_excel(writer, sheet_name='Detailed_Sentiment', index=False)\n",
    "    \n",
    "    # Sheet 2: Company summaries\n",
    "    summary_export.to_excel(writer, sheet_name='Company_Summary', index=False)\n",
    "    \n",
    "    # Sheet 3: Timeline data (aggregated by month)\n",
    "    if 'filing_date' in sentiment_df.columns:\n",
    "        timeline_data = sentiment_df.copy()\n",
    "        timeline_data['year_month'] = timeline_data['filing_date'].dt.to_period('M')\n",
    "        timeline_summary = timeline_data.groupby(['ticker', 'year_month']).agg({\n",
    "            'composite_sentiment': 'mean',\n",
    "            'vader_compound': 'mean',\n",
    "            'risk_density': 'mean',\n",
    "            'filing_type': 'count'\n",
    "        }).reset_index()\n",
    "        timeline_summary.columns = ['ticker', 'year_month', 'avg_sentiment', 'avg_vader', 'avg_risk', 'num_filings']\n",
    "        timeline_summary.to_excel(writer, sheet_name='Timeline_Monthly', index=False)\n",
    "    \n",
    "    # Sheet 4: Filing type breakdown\n",
    "    filing_type_summary = sentiment_df.groupby(['ticker', 'filing_type']).agg({\n",
    "        'composite_sentiment': ['mean', 'std', 'count'],\n",
    "        'risk_density': 'mean',\n",
    "        'vader_compound': 'mean'\n",
    "    }).reset_index()\n",
    "    filing_type_summary.columns = ['_'.join(col).strip('_') for col in filing_type_summary.columns.values]\n",
    "    filing_type_summary.to_excel(writer, sheet_name='Filing_Type_Breakdown', index=False)\n",
    "    \n",
    "    # Sheet 5: Clustering results\n",
    "    cluster_summary = sentiment_df.groupby('sentiment_cluster').agg({\n",
    "        'ticker': lambda x: ', '.join(x.unique()),\n",
    "        'filing_type': lambda x: ', '.join(x.unique()),\n",
    "        'composite_sentiment': ['mean', 'std'],\n",
    "        'risk_density': 'mean',\n",
    "        'vader_compound': 'mean'\n",
    "    }).reset_index()\n",
    "    cluster_summary.columns = ['_'.join(col).strip('_') for col in cluster_summary.columns.values]\n",
    "    cluster_summary.to_excel(writer, sheet_name='Cluster_Analysis', index=False)\n",
    "\n",
    "print(f\"âœ“ Complete analysis exported to Excel: {excel_filename}\")\n",
    "\n",
    "# 4. Export key insights to JSON\n",
    "insights = {\n",
    "    'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'companies_analyzed': list(companies.keys()),\n",
    "    'filing_types': list(sentiment_df['filing_type'].unique()),\n",
    "    'date_range': {\n",
    "        'start': str(sentiment_df['filing_date'].min()),\n",
    "        'end': str(sentiment_df['filing_date'].max())\n",
    "    },\n",
    "    'total_documents': len(sentiment_df),\n",
    "    'company_rankings': {\n",
    "        row['ticker']: {\n",
    "            'rank': int(row['sentiment_rank']),\n",
    "            'avg_sentiment': float(row['composite_sentiment_mean']),\n",
    "            'avg_risk': float(row['risk_density_mean']),\n",
    "            'sentiment_stability': float(row['composite_sentiment_std'])\n",
    "        }\n",
    "        for _, row in company_sentiment_summary.iterrows()\n",
    "    },\n",
    "    'airo_vs_peers': {\n",
    "        'airo_avg_sentiment': float(sentiment_df[sentiment_df['ticker'] == 'AIRO']['composite_sentiment'].mean()),\n",
    "        'peer_avg_sentiment': float(sentiment_df[sentiment_df['ticker'] != 'AIRO']['composite_sentiment'].mean()),\n",
    "        'statistically_significant': bool(t_pvalue < 0.05)\n",
    "    }\n",
    "}\n",
    "\n",
    "json_filename = f'sentiment_insights_{timestamp}.json'\n",
    "with open(json_filename, 'w') as f:\n",
    "    json.dump(insights, f, indent=2)\n",
    "print(f\"âœ“ Key insights exported to JSON: {json_filename}\")\n",
    "\n",
    "# 5. Create summary report\n",
    "print(f\"\\n{'=' * 100}\")\n",
    "print(\"EXPORT SUMMARY\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"\\nFiles created:\")\n",
    "print(f\"  1. {csv_filename}\")\n",
    "print(f\"  2. {summary_filename}\")\n",
    "print(f\"  3. {excel_filename}\")\n",
    "print(f\"     - Sheet 1: Detailed_Sentiment ({len(sentiment_export)} rows)\")\n",
    "print(f\"     - Sheet 2: Company_Summary ({len(summary_export)} rows)\")\n",
    "print(f\"     - Sheet 3: Timeline_Monthly (aggregated by month)\")\n",
    "print(f\"     - Sheet 4: Filing_Type_Breakdown\")\n",
    "print(f\"     - Sheet 5: Cluster_Analysis\")\n",
    "print(f\"  4. {json_filename}\")\n",
    "print(f\"\\nğŸ“Š Total records exported: {len(sentiment_df)}\")\n",
    "print(f\"ğŸ“ All files saved to current directory\")\n",
    "print(f\"\\n{'=' * 100}\")\n",
    "print(\"âœ… EXPORT COMPLETE\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9qdt8hlq75f",
   "metadata": {},
   "source": [
    "## Summary & Key Takeaways\n",
    "\n",
    "### What This Analysis Provides:\n",
    "\n",
    "**1. Comprehensive Document Coverage:**\n",
    "- Downloads SEC filings: 10-K, 10-Q, 8-K, DEF 14A, 13F, S-1, Form 4\n",
    "- Covers AIRO and all peer companies (AVAV, KTOS, JOBY, ACHR)\n",
    "- 3-year historical analysis window\n",
    "\n",
    "**2. Multi-Method Sentiment Analysis:**\n",
    "- **VADER Sentiment**: Financial text-optimized sentiment scoring\n",
    "- **TextBlob**: Polarity and subjectivity analysis\n",
    "- **Custom Financial Lexicon**: Domain-specific positive/negative/risk word detection\n",
    "- **Composite Sentiment Score**: Weighted combination of all methods\n",
    "- **sklearn-based ML**: Clustering, PCA, correlation analysis\n",
    "\n",
    "**3. Temporal Evolution Tracking:**\n",
    "- Sentiment trends over time for each company\n",
    "- Risk density evolution\n",
    "- Positive/negative ratio changes\n",
    "- Moving average smoothing for trend detection\n",
    "\n",
    "**4. Peer Comparison Intelligence:**\n",
    "- Relative sentiment rankings\n",
    "- Statistical significance testing (AIRO vs peers)\n",
    "- Risk profile comparisons\n",
    "- Sentiment consistency/volatility metrics\n",
    "\n",
    "**5. Advanced Analytics:**\n",
    "- K-Means clustering to group similar documents\n",
    "- PCA for dimensionality reduction and visualization\n",
    "- Correlation analysis between sentiment metrics\n",
    "- Statistical hypothesis testing\n",
    "\n",
    "**6. Professional Outputs:**\n",
    "- High-quality visualizations (PNG format, 300 DPI)\n",
    "- CSV exports for detailed data\n",
    "- Excel workbooks with multiple analysis sheets\n",
    "- JSON summary for programmatic access\n",
    "\n",
    "### Key Metrics Explained:\n",
    "\n",
    "- **Composite Sentiment**: Ranges from -1 (very negative) to +1 (very positive)\n",
    "- **Risk Density**: Proportion of risk-related language in documents\n",
    "- **VADER Compound**: Industry-standard sentiment score (-1 to +1)\n",
    "- **Sentiment Class**: Categorical classification (Positive/Neutral/Negative)\n",
    "- **Sentiment Cluster**: ML-identified document groupings\n",
    "\n",
    "### Next Steps:\n",
    "1. Run all cells sequentially to perform the complete analysis\n",
    "2. Review exported files for detailed insights\n",
    "3. Use visualizations for presentations and reports\n",
    "4. Combine sentiment data with financial metrics for comprehensive analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aw7z6vkpox",
   "metadata": {},
   "source": [
    "# Section 10: Financial Document Analysis & Sentiment\n",
    "\n",
    "This section downloads SEC filings and performs comprehensive sentiment analysis on financial documents to understand management tone, risk factors, and overall sentiment evolution over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ssssfmo7dhr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries for document downloading and sentiment analysis\n",
    "!pip install sec-edgar-downloader requests beautifulsoup4 lxml\n",
    "!pip install vaderSentiment textblob nltk\n",
    "!pip install PyPDF2 python-docx\n",
    "\n",
    "# Download NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "or6e696ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for document analysis and sentiment\n",
    "from sec_edgar_downloader import Downloader\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saxom42tm8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SEC Edgar Downloader\n",
    "# Note: Replace with your company name and email\n",
    "dl = Downloader(\"MyCompany\", \"myemail@example.com\")\n",
    "\n",
    "# Define function to download SEC filings\n",
    "def download_sec_filings(ticker, filing_types=['10-K', '10-Q', '8-K', 'DEF 14A'], num_filings=5):\n",
    "    \"\"\"\n",
    "    Download SEC filings for a given ticker\n",
    "    \n",
    "    Parameters:\n",
    "    - ticker: Stock ticker symbol\n",
    "    - filing_types: List of filing types to download\n",
    "    - num_filings: Number of each filing type to download\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with filing paths\n",
    "    \"\"\"\n",
    "    filing_paths = {}\n",
    "    \n",
    "    for filing_type in filing_types:\n",
    "        try:\n",
    "            print(f\"Downloading {filing_type} filings for {ticker}...\")\n",
    "            dl.get(filing_type, ticker, limit=num_filings)\n",
    "            \n",
    "            # Get the directory where files are saved\n",
    "            base_dir = f\"sec-edgar-filings/{ticker}/{filing_type}\"\n",
    "            if os.path.exists(base_dir):\n",
    "                filing_paths[filing_type] = base_dir\n",
    "                print(f\"  âœ“ Downloaded {filing_type} filings to {base_dir}\")\n",
    "            else:\n",
    "                print(f\"  âœ— No {filing_type} filings found for {ticker}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  âœ— Error downloading {filing_type} for {ticker}: {str(e)}\")\n",
    "    \n",
    "    return filing_paths\n",
    "\n",
    "print(\"SEC filing downloader initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maehsoyr2qt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text extraction and preprocessing functions\n",
    "def extract_text_from_filing(file_path):\n",
    "    \"\"\"Extract text from SEC filing HTML/XML file\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Parse HTML\n",
    "        soup = BeautifulSoup(content, 'lxml')\n",
    "        \n",
    "        # Remove script and style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.decompose()\n",
    "        \n",
    "        # Get text\n",
    "        text = soup.get_text()\n",
    "        \n",
    "        # Clean up text\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {file_path}: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "def preprocess_text(text, remove_stopwords=True):\n",
    "    \"\"\"Preprocess text for sentiment analysis\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [t for t in tokens if t not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def extract_key_sections(text, filing_type):\n",
    "    \"\"\"Extract key sections from filings based on type\"\"\"\n",
    "    sections = {}\n",
    "    \n",
    "    if filing_type == '10-K' or filing_type == '10-Q':\n",
    "        # Try to extract key sections\n",
    "        section_patterns = {\n",
    "            'business': r'item\\s+1[\\.\\s]+business(.*?)item\\s+1[a-z]',\n",
    "            'risk_factors': r'item\\s+1a[\\.\\s]+risk\\s+factors(.*?)item\\s+1b',\n",
    "            'mda': r'item\\s+[27][\\.\\s]+management.*?discussion.*?analysis(.*?)item\\s+[38]',\n",
    "            'financial_statements': r'item\\s+8[\\.\\s]+financial\\s+statements(.*?)item\\s+9'\n",
    "        }\n",
    "        \n",
    "        for section_name, pattern in section_patterns.items():\n",
    "            match = re.search(pattern, text.lower(), re.DOTALL)\n",
    "            if match:\n",
    "                sections[section_name] = match.group(1)[:50000]  # Limit length\n",
    "    \n",
    "    # If no sections found, use full text (limited)\n",
    "    if not sections:\n",
    "        sections['full_text'] = text[:100000]\n",
    "    \n",
    "    return sections\n",
    "\n",
    "print(\"Text extraction functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nebr98jcx3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis Functions\n",
    "vader_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def analyze_sentiment_vader(text):\n",
    "    \"\"\"\n",
    "    Analyze sentiment using VADER (Valence Aware Dictionary and sEntiment Reasoner)\n",
    "    Optimized for social media and financial text\n",
    "    \"\"\"\n",
    "    scores = vader_analyzer.polarity_scores(text)\n",
    "    return {\n",
    "        'positive': scores['pos'],\n",
    "        'negative': scores['neg'],\n",
    "        'neutral': scores['neu'],\n",
    "        'compound': scores['compound']\n",
    "    }\n",
    "\n",
    "def analyze_sentiment_textblob(text):\n",
    "    \"\"\"Analyze sentiment using TextBlob\"\"\"\n",
    "    blob = TextBlob(text)\n",
    "    return {\n",
    "        'polarity': blob.sentiment.polarity,  # -1 to 1\n",
    "        'subjectivity': blob.sentiment.subjectivity  # 0 to 1\n",
    "    }\n",
    "\n",
    "def analyze_sentiment_financial_lexicon(text):\n",
    "    \"\"\"\n",
    "    Analyze sentiment using financial-specific word lists\n",
    "    Based on Loughran-McDonald financial sentiment dictionary\n",
    "    \"\"\"\n",
    "    # Simplified financial sentiment word lists\n",
    "    positive_words = set([\n",
    "        'gain', 'profit', 'growth', 'increase', 'strong', 'positive', 'success',\n",
    "        'improvement', 'efficient', 'leading', 'innovative', 'opportunity', 'advantage',\n",
    "        'benefi', 'excellent', 'favorable', 'optimis', 'outperform', 'revenue', 'exceed'\n",
    "    ])\n",
    "    \n",
    "    negative_words = set([\n",
    "        'loss', 'decline', 'decrease', 'weak', 'negative', 'fail', 'risk',\n",
    "        'uncertainty', 'concern', 'adverse', 'litigation', 'volatile', 'challenge',\n",
    "        'deficit', 'impair', 'delay', 'difficult', 'threat', 'downturn', 'problem'\n",
    "    ])\n",
    "    \n",
    "    # Tokenize and count\n",
    "    words = text.lower().split()\n",
    "    pos_count = sum(1 for word in words if any(pw in word for pw in positive_words))\n",
    "    neg_count = sum(1 for word in words if any(nw in word for nw in negative_words))\n",
    "    \n",
    "    total = pos_count + neg_count\n",
    "    if total == 0:\n",
    "        return {'positive_ratio': 0.5, 'negative_ratio': 0.5, 'net_sentiment': 0}\n",
    "    \n",
    "    return {\n",
    "        'positive_ratio': pos_count / total,\n",
    "        'negative_ratio': neg_count / total,\n",
    "        'net_sentiment': (pos_count - neg_count) / total,\n",
    "        'positive_count': pos_count,\n",
    "        'negative_count': neg_count\n",
    "    }\n",
    "\n",
    "def comprehensive_sentiment_analysis(text):\n",
    "    \"\"\"Perform comprehensive sentiment analysis using multiple methods\"\"\"\n",
    "    # Sentence-level analysis for more granular insights\n",
    "    sentences = sent_tokenize(text[:50000])  # Limit for performance\n",
    "    \n",
    "    sentence_sentiments = []\n",
    "    for sent in sentences[:500]:  # Analyze first 500 sentences\n",
    "        vader_score = vader_analyzer.polarity_scores(sent)\n",
    "        sentence_sentiments.append(vader_score['compound'])\n",
    "    \n",
    "    # Overall analysis\n",
    "    vader_scores = analyze_sentiment_vader(text[:50000])\n",
    "    textblob_scores = analyze_sentiment_textblob(text[:50000])\n",
    "    financial_scores = analyze_sentiment_financial_lexicon(text[:50000])\n",
    "    \n",
    "    return {\n",
    "        'vader': vader_scores,\n",
    "        'textblob': textblob_scores,\n",
    "        'financial_lexicon': financial_scores,\n",
    "        'sentence_level': {\n",
    "            'mean_sentiment': np.mean(sentence_sentiments) if sentence_sentiments else 0,\n",
    "            'std_sentiment': np.std(sentence_sentiments) if sentence_sentiments else 0,\n",
    "            'positive_sentences': sum(1 for s in sentence_sentiments if s > 0.05),\n",
    "            'negative_sentences': sum(1 for s in sentence_sentiments if s < -0.05),\n",
    "            'neutral_sentences': sum(1 for s in sentence_sentiments if -0.05 <= s <= 0.05)\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"Sentiment analysis functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yho5r5pkpei",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn-based Topic Modeling and Advanced Analysis\n",
    "def perform_topic_modeling(texts, n_topics=5, n_top_words=10):\n",
    "    \"\"\"\n",
    "    Perform topic modeling using sklearn's Latent Dirichlet Allocation (LDA)\n",
    "    \"\"\"\n",
    "    # Create document-term matrix\n",
    "    vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000, \n",
    "                                  stop_words='english')\n",
    "    \n",
    "    try:\n",
    "        doc_term_matrix = vectorizer.fit_transform(texts)\n",
    "        \n",
    "        # Fit LDA model\n",
    "        lda = LatentDirichletAllocation(n_components=n_topics, random_state=42, \n",
    "                                        max_iter=10, learning_method='online')\n",
    "        lda.fit(doc_term_matrix)\n",
    "        \n",
    "        # Get topics\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        topics = []\n",
    "        \n",
    "        for topic_idx, topic in enumerate(lda.components_):\n",
    "            top_words_idx = topic.argsort()[-n_top_words:][::-1]\n",
    "            top_words = [feature_names[i] for i in top_words_idx]\n",
    "            topics.append({\n",
    "                'topic_id': topic_idx,\n",
    "                'keywords': top_words,\n",
    "                'weights': topic[top_words_idx].tolist()\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            'topics': topics,\n",
    "            'model': lda,\n",
    "            'vectorizer': vectorizer\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error in topic modeling: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def create_tfidf_features(texts):\n",
    "    \"\"\"\n",
    "    Create TF-IDF features using sklearn for sentiment classification\n",
    "    \"\"\"\n",
    "    try:\n",
    "        vectorizer = TfidfVectorizer(max_features=500, stop_words='english', \n",
    "                                     ngram_range=(1, 2))\n",
    "        tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "        \n",
    "        # Get top TF-IDF terms\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        dense_matrix = tfidf_matrix.todense()\n",
    "        \n",
    "        # Calculate mean TF-IDF scores\n",
    "        mean_tfidf = np.array(dense_matrix.mean(axis=0)).flatten()\n",
    "        top_indices = mean_tfidf.argsort()[-20:][::-1]\n",
    "        \n",
    "        top_terms = [(feature_names[i], mean_tfidf[i]) for i in top_indices]\n",
    "        \n",
    "        return {\n",
    "            'vectorizer': vectorizer,\n",
    "            'tfidf_matrix': tfidf_matrix,\n",
    "            'top_terms': top_terms\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error in TF-IDF analysis: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def extract_risk_indicators(text):\n",
    "    \"\"\"\n",
    "    Extract risk-related indicators from financial text\n",
    "    \"\"\"\n",
    "    risk_keywords = [\n",
    "        'risk', 'uncertainty', 'volatility', 'adverse', 'litigation',\n",
    "        'competition', 'regulatory', 'compliance', 'loss', 'decline',\n",
    "        'challenge', 'threat', 'difficult', 'material weakness', 'going concern'\n",
    "    ]\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    risk_mentions = {}\n",
    "    \n",
    "    for keyword in risk_keywords:\n",
    "        count = text_lower.count(keyword)\n",
    "        risk_mentions[keyword] = count\n",
    "    \n",
    "    total_risks = sum(risk_mentions.values())\n",
    "    \n",
    "    return {\n",
    "        'risk_mentions': risk_mentions,\n",
    "        'total_risk_count': total_risks,\n",
    "        'risk_density': total_risks / max(len(text.split()), 1) * 1000  # per 1000 words\n",
    "    }\n",
    "\n",
    "print(\"sklearn-based analysis functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "do7s8aadnxe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download SEC filings for all companies\n",
    "companies = {\n",
    "    'AIRO': 'AIRO Group Holdings',\n",
    "    'AVAV': 'AeroVironment',\n",
    "    'KTOS': 'Kratos Defense',\n",
    "    'JOBY': 'Joby Aviation',\n",
    "    'ACHR': 'Archer Aviation'\n",
    "}\n",
    "\n",
    "filing_types = ['10-K', '10-Q', '8-K', 'DEF 14A']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DOWNLOADING SEC FILINGS FOR ALL COMPANIES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "all_filings = {}\n",
    "\n",
    "for ticker, company_name in companies.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Processing {ticker} - {company_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    try:\n",
    "        filing_paths = download_sec_filings(ticker, filing_types, num_filings=3)\n",
    "        all_filings[ticker] = filing_paths\n",
    "        print(f\"\\nâœ“ Successfully downloaded filings for {ticker}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâœ— Error downloading filings for {ticker}: {str(e)}\")\n",
    "        all_filings[ticker] = {}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DOWNLOAD COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTotal companies processed: {len(all_filings)}\")\n",
    "print(f\"Companies with filings: {sum(1 for v in all_filings.values() if v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1oj1um954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform sentiment analysis on all downloaded filings\n",
    "print(\"=\" * 80)\n",
    "print(\"PERFORMING SENTIMENT ANALYSIS ON SEC FILINGS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "sentiment_results = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for ticker, filings in all_filings.items():\n",
    "    if not filings:\n",
    "        print(f\"\\nâœ— No filings found for {ticker}, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Analyzing {ticker} - {companies[ticker]}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for filing_type, base_dir in filings.items():\n",
    "        print(f\"\\n  Processing {filing_type} filings...\")\n",
    "        \n",
    "        # Walk through all subdirectories\n",
    "        for root, dirs, files in os.walk(base_dir):\n",
    "            for file in files:\n",
    "                if file.endswith(('.htm', '.html', '.txt')):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    \n",
    "                    # Extract filing date from directory structure\n",
    "                    try:\n",
    "                        date_match = re.search(r'(\\d{4}-\\d{2}-\\d{2})', root)\n",
    "                        filing_date = date_match.group(1) if date_match else 'unknown'\n",
    "                    except:\n",
    "                        filing_date = 'unknown'\n",
    "                    \n",
    "                    print(f\"    â€¢ Analyzing: {file[:50]}... (Date: {filing_date})\")\n",
    "                    \n",
    "                    # Extract and analyze text\n",
    "                    text = extract_text_from_filing(file_path)\n",
    "                    \n",
    "                    if len(text) > 1000:  # Only analyze if substantial content\n",
    "                        # Perform comprehensive sentiment analysis\n",
    "                        sentiment = comprehensive_sentiment_analysis(text)\n",
    "                        \n",
    "                        # Extract risk indicators\n",
    "                        risk_info = extract_risk_indicators(text)\n",
    "                        \n",
    "                        # Store results\n",
    "                        result = {\n",
    "                            'filing_type': filing_type,\n",
    "                            'filing_date': filing_date,\n",
    "                            'ticker': ticker,\n",
    "                            'company': companies[ticker],\n",
    "                            'sentiment': sentiment,\n",
    "                            'risk_indicators': risk_info,\n",
    "                            'text_length': len(text),\n",
    "                            'file_path': file_path\n",
    "                        }\n",
    "                        \n",
    "                        sentiment_results[ticker][filing_type].append(result)\n",
    "                        \n",
    "                        # Print summary\n",
    "                        print(f\"      VADER Compound: {sentiment['vader']['compound']:.3f}\")\n",
    "                        print(f\"      TextBlob Polarity: {sentiment['textblob']['polarity']:.3f}\")\n",
    "                        print(f\"      Financial Net Sentiment: {sentiment['financial_lexicon']['net_sentiment']:.3f}\")\n",
    "                        print(f\"      Risk Density: {risk_info['risk_density']:.2f} per 1000 words\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SENTIMENT ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTotal companies analyzed: {len(sentiment_results)}\")\n",
    "for ticker in sentiment_results:\n",
    "    total_filings = sum(len(filings) for filings in sentiment_results[ticker].values())\n",
    "    print(f\"  {ticker}: {total_filings} filings analyzed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mhkcpkvgs1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for timeline analysis\n",
    "print(\"Preparing sentiment timeline data...\")\n",
    "\n",
    "timeline_data = []\n",
    "\n",
    "for ticker, filing_types in sentiment_results.items():\n",
    "    for filing_type, filings in filing_types.items():\n",
    "        for filing in filings:\n",
    "            if filing['filing_date'] != 'unknown':\n",
    "                try:\n",
    "                    timeline_data.append({\n",
    "                        'ticker': ticker,\n",
    "                        'company': filing['company'],\n",
    "                        'filing_type': filing_type,\n",
    "                        'date': pd.to_datetime(filing['filing_date']),\n",
    "                        'vader_compound': filing['sentiment']['vader']['compound'],\n",
    "                        'vader_positive': filing['sentiment']['vader']['positive'],\n",
    "                        'vader_negative': filing['sentiment']['vader']['negative'],\n",
    "                        'textblob_polarity': filing['sentiment']['textblob']['polarity'],\n",
    "                        'textblob_subjectivity': filing['sentiment']['textblob']['subjectivity'],\n",
    "                        'financial_net_sentiment': filing['sentiment']['financial_lexicon']['net_sentiment'],\n",
    "                        'financial_positive_count': filing['sentiment']['financial_lexicon']['positive_count'],\n",
    "                        'financial_negative_count': filing['sentiment']['financial_lexicon']['negative_count'],\n",
    "                        'risk_density': filing['risk_indicators']['risk_density'],\n",
    "                        'total_risk_count': filing['risk_indicators']['total_risk_count'],\n",
    "                        'mean_sentence_sentiment': filing['sentiment']['sentence_level']['mean_sentiment'],\n",
    "                        'positive_sentences': filing['sentiment']['sentence_level']['positive_sentences'],\n",
    "                        'negative_sentences': filing['sentiment']['sentence_level']['negative_sentences']\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing filing for {ticker}: {str(e)}\")\n",
    "\n",
    "# Create DataFrame\n",
    "sentiment_df = pd.DataFrame(timeline_data)\n",
    "\n",
    "if not sentiment_df.empty:\n",
    "    sentiment_df = sentiment_df.sort_values('date')\n",
    "    print(f\"\\nâœ“ Created sentiment timeline with {len(sentiment_df)} filings\")\n",
    "    print(f\"  Date range: {sentiment_df['date'].min()} to {sentiment_df['date'].max()}\")\n",
    "    print(f\"  Companies: {', '.join(sentiment_df['ticker'].unique())}\")\n",
    "    \n",
    "    # Display summary statistics\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SENTIMENT SUMMARY STATISTICS BY COMPANY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    summary_stats = sentiment_df.groupby('ticker').agg({\n",
    "        'vader_compound': ['mean', 'std', 'min', 'max'],\n",
    "        'financial_net_sentiment': ['mean', 'std'],\n",
    "        'risk_density': ['mean', 'std']\n",
    "    }).round(3)\n",
    "    \n",
    "    print(summary_stats)\n",
    "else:\n",
    "    print(\"âš  No valid timeline data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y39gfgac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Sentiment Timeline Evolution\n",
    "if not sentiment_df.empty:\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(20, 16))\n",
    "    fig.suptitle('Sentiment Evolution Timeline Across All Companies', fontsize=20, fontweight='bold', y=0.995)\n",
    "    \n",
    "    # 1. VADER Compound Sentiment Over Time\n",
    "    ax1 = axes[0, 0]\n",
    "    for ticker in sentiment_df['ticker'].unique():\n",
    "        ticker_data = sentiment_df[sentiment_df['ticker'] == ticker].sort_values('date')\n",
    "        ax1.plot(ticker_data['date'], ticker_data['vader_compound'], \n",
    "                marker='o', label=ticker, linewidth=2, markersize=8, alpha=0.7)\n",
    "    ax1.axhline(y=0, color='black', linestyle='--', alpha=0.3, linewidth=1)\n",
    "    ax1.set_xlabel('Filing Date', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('VADER Compound Score', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title('VADER Sentiment Evolution', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(loc='best', fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 2. Financial Lexicon Net Sentiment Over Time\n",
    "    ax2 = axes[0, 1]\n",
    "    for ticker in sentiment_df['ticker'].unique():\n",
    "        ticker_data = sentiment_df[sentiment_df['ticker'] == ticker].sort_values('date')\n",
    "        ax2.plot(ticker_data['date'], ticker_data['financial_net_sentiment'], \n",
    "                marker='s', label=ticker, linewidth=2, markersize=8, alpha=0.7)\n",
    "    ax2.axhline(y=0, color='black', linestyle='--', alpha=0.3, linewidth=1)\n",
    "    ax2.set_xlabel('Filing Date', fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylabel('Financial Net Sentiment', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title('Financial Lexicon Sentiment Evolution', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(loc='best', fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 3. Risk Density Over Time\n",
    "    ax3 = axes[1, 0]\n",
    "    for ticker in sentiment_df['ticker'].unique():\n",
    "        ticker_data = sentiment_df[sentiment_df['ticker'] == ticker].sort_values('date')\n",
    "        ax3.plot(ticker_data['date'], ticker_data['risk_density'], \n",
    "                marker='^', label=ticker, linewidth=2, markersize=8, alpha=0.7)\n",
    "    ax3.set_xlabel('Filing Date', fontsize=12, fontweight='bold')\n",
    "    ax3.set_ylabel('Risk Mentions per 1000 Words', fontsize=12, fontweight='bold')\n",
    "    ax3.set_title('Risk Density Evolution', fontsize=14, fontweight='bold')\n",
    "    ax3.legend(loc='best', fontsize=10)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 4. TextBlob Polarity Over Time\n",
    "    ax4 = axes[1, 1]\n",
    "    for ticker in sentiment_df['ticker'].unique():\n",
    "        ticker_data = sentiment_df[sentiment_df['ticker'] == ticker].sort_values('date')\n",
    "        ax4.plot(ticker_data['date'], ticker_data['textblob_polarity'], \n",
    "                marker='D', label=ticker, linewidth=2, markersize=8, alpha=0.7)\n",
    "    ax4.axhline(y=0, color='black', linestyle='--', alpha=0.3, linewidth=1)\n",
    "    ax4.set_xlabel('Filing Date', fontsize=12, fontweight='bold')\n",
    "    ax4.set_ylabel('TextBlob Polarity', fontsize=12, fontweight='bold')\n",
    "    ax4.set_title('TextBlob Sentiment Evolution', fontsize=14, fontweight='bold')\n",
    "    ax4.legend(loc='best', fontsize=10)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 5. Positive vs Negative Sentence Ratio\n",
    "    ax5 = axes[2, 0]\n",
    "    for ticker in sentiment_df['ticker'].unique():\n",
    "        ticker_data = sentiment_df[sentiment_df['ticker'] == ticker].sort_values('date')\n",
    "        ratio = ticker_data['positive_sentences'] / (ticker_data['negative_sentences'] + 1)\n",
    "        ax5.plot(ticker_data['date'], ratio, \n",
    "                marker='*', label=ticker, linewidth=2, markersize=10, alpha=0.7)\n",
    "    ax5.axhline(y=1, color='black', linestyle='--', alpha=0.3, linewidth=1)\n",
    "    ax5.set_xlabel('Filing Date', fontsize=12, fontweight='bold')\n",
    "    ax5.set_ylabel('Positive/Negative Sentence Ratio', fontsize=12, fontweight='bold')\n",
    "    ax5.set_title('Sentiment Balance Evolution', fontsize=14, fontweight='bold')\n",
    "    ax5.legend(loc='best', fontsize=10)\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    ax5.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 6. Mean Sentence Sentiment\n",
    "    ax6 = axes[2, 1]\n",
    "    for ticker in sentiment_df['ticker'].unique():\n",
    "        ticker_data = sentiment_df[sentiment_df['ticker'] == ticker].sort_values('date')\n",
    "        ax6.plot(ticker_data['date'], ticker_data['mean_sentence_sentiment'], \n",
    "                marker='p', label=ticker, linewidth=2, markersize=8, alpha=0.7)\n",
    "    ax6.axhline(y=0, color='black', linestyle='--', alpha=0.3, linewidth=1)\n",
    "    ax6.set_xlabel('Filing Date', fontsize=12, fontweight='bold')\n",
    "    ax6.set_ylabel('Mean Sentence Sentiment', fontsize=12, fontweight='bold')\n",
    "    ax6.set_title('Average Sentence-Level Sentiment', fontsize=14, fontweight='bold')\n",
    "    ax6.legend(loc='best', fontsize=10)\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "    ax6.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nâœ“ Sentiment timeline visualizations generated successfully!\")\n",
    "else:\n",
    "    print(\"âš  No data available for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gyeea6q9m85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparative Sentiment Analysis Across Companies\n",
    "if not sentiment_df.empty:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(22, 12))\n",
    "    fig.suptitle('Comparative Sentiment Analysis: AIRO vs Competitors', fontsize=20, fontweight='bold', y=0.995)\n",
    "    \n",
    "    # 1. Average VADER Sentiment by Company\n",
    "    ax1 = axes[0, 0]\n",
    "    avg_vader = sentiment_df.groupby('ticker')['vader_compound'].mean().sort_values(ascending=False)\n",
    "    colors = ['#2ecc71' if x > 0 else '#e74c3c' for x in avg_vader.values]\n",
    "    bars1 = ax1.bar(avg_vader.index, avg_vader.values, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "    ax1.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    ax1.set_ylabel('Average VADER Compound Score', fontsize=11, fontweight='bold')\n",
    "    ax1.set_title('Average VADER Sentiment by Company', fontsize=13, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}', ha='center', va='bottom' if height > 0 else 'top', fontweight='bold')\n",
    "    \n",
    "    # 2. Average Financial Net Sentiment by Company\n",
    "    ax2 = axes[0, 1]\n",
    "    avg_financial = sentiment_df.groupby('ticker')['financial_net_sentiment'].mean().sort_values(ascending=False)\n",
    "    colors = ['#3498db' if x > 0 else '#e67e22' for x in avg_financial.values]\n",
    "    bars2 = ax2.bar(avg_financial.index, avg_financial.values, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "    ax2.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    ax2.set_ylabel('Average Financial Net Sentiment', fontsize=11, fontweight='bold')\n",
    "    ax2.set_title('Average Financial Sentiment by Company', fontsize=13, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}', ha='center', va='bottom' if height > 0 else 'top', fontweight='bold')\n",
    "    \n",
    "    # 3. Average Risk Density by Company\n",
    "    ax3 = axes[0, 2]\n",
    "    avg_risk = sentiment_df.groupby('ticker')['risk_density'].mean().sort_values(ascending=True)\n",
    "    bars3 = ax3.bar(avg_risk.index, avg_risk.values, color='#95a5a6', alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "    ax3.set_ylabel('Average Risk Density (per 1000 words)', fontsize=11, fontweight='bold')\n",
    "    ax3.set_title('Average Risk Density by Company', fontsize=13, fontweight='bold')\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    for bar in bars3:\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 4. Sentiment Distribution (Box Plot)\n",
    "    ax4 = axes[1, 0]\n",
    "    sentiment_df.boxplot(column='vader_compound', by='ticker', ax=ax4, patch_artist=True)\n",
    "    ax4.set_xlabel('Company', fontsize=11, fontweight='bold')\n",
    "    ax4.set_ylabel('VADER Compound Score', fontsize=11, fontweight='bold')\n",
    "    ax4.set_title('Sentiment Distribution by Company', fontsize=13, fontweight='bold')\n",
    "    ax4.get_figure().suptitle('')  # Remove default title\n",
    "    plt.setp(ax4.xaxis.get_majorticklabels(), rotation=0)\n",
    "    \n",
    "    # 5. Positive/Negative Word Count Comparison\n",
    "    ax5 = axes[1, 1]\n",
    "    pos_neg_data = sentiment_df.groupby('ticker')[['financial_positive_count', 'financial_negative_count']].mean()\n",
    "    x_pos = np.arange(len(pos_neg_data.index))\n",
    "    width = 0.35\n",
    "    bars_pos = ax5.bar(x_pos - width/2, pos_neg_data['financial_positive_count'], \n",
    "                       width, label='Positive Words', color='#27ae60', alpha=0.7, edgecolor='black')\n",
    "    bars_neg = ax5.bar(x_pos + width/2, pos_neg_data['financial_negative_count'], \n",
    "                       width, label='Negative Words', color='#c0392b', alpha=0.7, edgecolor='black')\n",
    "    ax5.set_xlabel('Company', fontsize=11, fontweight='bold')\n",
    "    ax5.set_ylabel('Average Word Count', fontsize=11, fontweight='bold')\n",
    "    ax5.set_title('Positive vs Negative Financial Terms', fontsize=13, fontweight='bold')\n",
    "    ax5.set_xticks(x_pos)\n",
    "    ax5.set_xticklabels(pos_neg_data.index)\n",
    "    ax5.legend(fontsize=10)\n",
    "    ax5.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 6. Sentiment Consistency (Standard Deviation)\n",
    "    ax6 = axes[1, 2]\n",
    "    sentiment_std = sentiment_df.groupby('ticker')['vader_compound'].std().sort_values()\n",
    "    bars6 = ax6.bar(sentiment_std.index, sentiment_std.values, color='#9b59b6', alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "    ax6.set_ylabel('Standard Deviation', fontsize=11, fontweight='bold')\n",
    "    ax6.set_title('Sentiment Consistency (Lower = More Consistent)', fontsize=13, fontweight='bold')\n",
    "    ax6.grid(True, alpha=0.3, axis='y')\n",
    "    for bar in bars6:\n",
    "        height = bar.get_height()\n",
    "        ax6.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nâœ“ Comparative analysis visualizations generated successfully!\")\n",
    "else:\n",
    "    print(\"âš  No data available for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ajpj7e0kyn7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn-based TF-IDF and Topic Modeling Analysis\n",
    "print(\"=\" * 80)\n",
    "print(\"PERFORMING SKLEARN-BASED TEXT ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Collect all texts for each company\n",
    "company_texts = defaultdict(list)\n",
    "\n",
    "for ticker, filing_types in sentiment_results.items():\n",
    "    for filing_type, filings in filing_types.items():\n",
    "        for filing in filings:\n",
    "            # Extract text again for topic modeling\n",
    "            text = extract_text_from_filing(filing['file_path'])\n",
    "            if len(text) > 5000:  # Only use substantial documents\n",
    "                # Preprocess text\n",
    "                clean_text = preprocess_text(text[:100000])\n",
    "                if len(clean_text) > 1000:\n",
    "                    company_texts[ticker].append(clean_text)\n",
    "\n",
    "print(f\"\\nCollected texts for {len(company_texts)} companies\")\n",
    "\n",
    "# Perform TF-IDF analysis for each company\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TF-IDF ANALYSIS - TOP TERMS BY COMPANY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "tfidf_results = {}\n",
    "\n",
    "for ticker, texts in company_texts.items():\n",
    "    if len(texts) > 0:\n",
    "        print(f\"\\n{ticker} - {companies[ticker]}:\")\n",
    "        tfidf_result = create_tfidf_features(texts)\n",
    "        if tfidf_result:\n",
    "            tfidf_results[ticker] = tfidf_result\n",
    "            print(\"  Top 10 TF-IDF terms:\")\n",
    "            for i, (term, score) in enumerate(tfidf_result['top_terms'][:10], 1):\n",
    "                print(f\"    {i}. {term}: {score:.4f}\")\n",
    "\n",
    "# Perform Topic Modeling for each company\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TOPIC MODELING ANALYSIS (LDA)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "topic_results = {}\n",
    "\n",
    "for ticker, texts in company_texts.items():\n",
    "    if len(texts) >= 3:  # Need multiple documents for topic modeling\n",
    "        print(f\"\\n{ticker} - {companies[ticker]}:\")\n",
    "        topic_result = perform_topic_modeling(texts, n_topics=3, n_top_words=8)\n",
    "        if topic_result:\n",
    "            topic_results[ticker] = topic_result\n",
    "            print(\"  Discovered Topics:\")\n",
    "            for topic in topic_result['topics']:\n",
    "                print(f\"    Topic {topic['topic_id']}: {', '.join(topic['keywords'][:5])}\")\n",
    "    else:\n",
    "        print(f\"\\n{ticker}: Insufficient documents for topic modeling (need at least 3)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SKLEARN TEXT ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pe3m9brg9p",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Comprehensive Sentiment Summary Table\n",
    "if not sentiment_df.empty:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"COMPREHENSIVE SENTIMENT SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Calculate comprehensive metrics for each company\n",
    "    summary_data = []\n",
    "    \n",
    "    for ticker in sentiment_df['ticker'].unique():\n",
    "        ticker_data = sentiment_df[sentiment_df['ticker'] == ticker]\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Ticker': ticker,\n",
    "            'Company': companies[ticker],\n",
    "            'Filings Analyzed': len(ticker_data),\n",
    "            'Avg VADER Sentiment': ticker_data['vader_compound'].mean(),\n",
    "            'VADER Std Dev': ticker_data['vader_compound'].std(),\n",
    "            'Avg Financial Sentiment': ticker_data['financial_net_sentiment'].mean(),\n",
    "            'Avg Risk Density': ticker_data['risk_density'].mean(),\n",
    "            'Avg Positive Words': ticker_data['financial_positive_count'].mean(),\n",
    "            'Avg Negative Words': ticker_data['financial_negative_count'].mean(),\n",
    "            'Pos/Neg Ratio': ticker_data['financial_positive_count'].mean() / max(ticker_data['financial_negative_count'].mean(), 1),\n",
    "            'Avg TextBlob Polarity': ticker_data['textblob_polarity'].mean(),\n",
    "            'Avg Subjectivity': ticker_data['textblob_subjectivity'].mean(),\n",
    "            'Avg Sentence Sentiment': ticker_data['mean_sentence_sentiment'].mean(),\n",
    "            'Latest VADER Score': ticker_data.iloc[-1]['vader_compound'] if len(ticker_data) > 0 else None,\n",
    "            'Sentiment Trend': 'Improving' if len(ticker_data) > 1 and ticker_data.iloc[-1]['vader_compound'] > ticker_data.iloc[0]['vader_compound'] else 'Declining'\n",
    "        })\n",
    "    \n",
    "    summary_table = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Sort by average VADER sentiment\n",
    "    summary_table = summary_table.sort_values('Avg VADER Sentiment', ascending=False)\n",
    "    \n",
    "    # Format for display\n",
    "    display_table = summary_table.copy()\n",
    "    display_table['Avg VADER Sentiment'] = display_table['Avg VADER Sentiment'].round(3)\n",
    "    display_table['VADER Std Dev'] = display_table['VADER Std Dev'].round(3)\n",
    "    display_table['Avg Financial Sentiment'] = display_table['Avg Financial Sentiment'].round(3)\n",
    "    display_table['Avg Risk Density'] = display_table['Avg Risk Density'].round(1)\n",
    "    display_table['Pos/Neg Ratio'] = display_table['Pos/Neg Ratio'].round(2)\n",
    "    \n",
    "    print(\"\\nSENTIMENT RANKING (by VADER Compound Score):\")\n",
    "    print(display_table.to_string(index=False))\n",
    "    \n",
    "    # Highlight AIRO's position\n",
    "    airo_data = summary_table[summary_table['Ticker'] == 'AIRO']\n",
    "    if not airo_data.empty:\n",
    "        airo_rank = summary_table[summary_table['Ticker'] == 'AIRO'].index[0] + 1\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"AIRO POSITION: #{airo_rank} out of {len(summary_table)} companies\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"AIRO's Average VADER Sentiment: {airo_data['Avg VADER Sentiment'].values[0]:.3f}\")\n",
    "        print(f\"AIRO's Risk Density: {airo_data['Avg Risk Density'].values[0]:.1f} (Lower is better)\")\n",
    "        print(f\"AIRO's Sentiment Trend: {airo_data['Sentiment Trend'].values[0]}\")\n",
    "    \n",
    "    # Export summary table\n",
    "    summary_table.to_csv('sentiment_analysis_summary.csv', index=False)\n",
    "    print(f\"\\nâœ“ Summary table exported to: sentiment_analysis_summary.csv\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš  No sentiment data available for summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hdc61edub7m",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Correlation Heatmap and Detailed Export\n",
    "if not sentiment_df.empty:\n",
    "    # Create correlation heatmap of sentiment metrics\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "    fig.suptitle('Sentiment Metrics Correlation Analysis', fontsize=18, fontweight='bold')\n",
    "    \n",
    "    # Select key sentiment metrics\n",
    "    sentiment_metrics = sentiment_df[[\n",
    "        'vader_compound', 'financial_net_sentiment', 'textblob_polarity',\n",
    "        'risk_density', 'mean_sentence_sentiment', 'textblob_subjectivity'\n",
    "    ]].copy()\n",
    "    \n",
    "    # Rename columns for better display\n",
    "    sentiment_metrics.columns = [\n",
    "        'VADER', 'Financial', 'TextBlob',\n",
    "        'Risk Density', 'Sentence Sent.', 'Subjectivity'\n",
    "    ]\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = sentiment_metrics.corr()\n",
    "    \n",
    "    # Plot correlation heatmap\n",
    "    ax1 = axes[0]\n",
    "    im1 = ax1.imshow(corr_matrix, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)\n",
    "    ax1.set_xticks(np.arange(len(corr_matrix.columns)))\n",
    "    ax1.set_yticks(np.arange(len(corr_matrix.columns)))\n",
    "    ax1.set_xticklabels(corr_matrix.columns, rotation=45, ha='right')\n",
    "    ax1.set_yticklabels(corr_matrix.columns)\n",
    "    ax1.set_title('Correlation Matrix of Sentiment Metrics', fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Add correlation values\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(len(corr_matrix.columns)):\n",
    "            text = ax1.text(j, i, f'{corr_matrix.iloc[i, j]:.2f}',\n",
    "                          ha=\"center\", va=\"center\", color=\"black\", fontweight='bold', fontsize=10)\n",
    "    \n",
    "    cbar1 = plt.colorbar(im1, ax=ax1)\n",
    "    cbar1.set_label('Correlation Coefficient', fontsize=11)\n",
    "    \n",
    "    # Create sentiment heatmap by company\n",
    "    ax2 = axes[1]\n",
    "    pivot_data = sentiment_df.pivot_table(\n",
    "        values=['vader_compound', 'financial_net_sentiment', 'risk_density'],\n",
    "        index='ticker',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    # Normalize risk_density (invert so lower is better)\n",
    "    pivot_data['risk_density'] = -pivot_data['risk_density'] / pivot_data['risk_density'].max()\n",
    "    \n",
    "    # Rename columns\n",
    "    pivot_data.columns = ['Financial', 'Risk (inv)', 'VADER']\n",
    "    \n",
    "    im2 = ax2.imshow(pivot_data.T, cmap='RdYlGn', aspect='auto', vmin=-0.5, vmax=0.5)\n",
    "    ax2.set_xticks(np.arange(len(pivot_data.index)))\n",
    "    ax2.set_yticks(np.arange(len(pivot_data.columns)))\n",
    "    ax2.set_xticklabels(pivot_data.index, fontsize=12, fontweight='bold')\n",
    "    ax2.set_yticklabels(pivot_data.columns, fontsize=11)\n",
    "    ax2.set_title('Average Sentiment Scores by Company', fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Add values\n",
    "    for i in range(len(pivot_data.columns)):\n",
    "        for j in range(len(pivot_data.index)):\n",
    "            value = pivot_data.iloc[j, i]\n",
    "            text = ax2.text(j, i, f'{value:.2f}',\n",
    "                          ha=\"center\", va=\"center\", \n",
    "                          color=\"white\" if abs(value) > 0.3 else \"black\", \n",
    "                          fontweight='bold', fontsize=11)\n",
    "    \n",
    "    cbar2 = plt.colorbar(im2, ax=ax2)\n",
    "    cbar2.set_label('Normalized Score', fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Export detailed sentiment data\n",
    "    sentiment_df.to_csv('detailed_sentiment_timeline.csv', index=False)\n",
    "    print(f\"\\nâœ“ Detailed timeline data exported to: detailed_sentiment_timeline.csv\")\n",
    "    \n",
    "    # Export raw sentiment results as JSON\n",
    "    with open('sentiment_analysis_results.json', 'w') as f:\n",
    "        # Convert results to JSON-serializable format\n",
    "        export_data = {}\n",
    "        for ticker, filing_types in sentiment_results.items():\n",
    "            export_data[ticker] = {}\n",
    "            for filing_type, filings in filing_types.items():\n",
    "                export_data[ticker][filing_type] = []\n",
    "                for filing in filings:\n",
    "                    export_data[ticker][filing_type].append({\n",
    "                        'filing_date': filing['filing_date'],\n",
    "                        'sentiment': filing['sentiment'],\n",
    "                        'risk_indicators': filing['risk_indicators'],\n",
    "                        'text_length': filing['text_length']\n",
    "                    })\n",
    "        json.dump(export_data, f, indent=2)\n",
    "    \n",
    "    print(f\"âœ“ Raw sentiment results exported to: sentiment_analysis_results.json\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ALL SENTIMENT ANALYSIS COMPLETE!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nKey Outputs Generated:\")\n",
    "    print(\"  1. Sentiment timeline visualizations (6 charts)\")\n",
    "    print(\"  2. Comparative analysis visualizations (6 charts)\")\n",
    "    print(\"  3. Correlation and heatmap analysis (2 charts)\")\n",
    "    print(\"  4. TF-IDF analysis for each company\")\n",
    "    print(\"  5. Topic modeling (LDA) for each company\")\n",
    "    print(\"  6. sentiment_analysis_summary.csv\")\n",
    "    print(\"  7. detailed_sentiment_timeline.csv\")\n",
    "    print(\"  8. sentiment_analysis_results.json\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš  No sentiment data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92osi5ku",
   "metadata": {},
   "source": [
    "## Key Insights & Interpretation Guide\n",
    "\n",
    "### Sentiment Metrics Explained\n",
    "\n",
    "1. **VADER Compound Score** (-1 to +1)\n",
    "   - Range: -1 (most negative) to +1 (most positive)\n",
    "   - Optimized for social media and financial texts\n",
    "   - **Interpretation**: Higher scores indicate more positive management tone and outlook\n",
    "   - Compound score â‰¥ 0.05 = Positive, â‰¤ -0.05 = Negative, between = Neutral\n",
    "\n",
    "2. **Financial Net Sentiment** (-1 to +1)\n",
    "   - Based on Loughran-McDonald financial dictionary\n",
    "   - Measures (Positive - Negative) / Total financial terms\n",
    "   - **Interpretation**: More relevant for financial disclosures than general sentiment tools\n",
    "\n",
    "3. **TextBlob Polarity** (-1 to +1)\n",
    "   - General-purpose sentiment from linguistic patterns\n",
    "   - **Subjectivity** (0 to 1): Higher = more opinionated vs factual\n",
    "\n",
    "4. **Risk Density** (per 1000 words)\n",
    "   - Count of risk-related keywords normalized by document length\n",
    "   - **Interpretation**: Lower is generally better; high values may indicate significant uncertainties or challenges\n",
    "\n",
    "5. **Sentence-Level Sentiment**\n",
    "   - Analyzes each sentence individually then aggregates\n",
    "   - Provides distribution of positive/negative/neutral sentences\n",
    "   - **Interpretation**: More granular view of sentiment throughout document\n",
    "\n",
    "### sklearn-Based Analysis\n",
    "\n",
    "- **TF-IDF (Term Frequency-Inverse Document Frequency)**\n",
    "  - Identifies most important/distinctive terms for each company\n",
    "  - Helps understand what topics dominate each company's filings\n",
    "  \n",
    "- **LDA Topic Modeling**\n",
    "  - Discovers latent themes in documents\n",
    "  - Groups related terms into coherent topics\n",
    "  - Useful for understanding strategic focus areas\n",
    "\n",
    "### Comparative Analysis Insights\n",
    "\n",
    "- **Sentiment Trend**: Compare how sentiment evolves over time for AIRO vs competitors\n",
    "- **Risk Profile**: Companies with lower risk density and higher sentiment may be viewed more favorably\n",
    "- **Consistency**: Standard deviation shows sentiment stability (lower = more consistent)\n",
    "- **Positive/Negative Ratio**: Balance of optimistic vs cautionary language\n",
    "\n",
    "### Investment Implications\n",
    "\n",
    "âš  **Important**: Sentiment analysis should complement, not replace, fundamental and technical analysis. Consider:\n",
    "- Positive sentiment + strong fundamentals = Potential opportunity\n",
    "- Deteriorating sentiment + weakening metrics = Warning signal\n",
    "- Rising risk mentions may precede operational challenges\n",
    "- Extremely positive sentiment without fundamental support = Caution\n",
    "\n",
    "### Files Generated\n",
    "\n",
    "All analysis results are saved for further analysis:\n",
    "- `sentiment_analysis_summary.csv` - Company-level aggregated metrics\n",
    "- `detailed_sentiment_timeline.csv` - Complete time-series data\n",
    "- `sentiment_analysis_results.json` - Raw sentiment scores for all filings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6342a8f0",
   "metadata": {
    "papermill": {
     "duration": 0.00812,
     "end_time": "2026-01-08T17:45:57.511356",
     "exception": false,
     "start_time": "2026-01-08T17:45:57.503236",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# AIRO Group Holdings - Quantitative Equity Analysis\n",
    "## Comparative Valuation: Aerospace, Defense & eVTOL Sectors\n",
    "\n",
    "**Analysis Date:** January 8, 2026  \n",
    "**Analyst Methodology:** Goldman Sachs-style equity research with DCF, Comparable Company Analysis, and Sum-of-Parts valuation\n",
    "\n",
    "### Objective\n",
    "This notebook provides comprehensive quantitative analysis of AIRO Group Holdings (NASDAQ: AIRO) against its primary competitors across three segments:\n",
    "- **Drone/Defense:** AeroVironment (AVAV), Kratos Defense (KTOS)\n",
    "- **eVTOL:** Joby Aviation (JOBY), Archer Aviation (ACHR)\n",
    "- **Diversified Comparison:** All peers\n",
    "\n",
    "### Valuation Methodologies\n",
    "1. **Trading Comparables Analysis** - EV/Sales, P/B, Price/Revenue multiples\n",
    "2. **Financial Performance Metrics** - Revenue growth, margin trends, cash burn\n",
    "3. **Sum-of-Parts (SOTP) Valuation** - Segment-level analysis for AIRO's four divisions\n",
    "4. **Technical Analysis** - Price momentum, relative strength, volatility\n",
    "5. **Risk-Adjusted Return Framework** - Sharpe ratios, beta analysis, downside protection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa44a0d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T17:45:57.525117Z",
     "iopub.status.busy": "2026-01-08T17:45:57.524720Z",
     "iopub.status.idle": "2026-01-08T17:46:03.816143Z",
     "shell.execute_reply": "2026-01-08T17:46:03.814729Z"
    },
    "papermill": {
     "duration": 6.301246,
     "end_time": "2026-01-08T17:46:03.818673",
     "exception": false,
     "start_time": "2026-01-08T17:45:57.517427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install yfinance pandas numpy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71a2458",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T17:46:03.834826Z",
     "iopub.status.busy": "2026-01-08T17:46:03.834427Z",
     "iopub.status.idle": "2026-01-08T17:46:09.278642Z",
     "shell.execute_reply": "2026-01-08T17:46:09.277380Z"
    },
    "papermill": {
     "duration": 5.455334,
     "end_time": "2026-01-08T17:46:09.280903",
     "exception": false,
     "start_time": "2026-01-08T17:46:03.825569",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure visualization aesthetics for professional presentation\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully\")\n",
    "print(f\"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4f9346",
   "metadata": {
    "papermill": {
     "duration": 0.006798,
     "end_time": "2026-01-08T17:46:09.295040",
     "exception": false,
     "start_time": "2026-01-08T17:46:09.288242",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Data Collection & Company Universe\n",
    "\n",
    "We'll analyze AIRO against five key competitors spanning the drone, defense, and eVTOL markets. Each company represents a different strategic positioning within the aerospace ecosystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b0a354",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T17:46:09.310093Z",
     "iopub.status.busy": "2026-01-08T17:46:09.308941Z",
     "iopub.status.idle": "2026-01-08T17:46:09.317379Z",
     "shell.execute_reply": "2026-01-08T17:46:09.315990Z"
    },
    "papermill": {
     "duration": 0.018312,
     "end_time": "2026-01-08T17:46:09.319615",
     "exception": false,
     "start_time": "2026-01-08T17:46:09.301303",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define company universe with strategic categorization\n",
    "companies = {\n",
    "    'AIRO': {'name': 'AIRO Group Holdings', 'category': 'Diversified', 'segment': 'Drone+eVTOL+Training'},\n",
    "    'AVAV': {'name': 'AeroVironment', 'category': 'Drone/Defense', 'segment': 'Military UAS'},\n",
    "    'KTOS': {'name': 'Kratos Defense', 'category': 'Drone/Defense', 'segment': 'Unmanned Systems'},\n",
    "    'JOBY': {'name': 'Joby Aviation', 'category': 'eVTOL', 'segment': 'Air Taxi'},\n",
    "    'ACHR': {'name': 'Archer Aviation', 'category': 'eVTOL', 'segment': 'Air Taxi'},\n",
    "}\n",
    "\n",
    "tickers = list(companies.keys())\n",
    "print(\"Company Universe:\")\n",
    "print(\"=\"*80)\n",
    "for ticker, info in companies.items():\n",
    "    print(f\"{ticker:6} | {info['name']:25} | {info['category']:15} | {info['segment']}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668a2e8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T17:46:09.336643Z",
     "iopub.status.busy": "2026-01-08T17:46:09.335320Z",
     "iopub.status.idle": "2026-01-08T17:46:12.145633Z",
     "shell.execute_reply": "2026-01-08T17:46:12.144256Z"
    },
    "papermill": {
     "duration": 2.821469,
     "end_time": "2026-01-08T17:46:12.148416",
     "exception": false,
     "start_time": "2026-01-08T17:46:09.326947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download historical price data (2 years for trend analysis)\n",
    "# Using 2-year window to capture IPO performance for AIRO (June 2025) and volatility patterns\n",
    "\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=730)  # 2 years\n",
    "\n",
    "print(f\"Downloading market data from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}...\")\n",
    "print()\n",
    "\n",
    "price_data = {}\n",
    "stock_info = {}\n",
    "\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        stock = yf.Ticker(ticker)\n",
    "        price_data[ticker] = stock.history(start=start_date, end=end_date)\n",
    "        stock_info[ticker] = stock.info\n",
    "        print(f\"âœ“ {ticker:6} - {len(price_data[ticker])} trading days retrieved\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— {ticker:6} - Error: {str(e)}\")\n",
    "        price_data[ticker] = pd.DataFrame()\n",
    "        stock_info[ticker] = {}\n",
    "\n",
    "print(\"\\nâœ“ Data collection complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b61bfa2",
   "metadata": {
    "papermill": {
     "duration": 0.006682,
     "end_time": "2026-01-08T17:46:12.162285",
     "exception": false,
     "start_time": "2026-01-08T17:46:12.155603",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Key Financial Metrics Extraction\n",
    "\n",
    "We'll extract critical valuation metrics from each company, focusing on market capitalization, revenue metrics, profitability indicators, and balance sheet strength. For pre-revenue companies like JOBY and ACHR, we'll use forward-looking estimates where available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80adb3ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T17:46:12.180484Z",
     "iopub.status.busy": "2026-01-08T17:46:12.180109Z",
     "iopub.status.idle": "2026-01-08T17:46:12.211746Z",
     "shell.execute_reply": "2026-01-08T17:46:12.210508Z"
    },
    "papermill": {
     "duration": 0.042497,
     "end_time": "2026-01-08T17:46:12.214056",
     "exception": false,
     "start_time": "2026-01-08T17:46:12.171559",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def safe_get(info_dict, key, default=np.nan):\n",
    "    \"\"\"Safely extract values from stock info dictionary, handling missing keys gracefully\"\"\"\n",
    "    return info_dict.get(key, default)\n",
    "\n",
    "def calculate_ev(market_cap, cash, debt):\n",
    "    \"\"\"Calculate Enterprise Value: Market Cap + Debt - Cash\"\"\"\n",
    "    try:\n",
    "        return market_cap + debt - cash\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# Build comprehensive financial metrics dataframe\n",
    "metrics_data = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    info = stock_info[ticker]\n",
    "    \n",
    "    # Current price from latest close\n",
    "    current_price = price_data[ticker]['Close'].iloc[-1] if len(price_data[ticker]) > 0 else np.nan\n",
    "    \n",
    "    # Market capitalization and enterprise value components\n",
    "    market_cap = safe_get(info, 'marketCap', np.nan)\n",
    "    cash = safe_get(info, 'totalCash', 0)\n",
    "    debt = safe_get(info, 'totalDebt', 0)\n",
    "    enterprise_value = calculate_ev(market_cap, cash, debt)\n",
    "    \n",
    "    # Revenue metrics (TTM = Trailing Twelve Months)\n",
    "    revenue = safe_get(info, 'totalRevenue', np.nan)\n",
    "    revenue_growth = safe_get(info, 'revenueGrowth', np.nan)\n",
    "    \n",
    "    # Profitability metrics\n",
    "    gross_margin = safe_get(info, 'grossMargins', np.nan)\n",
    "    operating_margin = safe_get(info, 'operatingMargins', np.nan)\n",
    "    profit_margin = safe_get(info, 'profitMargins', np.nan)\n",
    "    \n",
    "    # Per-share metrics\n",
    "    eps = safe_get(info, 'trailingEps', np.nan)\n",
    "    book_value = safe_get(info, 'bookValue', np.nan)\n",
    "    \n",
    "    # Valuation multiples\n",
    "    pe_ratio = safe_get(info, 'trailingPE', np.nan)\n",
    "    pb_ratio = safe_get(info, 'priceToBook', np.nan)\n",
    "    ps_ratio = market_cap / revenue if revenue and revenue > 0 else np.nan\n",
    "    ev_sales = enterprise_value / revenue if revenue and revenue > 0 else np.nan\n",
    "    \n",
    "    # Risk metrics\n",
    "    beta = safe_get(info, 'beta', np.nan)\n",
    "    \n",
    "    # Analyst recommendations\n",
    "    target_price = safe_get(info, 'targetMeanPrice', np.nan)\n",
    "    num_analysts = safe_get(info, 'numberOfAnalystOpinions', 0)\n",
    "    \n",
    "    metrics_data.append({\n",
    "        'Ticker': ticker,\n",
    "        'Company': companies[ticker]['name'],\n",
    "        'Category': companies[ticker]['category'],\n",
    "        'Current Price': current_price,\n",
    "        'Market Cap (M)': market_cap / 1e6 if market_cap else np.nan,\n",
    "        'Enterprise Value (M)': enterprise_value / 1e6 if enterprise_value else np.nan,\n",
    "        'Revenue TTM (M)': revenue / 1e6 if revenue else np.nan,\n",
    "        'Revenue Growth %': revenue_growth * 100 if revenue_growth else np.nan,\n",
    "        'Gross Margin %': gross_margin * 100 if gross_margin else np.nan,\n",
    "        'Operating Margin %': operating_margin * 100 if operating_margin else np.nan,\n",
    "        'Profit Margin %': profit_margin * 100 if profit_margin else np.nan,\n",
    "        'EPS': eps,\n",
    "        'Book Value': book_value,\n",
    "        'P/E': pe_ratio,\n",
    "        'P/B': pb_ratio,\n",
    "        'P/S': ps_ratio,\n",
    "        'EV/Sales': ev_sales,\n",
    "        'Beta': beta,\n",
    "        'Analyst Target': target_price,\n",
    "        'Upside to Target %': ((target_price / current_price - 1) * 100) if target_price and current_price else np.nan,\n",
    "        'Num Analysts': num_analysts\n",
    "    })\n",
    "\n",
    "df_metrics = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Display formatted metrics table\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"FINANCIAL METRICS SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "print(df_metrics.to_string(index=False))\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429cce80",
   "metadata": {
    "papermill": {
     "duration": 0.006715,
     "end_time": "2026-01-08T17:46:12.228155",
     "exception": false,
     "start_time": "2026-01-08T17:46:12.221440",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Valuation Analysis - Trading Comparables\n",
    "\n",
    "We'll analyze AIRO's valuation relative to peers using multiple frameworks. This is the cornerstone of relative valuation methodology used by Goldman Sachs and other bulge bracket investment banks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84fc8c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T17:46:12.243489Z",
     "iopub.status.busy": "2026-01-08T17:46:12.243093Z",
     "iopub.status.idle": "2026-01-08T17:46:12.270388Z",
     "shell.execute_reply": "2026-01-08T17:46:12.269074Z"
    },
    "papermill": {
     "duration": 0.037834,
     "end_time": "2026-01-08T17:46:12.272749",
     "exception": false,
     "start_time": "2026-01-08T17:46:12.234915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create valuation comparison focusing on key multiples\n",
    "valuation_cols = ['Ticker', 'Company', 'Category', 'Market Cap (M)', 'P/S', 'EV/Sales', 'P/B', 'P/E']\n",
    "df_valuation = df_metrics[valuation_cols].copy()\n",
    "\n",
    "# Calculate peer group statistics for relative valuation analysis\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"VALUATION MULTIPLES ANALYSIS\")\n",
    "print(\"=\"*100)\n",
    "print(df_valuation.to_string(index=False))\n",
    "\n",
    "# Peer group averages by category\n",
    "print(\"\\n\" + \"-\"*100)\n",
    "print(\"PEER GROUP AVERAGES BY CATEGORY\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "peer_averages = df_metrics.groupby('Category')[['P/S', 'EV/Sales', 'P/B', 'Revenue Growth %', 'Gross Margin %']].mean()\n",
    "print(peer_averages)\n",
    "\n",
    "# AIRO's position relative to each peer group\n",
    "airo_metrics = df_metrics[df_metrics['Ticker'] == 'AIRO'].iloc[0]\n",
    "\n",
    "print(\"\\n\" + \"-\"*100)\n",
    "print(\"AIRO RELATIVE VALUATION POSITIONING\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "for category in peer_averages.index:\n",
    "    if category != 'Diversified':\n",
    "        print(f\"\\nVs. {category} Peers:\")\n",
    "        for metric in ['P/S', 'EV/Sales']:\n",
    "            airo_val = airo_metrics[metric]\n",
    "            peer_avg = peer_averages.loc[category, metric]\n",
    "            if not np.isnan(airo_val) and not np.isnan(peer_avg):\n",
    "                discount_premium = ((airo_val / peer_avg) - 1) * 100\n",
    "                status = \"PREMIUM\" if discount_premium > 0 else \"DISCOUNT\"\n",
    "                print(f\"  {metric:12} | AIRO: {airo_val:.2f}x | Peer Avg: {peer_avg:.2f}x | {discount_premium:+.1f}% {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e33f6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T17:46:12.289082Z",
     "iopub.status.busy": "2026-01-08T17:46:12.288063Z",
     "iopub.status.idle": "2026-01-08T17:46:13.369474Z",
     "shell.execute_reply": "2026-01-08T17:46:13.368481Z"
    },
    "papermill": {
     "duration": 1.094159,
     "end_time": "2026-01-08T17:46:13.374082",
     "exception": false,
     "start_time": "2026-01-08T17:46:12.279923",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualization 1: Valuation Multiples Comparison\n",
    "# This chart provides immediate visual insight into relative valuation positioning\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Comparative Valuation Analysis - AIRO vs. Peers', fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "# Plot 1: EV/Sales Multiple Comparison\n",
    "ax1 = axes[0, 0]\n",
    "df_plot = df_metrics.dropna(subset=['EV/Sales']).sort_values('EV/Sales')\n",
    "colors = ['#FF4444' if x == 'AIRO' else '#4444FF' for x in df_plot['Ticker']]\n",
    "bars1 = ax1.barh(df_plot['Ticker'], df_plot['EV/Sales'], color=colors, alpha=0.7, edgecolor='black')\n",
    "ax1.set_xlabel('EV/Sales Multiple', fontweight='bold')\n",
    "ax1.set_title('Enterprise Value / Sales Ratio', fontweight='bold')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "for i, (ticker, val) in enumerate(zip(df_plot['Ticker'], df_plot['EV/Sales'])):\n",
    "    ax1.text(val + 0.1, i, f'{val:.2f}x', va='center', fontweight='bold')\n",
    "\n",
    "# Plot 2: Price/Sales Multiple Comparison\n",
    "ax2 = axes[0, 1]\n",
    "df_plot = df_metrics.dropna(subset=['P/S']).sort_values('P/S')\n",
    "colors = ['#FF4444' if x == 'AIRO' else '#44AA44' for x in df_plot['Ticker']]\n",
    "bars2 = ax2.barh(df_plot['Ticker'], df_plot['P/S'], color=colors, alpha=0.7, edgecolor='black')\n",
    "ax2.set_xlabel('P/S Multiple', fontweight='bold')\n",
    "ax2.set_title('Price / Sales Ratio', fontweight='bold')\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "for i, (ticker, val) in enumerate(zip(df_plot['Ticker'], df_plot['P/S'])):\n",
    "    ax2.text(val + 0.1, i, f'{val:.2f}x', va='center', fontweight='bold')\n",
    "\n",
    "# Plot 3: Market Cap Comparison (Log Scale)\n",
    "ax3 = axes[1, 0]\n",
    "df_plot = df_metrics.dropna(subset=['Market Cap (M)']).sort_values('Market Cap (M)')\n",
    "colors = ['#FF4444' if x == 'AIRO' else '#AA44AA' for x in df_plot['Ticker']]\n",
    "bars3 = ax3.barh(df_plot['Ticker'], df_plot['Market Cap (M)'], color=colors, alpha=0.7, edgecolor='black')\n",
    "ax3.set_xlabel('Market Capitalization ($M)', fontweight='bold')\n",
    "ax3.set_title('Market Capitalization Comparison (Log Scale)', fontweight='bold')\n",
    "ax3.set_xscale('log')\n",
    "ax3.grid(axis='x', alpha=0.3, which='both')\n",
    "for i, (ticker, val) in enumerate(zip(df_plot['Ticker'], df_plot['Market Cap (M)'])):\n",
    "    ax3.text(val * 1.2, i, f'${val:,.0f}M', va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Plot 4: Gross Margin vs EV/Sales (Efficiency-Valuation Matrix)\n",
    "ax4 = axes[1, 1]\n",
    "df_plot = df_metrics.dropna(subset=['Gross Margin %', 'EV/Sales'])\n",
    "for idx, row in df_plot.iterrows():\n",
    "    color = '#FF4444' if row['Ticker'] == 'AIRO' else '#4444FF'\n",
    "    size = 200 if row['Ticker'] == 'AIRO' else 100\n",
    "    ax4.scatter(row['Gross Margin %'], row['EV/Sales'], s=size, color=color, alpha=0.6, edgecolor='black')\n",
    "    ax4.annotate(row['Ticker'], (row['Gross Margin %'], row['EV/Sales']), \n",
    "                fontweight='bold', fontsize=10, ha='center', va='bottom')\n",
    "ax4.set_xlabel('Gross Margin (%)', fontweight='bold')\n",
    "ax4.set_ylabel('EV/Sales Multiple', fontweight='bold')\n",
    "ax4.set_title('Profitability vs Valuation Matrix', fontweight='bold')\n",
    "ax4.grid(alpha=0.3)\n",
    "ax4.axhline(df_metrics['EV/Sales'].median(), color='gray', linestyle='--', alpha=0.5, label='Median EV/Sales')\n",
    "ax4.axvline(df_metrics['Gross Margin %'].median(), color='gray', linestyle='--', alpha=0.5, label='Median Gross Margin')\n",
    "ax4.legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Valuation comparison charts generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b44c78",
   "metadata": {
    "papermill": {
     "duration": 0.008699,
     "end_time": "2026-01-08T17:46:13.393059",
     "exception": false,
     "start_time": "2026-01-08T17:46:13.384360",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Stock Price Performance Analysis\n",
    "\n",
    "Technical analysis of price momentum, relative performance, and volatility patterns. This section provides insight into market sentiment and trading dynamics for each security."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac01747",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T17:46:13.414101Z",
     "iopub.status.busy": "2026-01-08T17:46:13.412559Z",
     "iopub.status.idle": "2026-01-08T17:46:13.443187Z",
     "shell.execute_reply": "2026-01-08T17:46:13.441758Z"
    },
    "papermill": {
     "duration": 0.043738,
     "end_time": "2026-01-08T17:46:13.445708",
     "exception": false,
     "start_time": "2026-01-08T17:46:13.401970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate performance metrics for different time periods\n",
    "def calculate_returns(price_series, periods=[5, 20, 60, 120, 252]):\n",
    "    \"\"\"\n",
    "    Calculate returns over multiple periods (trading days):\n",
    "    5d = 1 week, 20d = 1 month, 60d = 3 months, 120d = 6 months, 252d = 1 year\n",
    "    \"\"\"\n",
    "    returns = {}\n",
    "    for period in periods:\n",
    "        if len(price_series) >= period:\n",
    "            returns[f'{period}d'] = ((price_series.iloc[-1] / price_series.iloc[-period]) - 1) * 100\n",
    "        else:\n",
    "            returns[f'{period}d'] = np.nan\n",
    "    return returns\n",
    "\n",
    "def calculate_volatility(price_series, window=20):\n",
    "    \"\"\"Calculate annualized volatility from daily returns\"\"\"\n",
    "    if len(price_series) >= window:\n",
    "        returns = price_series.pct_change().dropna()\n",
    "        return returns.std() * np.sqrt(252) * 100  # Annualized volatility\n",
    "    return np.nan\n",
    "\n",
    "# Build performance metrics dataframe\n",
    "performance_data = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    if len(price_data[ticker]) > 0:\n",
    "        close_prices = price_data[ticker]['Close']\n",
    "        returns = calculate_returns(close_prices)\n",
    "        volatility = calculate_volatility(close_prices, window=60)\n",
    "        \n",
    "        # Current price levels\n",
    "        current_price = close_prices.iloc[-1]\n",
    "        high_52w = close_prices[-252:].max() if len(close_prices) >= 252 else close_prices.max()\n",
    "        low_52w = close_prices[-252:].min() if len(close_prices) >= 252 else close_prices.min()\n",
    "        \n",
    "        performance_data.append({\n",
    "            'Ticker': ticker,\n",
    "            'Company': companies[ticker]['name'],\n",
    "            'Current Price': current_price,\n",
    "            '52W High': high_52w,\n",
    "            '52W Low': low_52w,\n",
    "            '% Off High': ((current_price / high_52w) - 1) * 100,\n",
    "            '1W Return %': returns.get('5d', np.nan),\n",
    "            '1M Return %': returns.get('20d', np.nan),\n",
    "            '3M Return %': returns.get('60d', np.nan),\n",
    "            '6M Return %': returns.get('120d', np.nan),\n",
    "            '1Y Return %': returns.get('252d', np.nan),\n",
    "            'Volatility % (Ann.)': volatility\n",
    "        })\n",
    "\n",
    "df_performance = pd.DataFrame(performance_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"PRICE PERFORMANCE & MOMENTUM ANALYSIS\")\n",
    "print(\"=\"*120)\n",
    "print(df_performance.to_string(index=False))\n",
    "print(\"=\"*120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6565bb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T17:46:13.464781Z",
     "iopub.status.busy": "2026-01-08T17:46:13.464416Z",
     "iopub.status.idle": "2026-01-08T17:46:18.197190Z",
     "shell.execute_reply": "2026-01-08T17:46:18.195998Z"
    },
    "papermill": {
     "duration": 4.750607,
     "end_time": "2026-01-08T17:46:18.205382",
     "exception": false,
     "start_time": "2026-01-08T17:46:13.454775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualization 2: Stock Price Charts with Technical Indicators\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(18, 14))\n",
    "fig.suptitle('Historical Price Performance & Technical Analysis', fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "for idx, ticker in enumerate(tickers):\n",
    "    row = idx // 2\n",
    "    col = idx % 2\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    if len(price_data[ticker]) > 0:\n",
    "        df = price_data[ticker].copy()\n",
    "        \n",
    "        # Calculate moving averages for trend identification\n",
    "        df['MA20'] = df['Close'].rolling(window=20).mean()  # Short-term trend\n",
    "        df['MA50'] = df['Close'].rolling(window=50).mean()  # Medium-term trend\n",
    "        df['MA200'] = df['Close'].rolling(window=200).mean()  # Long-term trend\n",
    "        \n",
    "        # Plot price with moving averages\n",
    "        color = '#FF4444' if ticker == 'AIRO' else '#4444FF'\n",
    "        ax.plot(df.index, df['Close'], linewidth=2, color=color, label='Close Price', alpha=0.8)\n",
    "        ax.plot(df.index, df['MA20'], linewidth=1.5, color='orange', label='MA20', alpha=0.7, linestyle='--')\n",
    "        ax.plot(df.index, df['MA50'], linewidth=1.5, color='green', label='MA50', alpha=0.7, linestyle='--')\n",
    "        \n",
    "        # Add volume on secondary axis\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.bar(df.index, df['Volume'], alpha=0.2, color='gray', label='Volume')\n",
    "        ax2.set_ylabel('Volume', fontsize=9)\n",
    "        ax2.tick_params(labelsize=8)\n",
    "        \n",
    "        # Formatting\n",
    "        ax.set_title(f\"{ticker} - {companies[ticker]['name']}\", fontweight='bold', fontsize=12)\n",
    "        ax.set_ylabel('Price ($)', fontweight='bold')\n",
    "        ax.legend(loc='upper left', fontsize=8)\n",
    "        ax.grid(alpha=0.3)\n",
    "        ax.tick_params(labelsize=8)\n",
    "        \n",
    "        # Add performance annotation\n",
    "        perf_1y = df_performance[df_performance['Ticker'] == ticker]['1Y Return %'].values[0]\n",
    "        if not np.isnan(perf_1y):\n",
    "            color_text = 'green' if perf_1y > 0 else 'red'\n",
    "            ax.text(0.02, 0.98, f'1Y: {perf_1y:+.1f}%', transform=ax.transAxes,\n",
    "                   fontsize=10, fontweight='bold', color=color_text,\n",
    "                   verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, f'No data available for {ticker}', \n",
    "               ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "        ax.axis('off')\n",
    "\n",
    "# Remove empty subplot if odd number of stocks\n",
    "if len(tickers) % 2 != 0:\n",
    "    fig.delaxes(axes[-1, -1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Price performance charts generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8154e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T17:46:18.235345Z",
     "iopub.status.busy": "2026-01-08T17:46:18.234401Z",
     "iopub.status.idle": "2026-01-08T17:46:18.983440Z",
     "shell.execute_reply": "2026-01-08T17:46:18.982219Z"
    },
    "papermill": {
     "duration": 0.771657,
     "end_time": "2026-01-08T17:46:18.990477",
     "exception": false,
     "start_time": "2026-01-08T17:46:18.218820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualization 3: Relative Performance Comparison (Indexed to 100)\n",
    "# This shows how each stock performed relative to each other from a common starting point\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 12))\n",
    "fig.suptitle('Relative Performance Analysis - Indexed to 100', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Find common date range across all stocks\n",
    "common_dates = None\n",
    "for ticker in tickers:\n",
    "    if len(price_data[ticker]) > 0:\n",
    "        if common_dates is None:\n",
    "            common_dates = price_data[ticker].index\n",
    "        else:\n",
    "            common_dates = common_dates.intersection(price_data[ticker].index)\n",
    "\n",
    "# Plot 1: Full period indexed performance\n",
    "for ticker in tickers:\n",
    "    if len(price_data[ticker]) > 0:\n",
    "        df = price_data[ticker].loc[common_dates].copy()\n",
    "        indexed = (df['Close'] / df['Close'].iloc[0]) * 100\n",
    "        \n",
    "        linewidth = 3 if ticker == 'AIRO' else 2\n",
    "        linestyle = '-' if ticker == 'AIRO' else '--'\n",
    "        alpha = 1.0 if ticker == 'AIRO' else 0.7\n",
    "        \n",
    "        ax1.plot(indexed.index, indexed, linewidth=linewidth, linestyle=linestyle, \n",
    "                alpha=alpha, label=f\"{ticker} ({companies[ticker]['name']})\")\n",
    "\n",
    "ax1.axhline(100, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
    "ax1.set_ylabel('Indexed Performance (Base = 100)', fontweight='bold')\n",
    "ax1.set_title('Full Period Relative Performance', fontweight='bold', fontsize=13)\n",
    "ax1.legend(loc='best', fontsize=9)\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Last 6 months detailed view\n",
    "recent_dates = common_dates[-120:] if len(common_dates) >= 120 else common_dates\n",
    "\n",
    "for ticker in tickers:\n",
    "    if len(price_data[ticker]) > 0:\n",
    "        df = price_data[ticker].loc[recent_dates].copy()\n",
    "        if len(df) > 0:\n",
    "            indexed = (df['Close'] / df['Close'].iloc[0]) * 100\n",
    "            \n",
    "            linewidth = 3 if ticker == 'AIRO' else 2\n",
    "            linestyle = '-' if ticker == 'AIRO' else '--'\n",
    "            alpha = 1.0 if ticker == 'AIRO' else 0.7\n",
    "            \n",
    "            ax2.plot(indexed.index, indexed, linewidth=linewidth, linestyle=linestyle,\n",
    "                    alpha=alpha, label=f\"{ticker}\")\n",
    "\n",
    "ax2.axhline(100, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
    "ax2.set_ylabel('Indexed Performance (Base = 100)', fontweight='bold')\n",
    "ax2.set_xlabel('Date', fontweight='bold')\n",
    "ax2.set_title('Recent 6-Month Performance (Detailed View)', fontweight='bold', fontsize=13)\n",
    "ax2.legend(loc='best', fontsize=9)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Relative performance charts generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d1b374",
   "metadata": {
    "papermill": {
     "duration": 0.017774,
     "end_time": "2026-01-08T17:46:19.027781",
     "exception": false,
     "start_time": "2026-01-08T17:46:19.010007",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Sum-of-Parts (SOTP) Valuation for AIRO\n",
    "\n",
    "AIRO operates four distinct business segments, each deserving different valuation multiples based on growth prospects, profitability, and competitive positioning. We'll value each segment separately and sum them to derive intrinsic value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd036e07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T17:46:19.064223Z",
     "iopub.status.busy": "2026-01-08T17:46:19.063703Z",
     "iopub.status.idle": "2026-01-08T17:46:19.089065Z",
     "shell.execute_reply": "2026-01-08T17:46:19.086960Z"
    },
    "papermill": {
     "duration": 0.0465,
     "end_time": "2026-01-08T17:46:19.091711",
     "exception": false,
     "start_time": "2026-01-08T17:46:19.045211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# AIRO Segment Revenue Estimates (Based on Q3 2025 Results and Management Guidance)\n",
    "# These estimates are derived from the earnings report and industry analysis\n",
    "\n",
    "airo_segment_data = {\n",
    "    'Segment': ['Drones (UAS)', 'Avionics', 'Training', 'Electric Air Mobility', 'Total'],\n",
    "    'Est. FY2025 Revenue ($M)': [60.0, 15.0, 12.0, 0.5, 87.5],  # FY2025 guidance >$86.9M\n",
    "    'Revenue %': [68.6, 17.1, 13.7, 0.6, 100.0],\n",
    "    'Est. Gross Margin %': [60, 50, 40, 30, 55],  # Segment-specific margins based on business model\n",
    "    'Growth Rate %': [40, 15, 10, 200, 35],  # Near-term growth expectations\n",
    "    'Comparable Multiple (EV/Sales)': [4.5, 2.0, 1.5, 0.0, np.nan],  # Based on peer analysis\n",
    "    'Applied Multiple Rationale': [\n",
    "        'Between AVAV (6x) and KTOS (2x), discounted for scale',\n",
    "        'Conservative avionics supplier multiple',\n",
    "        'Defense training services multiple',\n",
    "        'Pre-certification, assigned zero value (conservative)',\n",
    "        ''\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_sotp = pd.DataFrame(airo_segment_data)\n",
    "\n",
    "# Calculate segment valuations\n",
    "df_sotp['Segment Valuation ($M)'] = df_sotp['Est. FY2025 Revenue ($M)'] * df_sotp['Comparable Multiple (EV/Sales)']\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"AIRO - SUM-OF-PARTS VALUATION ANALYSIS\")\n",
    "print(\"=\"*120)\n",
    "print(df_sotp[['Segment', 'Est. FY2025 Revenue ($M)', 'Revenue %', 'Growth Rate %', \n",
    "               'Comparable Multiple (EV/Sales)', 'Segment Valuation ($M)']].to_string(index=False))\n",
    "print(\"=\"*120)\n",
    "\n",
    "# Calculate enterprise value and equity value\n",
    "total_ev = df_sotp['Segment Valuation ($M)'].iloc[:-1].sum()  # Exclude 'Total' row\n",
    "\n",
    "# Get AIRO's net cash position\n",
    "airo_info = stock_info['AIRO']\n",
    "airo_cash = safe_get(airo_info, 'totalCash', 120_000_000)  # Estimated ~$120M post-offerings\n",
    "airo_debt = safe_get(airo_info, 'totalDebt', 0)\n",
    "net_cash = airo_cash - airo_debt\n",
    "\n",
    "equity_value = total_ev + (net_cash / 1e6)\n",
    "\n",
    "# Calculate implied share price\n",
    "shares_outstanding = 31.3  # Million shares (from latest filings)\n",
    "implied_price_sotp = equity_value / shares_outstanding\n",
    "\n",
    "# Get current price for comparison\n",
    "current_price = df_metrics[df_metrics['Ticker'] == 'AIRO']['Current Price'].values[0]\n",
    "upside = ((implied_price_sotp / current_price) - 1) * 100\n",
    "\n",
    "print(\"\\n\" + \"-\"*120)\n",
    "print(\"VALUATION SUMMARY\")\n",
    "print(\"-\"*120)\n",
    "print(f\"Total Enterprise Value (Sum of Parts):    ${total_ev:,.1f}M\")\n",
    "print(f\"Net Cash Position:                        ${net_cash/1e6:,.1f}M\")\n",
    "print(f\"Implied Equity Value:                     ${equity_value:,.1f}M\")\n",
    "print(f\"Shares Outstanding:                       {shares_outstanding:.1f}M\")\n",
    "print(f\"\\nImplied Share Price (SOTP):               ${implied_price_sotp:.2f}\")\n",
    "print(f\"Current Market Price:                     ${current_price:.2f}\")\n",
    "print(f\"Implied Upside/Downside:                  {upside:+.1f}%\")\n",
    "print(\"-\"*120)\n",
    "\n",
    "# Sensitivity analysis - vary the drone segment multiple (key value driver)\n",
    "print(\"\\n\" + \"-\"*120)\n",
    "print(\"SENSITIVITY ANALYSIS - Drone Segment EV/Sales Multiple\")\n",
    "print(\"-\"*120)\n",
    "print(f\"{'Multiple':<15} {'Enterprise Value':<20} {'Equity Value':<20} {'Price/Share':<15} {'Upside %':<15}\")\n",
    "print(\"-\"*120)\n",
    "\n",
    "for multiple in [3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0]:\n",
    "    drone_value = 60.0 * multiple\n",
    "    other_value = df_sotp['Segment Valuation ($M)'].iloc[1:4].sum()\n",
    "    ev = drone_value + other_value\n",
    "    eq_val = ev + (net_cash / 1e6)\n",
    "    price = eq_val / shares_outstanding\n",
    "    upside_pct = ((price / current_price) - 1) * 100\n",
    "    print(f\"{multiple:.1f}x{'':<10} ${ev:>7,.0f}M{'':<10} ${eq_val:>7,.0f}M{'':<10} ${price:>6.2f}{'':<8} {upside_pct:>+6.1f}%\")\n",
    "\n",
    "print(\"-\"*120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a398a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T17:46:19.130968Z",
     "iopub.status.busy": "2026-01-08T17:46:19.130382Z",
     "iopub.status.idle": "2026-01-08T17:46:19.827154Z",
     "shell.execute_reply": "2026-01-08T17:46:19.825944Z"
    },
    "papermill": {
     "duration": 0.721527,
     "end_time": "2026-01-08T17:46:19.831225",
     "exception": false,
     "start_time": "2026-01-08T17:46:19.109698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualization 4: SOTP Valuation Breakdown\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('AIRO - Sum-of-Parts Valuation Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Revenue Breakdown by Segment (Pie Chart)\n",
    "segments = df_sotp['Segment'].iloc[:-1]\n",
    "revenue = df_sotp['Est. FY2025 Revenue ($M)'].iloc[:-1]\n",
    "colors_pie = plt.cm.Set3(range(len(segments)))\n",
    "wedges, texts, autotexts = ax1.pie(revenue, labels=segments, autopct='%1.1f%%', colors=colors_pie,\n",
    "                                     startangle=90, textprops={'fontsize': 10, 'fontweight': 'bold'})\n",
    "ax1.set_title('Revenue Mix by Segment (FY2025E)', fontweight='bold', fontsize=12)\n",
    "\n",
    "# Plot 2: Segment Valuation Contributions\n",
    "valuations = df_sotp['Segment Valuation ($M)'].iloc[:-1]\n",
    "colors_bar = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']\n",
    "bars = ax2.bar(segments, valuations, color=colors_bar, alpha=0.7, edgecolor='black')\n",
    "ax2.set_ylabel('Valuation ($M)', fontweight='bold')\n",
    "ax2.set_title('Segment Valuation Contribution', fontweight='bold', fontsize=12)\n",
    "ax2.tick_params(axis='x', rotation=15)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "for bar, val in zip(bars, valuations):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'${val:.0f}M', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "\n",
    "# Plot 3: Applied Multiples vs Peer Averages\n",
    "multiples_data = {\n",
    "    'Drones': {'AIRO Applied': 4.5, 'AVAV Actual': 6.0, 'KTOS Actual': 1.8},\n",
    "    'Avionics': {'AIRO Applied': 2.0, 'Industry Avg': 2.5},\n",
    "    'Training': {'AIRO Applied': 1.5, 'Defense Svc Avg': 2.0}\n",
    "}\n",
    "\n",
    "x = np.arange(len(multiples_data))\n",
    "width = 0.35\n",
    "segments_mult = list(multiples_data.keys())\n",
    "applied = [multiples_data[seg]['AIRO Applied'] for seg in segments_mult]\n",
    "comparables = [multiples_data['Drones']['AVAV Actual'], \n",
    "               multiples_data['Avionics']['Industry Avg'],\n",
    "               multiples_data['Training']['Defense Svc Avg']]\n",
    "\n",
    "bars1 = ax3.bar(x - width/2, applied, width, label='AIRO Applied', color='#FF6B6B', alpha=0.7, edgecolor='black')\n",
    "bars2 = ax3.bar(x + width/2, comparables, width, label='Comparable Avg', color='#4ECDC4', alpha=0.7, edgecolor='black')\n",
    "\n",
    "ax3.set_ylabel('EV/Sales Multiple', fontweight='bold')\n",
    "ax3.set_title('Applied Multiples vs Comparables', fontweight='bold', fontsize=12)\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(segments_mult)\n",
    "ax3.legend()\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.1f}x', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Plot 4: Valuation Bridge (Waterfall)\n",
    "categories = ['Drones', 'Avionics', 'Training', 'eVTOL', 'Net Cash', 'Total Equity Value']\n",
    "values = list(df_sotp['Segment Valuation ($M)'].iloc[:-1]) + [net_cash/1e6]\n",
    "values.append(equity_value)\n",
    "\n",
    "cumulative = [0]\n",
    "for i, val in enumerate(values[:-1]):\n",
    "    cumulative.append(cumulative[-1] + val)\n",
    "\n",
    "colors_waterfall = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#95E1D3', '#38A169']\n",
    "\n",
    "for i in range(len(categories)-1):\n",
    "    ax4.bar(i, values[i], bottom=cumulative[i], color=colors_waterfall[i], alpha=0.7, edgecolor='black')\n",
    "    ax4.text(i, cumulative[i] + values[i]/2, f'${values[i]:.0f}M',\n",
    "            ha='center', va='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "ax4.bar(len(categories)-1, equity_value, color=colors_waterfall[-1], alpha=0.9, edgecolor='black', linewidth=2)\n",
    "ax4.text(len(categories)-1, equity_value/2, f'${equity_value:.0f}M',\n",
    "        ha='center', va='center', fontweight='bold', fontsize=10, color='white')\n",
    "\n",
    "ax4.set_xticks(range(len(categories)))\n",
    "ax4.set_xticklabels(categories, rotation=15, ha='right')\n",
    "ax4.set_ylabel('Valuation ($M)', fontweight='bold')\n",
    "ax4.set_title('Valuation Bridge - Sum of Parts', fontweight='bold', fontsize=12)\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Sum-of-Parts valuation visualizations generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6872571",
   "metadata": {
    "papermill": {
     "duration": 0.021374,
     "end_time": "2026-01-08T17:46:19.875532",
     "exception": false,
     "start_time": "2026-01-08T17:46:19.854158",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. Risk-Adjusted Return Analysis\n",
    "\n",
    "We'll calculate risk metrics including volatility, beta, Sharpe ratios, and maximum drawdown to understand the risk profile of each investment. This is critical for portfolio construction and position sizing decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d688e13c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T17:46:19.918364Z",
     "iopub.status.busy": "2026-01-08T17:46:19.917990Z",
     "iopub.status.idle": "2026-01-08T17:46:19.969580Z",
     "shell.execute_reply": "2026-01-08T17:46:19.967600Z"
    },
    "papermill": {
     "duration": 0.075767,
     "end_time": "2026-01-08T17:46:19.972091",
     "exception": false,
     "start_time": "2026-01-08T17:46:19.896324",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_max_drawdown(price_series):\n",
    "    \"\"\"Calculate maximum peak-to-trough decline\"\"\"\n",
    "    cumulative = (1 + price_series.pct_change()).cumprod()\n",
    "    running_max = cumulative.expanding().max()\n",
    "    drawdown = (cumulative - running_max) / running_max\n",
    "    return drawdown.min() * 100\n",
    "\n",
    "def calculate_sharpe_ratio(returns, risk_free_rate=0.04):\n",
    "    \"\"\"Calculate Sharpe Ratio (assuming 4% risk-free rate)\"\"\"\n",
    "    excess_returns = returns - (risk_free_rate / 252)  # Daily risk-free rate\n",
    "    if len(excess_returns) > 0 and excess_returns.std() > 0:\n",
    "        return (excess_returns.mean() * 252) / (excess_returns.std() * np.sqrt(252))\n",
    "    return np.nan\n",
    "\n",
    "# Calculate comprehensive risk metrics\n",
    "risk_data = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    if len(price_data[ticker]) > 0:\n",
    "        df = price_data[ticker].copy()\n",
    "        close = df['Close']\n",
    "        returns = close.pct_change().dropna()\n",
    "        \n",
    "        # Risk metrics\n",
    "        volatility_daily = returns.std()\n",
    "        volatility_annual = volatility_daily * np.sqrt(252) * 100\n",
    "        max_dd = calculate_max_drawdown(close)\n",
    "        sharpe = calculate_sharpe_ratio(returns)\n",
    "        \n",
    "        # Return metrics\n",
    "        mean_daily_return = returns.mean()\n",
    "        mean_annual_return = mean_daily_return * 252 * 100\n",
    "        \n",
    "        # Get beta from stock info\n",
    "        beta = df_metrics[df_metrics['Ticker'] == ticker]['Beta'].values[0]\n",
    "        \n",
    "        risk_data.append({\n",
    "            'Ticker': ticker,\n",
    "            'Company': companies[ticker]['name'],\n",
    "            'Ann. Return %': mean_annual_return,\n",
    "            'Ann. Volatility %': volatility_annual,\n",
    "            'Max Drawdown %': max_dd,\n",
    "            'Sharpe Ratio': sharpe,\n",
    "            'Beta': beta,\n",
    "            'Risk/Return': volatility_annual / abs(mean_annual_return) if mean_annual_return != 0 else np.nan\n",
    "        })\n",
    "\n",
    "df_risk = pd.DataFrame(risk_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"RISK-ADJUSTED RETURN ANALYSIS\")\n",
    "print(\"=\"*100)\n",
    "print(df_risk.to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(\"\\n\" + \"-\"*100)\n",
    "print(\"RISK ASSESSMENT SUMMARY\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "# Rankings\n",
    "print(\"\\nRankings (Best to Worst):\")\n",
    "print(f\"\\nBest Sharpe Ratio:    {df_risk.nlargest(1, 'Sharpe Ratio')['Ticker'].values[0]}\")\n",
    "print(f\"Lowest Volatility:    {df_risk.nsmallest(1, 'Ann. Volatility %')['Ticker'].values[0]}\")\n",
    "print(f\"Smallest Drawdown:    {df_risk.nlargest(1, 'Max Drawdown %')['Ticker'].values[0]}\")\n",
    "print(f\"\\nWorst Sharpe Ratio:   {df_risk.nsmallest(1, 'Sharpe Ratio')['Ticker'].values[0]}\")\n",
    "print(f\"Highest Volatility:   {df_risk.nlargest(1, 'Ann. Volatility %')['Ticker'].values[0]}\")\n",
    "print(f\"Largest Drawdown:     {df_risk.nsmallest(1, 'Max Drawdown %')['Ticker'].values[0]}\")\n",
    "\n",
    "# AIRO-specific commentary\n",
    "airo_risk = df_risk[df_risk['Ticker'] == 'AIRO'].iloc[0]\n",
    "print(\"\\n\" + \"-\"*100)\n",
    "print(\"AIRO RISK PROFILE:\")\n",
    "print(\"-\"*100)\n",
    "print(f\"Volatility: {airo_risk['Ann. Volatility %']:.1f}% (peer median: {df_risk['Ann. Volatility %'].median():.1f}%)\")\n",
    "print(f\"Max Drawdown: {airo_risk['Max Drawdown %']:.1f}% (peer median: {df_risk['Max Drawdown %'].median():.1f}%)\")\n",
    "print(f\"Sharpe Ratio: {airo_risk['Sharpe Ratio']:.2f} (peer median: {df_risk['Sharpe Ratio'].median():.2f})\")\n",
    "print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35de49f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T17:46:20.014300Z",
     "iopub.status.busy": "2026-01-08T17:46:20.013876Z",
     "iopub.status.idle": "2026-01-08T17:46:20.817534Z",
     "shell.execute_reply": "2026-01-08T17:46:20.816437Z"
    },
    "papermill": {
     "duration": 0.829271,
     "end_time": "2026-01-08T17:46:20.820822",
     "exception": false,
     "start_time": "2026-01-08T17:46:19.991551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualization 5: Risk-Return Analysis\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Risk-Adjusted Return Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Risk-Return Scatter (Efficient Frontier View)\n",
    "for idx, row in df_risk.iterrows():\n",
    "    color = '#FF4444' if row['Ticker'] == 'AIRO' else '#4444FF'\n",
    "    size = 300 if row['Ticker'] == 'AIRO' else 150\n",
    "    ax1.scatter(row['Ann. Volatility %'], row['Ann. Return %'], s=size, color=color, \n",
    "               alpha=0.6, edgecolor='black', linewidth=2)\n",
    "    ax1.annotate(row['Ticker'], (row['Ann. Volatility %'], row['Ann. Return %']),\n",
    "                fontweight='bold', fontsize=11, ha='center', va='bottom')\n",
    "\n",
    "ax1.axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax1.axvline(df_risk['Ann. Volatility %'].median(), color='gray', linestyle='--', alpha=0.5, label='Median Vol')\n",
    "ax1.set_xlabel('Annualized Volatility (%)', fontweight='bold')\n",
    "ax1.set_ylabel('Annualized Return (%)', fontweight='bold')\n",
    "ax1.set_title('Risk-Return Profile', fontweight='bold', fontsize=13)\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Sharpe Ratio Comparison\n",
    "df_plot = df_risk.sort_values('Sharpe Ratio')\n",
    "colors = ['#FF4444' if x == 'AIRO' else '#4444FF' for x in df_plot['Ticker']]\n",
    "bars = ax2.barh(df_plot['Ticker'], df_plot['Sharpe Ratio'], color=colors, alpha=0.7, edgecolor='black')\n",
    "ax2.set_xlabel('Sharpe Ratio', fontweight='bold')\n",
    "ax2.set_title('Risk-Adjusted Returns (Sharpe Ratio)', fontweight='bold', fontsize=13)\n",
    "ax2.axvline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "for i, (ticker, val) in enumerate(zip(df_plot['Ticker'], df_plot['Sharpe Ratio'])):\n",
    "    ax2.text(val + 0.05, i, f'{val:.2f}', va='center', fontweight='bold')\n",
    "\n",
    "# Plot 3: Maximum Drawdown Comparison\n",
    "df_plot = df_risk.sort_values('Max Drawdown %', ascending=False)\n",
    "colors = ['#FF4444' if x == 'AIRO' else '#44AA44' for x in df_plot['Ticker']]\n",
    "bars = ax3.barh(df_plot['Ticker'], df_plot['Max Drawdown %'], color=colors, alpha=0.7, edgecolor='black')\n",
    "ax3.set_xlabel('Maximum Drawdown (%)', fontweight='bold')\n",
    "ax3.set_title('Worst Peak-to-Trough Decline', fontweight='bold', fontsize=13)\n",
    "ax3.grid(axis='x', alpha=0.3)\n",
    "for i, (ticker, val) in enumerate(zip(df_plot['Ticker'], df_plot['Max Drawdown %'])):\n",
    "    ax3.text(val - 2, i, f'{val:.1f}%', va='center', fontweight='bold', ha='right')\n",
    "\n",
    "# Plot 4: Beta Comparison (Market Sensitivity)\n",
    "df_plot = df_risk.dropna(subset=['Beta']).sort_values('Beta')\n",
    "colors = ['#FF4444' if x == 'AIRO' else '#AA44AA' for x in df_plot['Ticker']]\n",
    "bars = ax4.barh(df_plot['Ticker'], df_plot['Beta'], color=colors, alpha=0.7, edgecolor='black')\n",
    "ax4.set_xlabel('Beta (Market Sensitivity)', fontweight='bold')\n",
    "ax4.set_title('Systematic Risk (Beta vs S&P 500)', fontweight='bold', fontsize=13)\n",
    "ax4.axvline(1.0, color='red', linestyle='--', alpha=0.5, linewidth=2, label='Market Beta = 1.0')\n",
    "ax4.legend(fontsize=9)\n",
    "ax4.grid(axis='x', alpha=0.3)\n",
    "for i, (ticker, val) in enumerate(zip(df_plot['Ticker'], df_plot['Beta'])):\n",
    "    if not np.isnan(val):\n",
    "        ax4.text(val + 0.05, i, f'{val:.2f}', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Risk analysis visualizations generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64fd798",
   "metadata": {
    "papermill": {
     "duration": 0.021439,
     "end_time": "2026-01-08T17:46:20.864641",
     "exception": false,
     "start_time": "2026-01-08T17:46:20.843202",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7. Comparative Valuation Dashboard\n",
    "\n",
    "A comprehensive side-by-side comparison of all key metrics to facilitate investment decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r4n521wbwd",
   "metadata": {},
   "source": [
    "## 7A. Advanced Technical Indicators\n",
    "\n",
    "Beyond basic moving averages, we'll implement comprehensive technical analysis including RSI, MACD, Bollinger Bands, VWAP, and On-Balance Volume (OBV). These indicators help identify momentum, overbought/oversold conditions, and buying/selling pressure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fua86ez9uqq",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Technical Indicators Implementation\n",
    "\n",
    "def calculate_rsi(prices, period=14):\n",
    "    \"\"\"Calculate Relative Strength Index\"\"\"\n",
    "    delta = prices.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
    "    rs = gain / loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "def calculate_macd(prices, fast=12, slow=26, signal=9):\n",
    "    \"\"\"Calculate MACD (Moving Average Convergence Divergence)\"\"\"\n",
    "    ema_fast = prices.ewm(span=fast).mean()\n",
    "    ema_slow = prices.ewm(span=slow).mean()\n",
    "    macd_line = ema_fast - ema_slow\n",
    "    signal_line = macd_line.ewm(span=signal).mean()\n",
    "    histogram = macd_line - signal_line\n",
    "    return macd_line, signal_line, histogram\n",
    "\n",
    "def calculate_bollinger_bands(prices, period=20, std_dev=2):\n",
    "    \"\"\"Calculate Bollinger Bands\"\"\"\n",
    "    sma = prices.rolling(window=period).mean()\n",
    "    std = prices.rolling(window=period).std()\n",
    "    upper_band = sma + (std * std_dev)\n",
    "    lower_band = sma - (std * std_dev)\n",
    "    return upper_band, sma, lower_band\n",
    "\n",
    "def calculate_vwap(df):\n",
    "    \"\"\"Calculate Volume Weighted Average Price\"\"\"\n",
    "    if len(df) > 0 and 'Volume' in df.columns:\n",
    "        return (df['Close'] * df['Volume']).cumsum() / df['Volume'].cumsum()\n",
    "    return pd.Series(dtype=float)\n",
    "\n",
    "def calculate_obv(df):\n",
    "    \"\"\"Calculate On-Balance Volume\"\"\"\n",
    "    if len(df) > 0:\n",
    "        obv = [0]\n",
    "        for i in range(1, len(df)):\n",
    "            if df['Close'].iloc[i] > df['Close'].iloc[i-1]:\n",
    "                obv.append(obv[-1] + df['Volume'].iloc[i])\n",
    "            elif df['Close'].iloc[i] < df['Close'].iloc[i-1]:\n",
    "                obv.append(obv[-1] - df['Volume'].iloc[i])\n",
    "            else:\n",
    "                obv.append(obv[-1])\n",
    "        return pd.Series(obv, index=df.index)\n",
    "    return pd.Series(dtype=float)\n",
    "\n",
    "# Calculate technical indicators for all stocks\n",
    "technical_data = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    if len(price_data[ticker]) > 0:\n",
    "        df = price_data[ticker].copy()\n",
    "        \n",
    "        # Calculate all indicators\n",
    "        rsi = calculate_rsi(df['Close'])\n",
    "        macd_line, signal_line, macd_hist = calculate_macd(df['Close'])\n",
    "        bb_upper, bb_middle, bb_lower = calculate_bollinger_bands(df['Close'])\n",
    "        vwap = calculate_vwap(df)\n",
    "        obv = calculate_obv(df)\n",
    "        \n",
    "        # Get current values\n",
    "        current_rsi = rsi.iloc[-1] if not rsi.empty else np.nan\n",
    "        current_macd = macd_line.iloc[-1] if not macd_line.empty else np.nan\n",
    "        current_signal = signal_line.iloc[-1] if not signal_line.empty else np.nan\n",
    "        current_price = df['Close'].iloc[-1]\n",
    "        current_bb_upper = bb_upper.iloc[-1] if not bb_upper.empty else np.nan\n",
    "        current_bb_lower = bb_lower.iloc[-1] if not bb_lower.empty else np.nan\n",
    "        \n",
    "        # Calculate position within Bollinger Bands (0-100%)\n",
    "        bb_position = ((current_price - current_bb_lower) / (current_bb_upper - current_bb_lower) * 100) if not np.isnan(current_bb_upper) else np.nan\n",
    "        \n",
    "        # Determine signals\n",
    "        rsi_signal = 'Oversold' if current_rsi < 30 else 'Overbought' if current_rsi > 70 else 'Neutral'\n",
    "        macd_signal = 'Bullish' if current_macd > current_signal else 'Bearish'\n",
    "        bb_signal = 'Overbought' if bb_position > 80 else 'Oversold' if bb_position < 20 else 'Neutral'\n",
    "        \n",
    "        technical_data.append({\n",
    "            'Ticker': ticker,\n",
    "            'Company': companies[ticker]['name'],\n",
    "            'Current Price': current_price,\n",
    "            'RSI (14)': current_rsi,\n",
    "            'RSI Signal': rsi_signal,\n",
    "            'MACD': current_macd,\n",
    "            'MACD Signal Line': current_signal,\n",
    "            'MACD Signal': macd_signal,\n",
    "            'BB Upper': current_bb_upper,\n",
    "            'BB Lower': current_bb_lower,\n",
    "            'BB Position %': bb_position,\n",
    "            'BB Signal': bb_signal,\n",
    "            'OBV Trend': 'Bullish' if obv.iloc[-20:].is_monotonic_increasing else 'Bearish' if obv.iloc[-20:].is_monotonic_decreasing else 'Mixed'\n",
    "        })\n",
    "\n",
    "df_technical = pd.DataFrame(technical_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"ADVANCED TECHNICAL ANALYSIS INDICATORS\")\n",
    "print(\"=\"*120)\n",
    "print(df_technical[['Ticker', 'Current Price', 'RSI (14)', 'RSI Signal', 'MACD Signal', 'BB Position %', 'BB Signal', 'OBV Trend']].to_string(index=False))\n",
    "print(\"=\"*120)\n",
    "\n",
    "# Visualization: Technical Indicators Dashboard\n",
    "fig, axes = plt.subplots(len(tickers), 4, figsize=(20, 4*len(tickers)))\n",
    "fig.suptitle('Technical Indicators Dashboard - All Companies', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, ticker in enumerate(tickers):\n",
    "    if len(price_data[ticker]) > 0:\n",
    "        df = price_data[ticker].copy()\n",
    "        \n",
    "        # Calculate indicators\n",
    "        rsi = calculate_rsi(df['Close'])\n",
    "        macd_line, signal_line, macd_hist = calculate_macd(df['Close'])\n",
    "        bb_upper, bb_middle, bb_lower = calculate_bollinger_bands(df['Close'])\n",
    "        obv = calculate_obv(df)\n",
    "        \n",
    "        # Plot 1: Price with Bollinger Bands\n",
    "        ax1 = axes[idx, 0] if len(tickers) > 1 else axes[0]\n",
    "        ax1.plot(df.index[-120:], df['Close'].iloc[-120:], label='Close', linewidth=2, color='blue')\n",
    "        ax1.plot(df.index[-120:], bb_upper.iloc[-120:], 'r--', alpha=0.5, label='BB Upper')\n",
    "        ax1.plot(df.index[-120:], bb_middle.iloc[-120:], 'g--', alpha=0.5, label='BB Middle')\n",
    "        ax1.plot(df.index[-120:], bb_lower.iloc[-120:], 'r--', alpha=0.5, label='BB Lower')\n",
    "        ax1.fill_between(df.index[-120:], bb_upper.iloc[-120:], bb_lower.iloc[-120:], alpha=0.1, color='gray')\n",
    "        ax1.set_title(f'{ticker} - Bollinger Bands', fontweight='bold', fontsize=10)\n",
    "        ax1.legend(fontsize=8)\n",
    "        ax1.grid(alpha=0.3)\n",
    "        ax1.tick_params(labelsize=8)\n",
    "        \n",
    "        # Plot 2: RSI\n",
    "        ax2 = axes[idx, 1] if len(tickers) > 1 else axes[1]\n",
    "        ax2.plot(df.index[-120:], rsi.iloc[-120:], linewidth=2, color='purple')\n",
    "        ax2.axhline(70, color='red', linestyle='--', alpha=0.5, label='Overbought')\n",
    "        ax2.axhline(30, color='green', linestyle='--', alpha=0.5, label='Oversold')\n",
    "        ax2.fill_between(df.index[-120:], 70, 100, alpha=0.1, color='red')\n",
    "        ax2.fill_between(df.index[-120:], 0, 30, alpha=0.1, color='green')\n",
    "        ax2.set_title(f'{ticker} - RSI (14)', fontweight='bold', fontsize=10)\n",
    "        ax2.set_ylim(0, 100)\n",
    "        ax2.legend(fontsize=8)\n",
    "        ax2.grid(alpha=0.3)\n",
    "        ax2.tick_params(labelsize=8)\n",
    "        \n",
    "        # Plot 3: MACD\n",
    "        ax3 = axes[idx, 2] if len(tickers) > 1 else axes[2]\n",
    "        ax3.plot(df.index[-120:], macd_line.iloc[-120:], label='MACD', linewidth=2, color='blue')\n",
    "        ax3.plot(df.index[-120:], signal_line.iloc[-120:], label='Signal', linewidth=2, color='red')\n",
    "        ax3.bar(df.index[-120:], macd_hist.iloc[-120:], label='Histogram', alpha=0.3, color='gray')\n",
    "        ax3.axhline(0, color='black', linestyle='-', alpha=0.3)\n",
    "        ax3.set_title(f'{ticker} - MACD', fontweight='bold', fontsize=10)\n",
    "        ax3.legend(fontsize=8)\n",
    "        ax3.grid(alpha=0.3)\n",
    "        ax3.tick_params(labelsize=8)\n",
    "        \n",
    "        # Plot 4: OBV\n",
    "        ax4 = axes[idx, 3] if len(tickers) > 1 else axes[3]\n",
    "        ax4.plot(df.index[-120:], obv.iloc[-120:], linewidth=2, color='orange')\n",
    "        ax4.set_title(f'{ticker} - On-Balance Volume', fontweight='bold', fontsize=10)\n",
    "        ax4.grid(alpha=0.3)\n",
    "        ax4.tick_params(labelsize=8)\n",
    "        ax4.ticklabel_format(style='plain', axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Technical indicators analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n7ldoj4vv0l",
   "metadata": {},
   "source": [
    "## 7B. DCF Valuation & Fundamental Ratios\n",
    "\n",
    "Discounted Cash Flow (DCF) valuation is the cornerstone of intrinsic value analysis. We'll also calculate PEG ratios, Free Cash Flow metrics, and fundamental quality indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "co30ssw6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCF Valuation and Advanced Fundamental Metrics\n",
    "\n",
    "def calculate_dcf_value(fcf, growth_rate, wacc, terminal_growth, years=5):\n",
    "    \"\"\"\n",
    "    Simple DCF Model\n",
    "    fcf: Current Free Cash Flow\n",
    "    growth_rate: Expected FCF growth rate for projection period\n",
    "    wacc: Weighted Average Cost of Capital (discount rate)\n",
    "    terminal_growth: Perpetual growth rate\n",
    "    years: Projection period\n",
    "    \"\"\"\n",
    "    if fcf <= 0 or np.isnan(fcf):\n",
    "        return np.nan\n",
    "    \n",
    "    # Project future cash flows\n",
    "    projected_fcf = []\n",
    "    for year in range(1, years + 1):\n",
    "        projected_fcf.append(fcf * ((1 + growth_rate) ** year))\n",
    "    \n",
    "    # Calculate present value of projected cash flows\n",
    "    pv_fcf = sum([cf / ((1 + wacc) ** (i+1)) for i, cf in enumerate(projected_fcf)])\n",
    "    \n",
    "    # Calculate terminal value\n",
    "    terminal_fcf = projected_fcf[-1] * (1 + terminal_growth)\n",
    "    terminal_value = terminal_fcf / (wacc - terminal_growth)\n",
    "    pv_terminal = terminal_value / ((1 + wacc) ** years)\n",
    "    \n",
    "    # Enterprise value\n",
    "    enterprise_value = pv_fcf + pv_terminal\n",
    "    \n",
    "    return enterprise_value\n",
    "\n",
    "# Calculate advanced fundamental metrics\n",
    "dcf_data = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    info = stock_info[ticker]\n",
    "    \n",
    "    # Get financial data\n",
    "    operating_cf = safe_get(info, 'operatingCashflow', np.nan)\n",
    "    capex = safe_get(info, 'capitalExpenditures', 0)\n",
    "    fcf = operating_cf - abs(capex) if not np.isnan(operating_cf) else np.nan\n",
    "    \n",
    "    # Get balance sheet data\n",
    "    total_assets = safe_get(info, 'totalAssets', np.nan)\n",
    "    total_equity = safe_get(info, 'totalStockholderEquity', np.nan)\n",
    "    total_debt = safe_get(info, 'totalDebt', 0)\n",
    "    cash = safe_get(info, 'totalCash', 0)\n",
    "    \n",
    "    # Revenue and earnings\n",
    "    revenue = safe_get(info, 'totalRevenue', np.nan)\n",
    "    ebitda = safe_get(info, 'ebitda', np.nan)\n",
    "    net_income = safe_get(info, 'netIncomeToCommon', np.nan)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    fcf_margin = (fcf / revenue * 100) if revenue and fcf and revenue > 0 else np.nan\n",
    "    fcf_yield = (fcf / df_metrics[df_metrics['Ticker']==ticker]['Market Cap (M)'].values[0] / 1e6 * 100) if fcf and not np.isnan(fcf) else np.nan\n",
    "    \n",
    "    # ROIC = NOPAT / Invested Capital\n",
    "    # Simplified: EBITDA * (1-tax_rate) / (Equity + Debt - Cash)\n",
    "    tax_rate = 0.21  # Assumed corporate tax rate\n",
    "    nopat = ebitda * (1 - tax_rate) if ebitda and not np.isnan(ebitda) else np.nan\n",
    "    invested_capital = total_equity + total_debt - cash if total_equity and not np.isnan(total_equity) else np.nan\n",
    "    roic = (nopat / invested_capital * 100) if invested_capital and nopat and invested_capital > 0 else np.nan\n",
    "    \n",
    "    # ROE = Net Income / Equity\n",
    "    roe = (net_income / total_equity * 100) if total_equity and net_income and total_equity > 0 else np.nan\n",
    "    \n",
    "    # PEG Ratio = P/E / Growth Rate\n",
    "    pe = df_metrics[df_metrics['Ticker']==ticker]['P/E'].values[0]\n",
    "    growth_rate = df_metrics[df_metrics['Ticker']==ticker]['Revenue Growth %'].values[0]\n",
    "    peg = (pe / growth_rate) if not np.isnan(pe) and not np.isnan(growth_rate) and growth_rate > 0 else np.nan\n",
    "    \n",
    "    # DCF Valuation (simplified)\n",
    "    # Assumptions: 15% growth for 5 years, 10% WACC, 3% terminal growth\n",
    "    if fcf and not np.isnan(fcf) and fcf > 0:\n",
    "        dcf_ev = calculate_dcf_value(fcf, 0.15, 0.10, 0.03, 5) / 1e6  # Convert to millions\n",
    "        dcf_equity_value = dcf_ev + (cash - total_debt) / 1e6\n",
    "        shares = safe_get(info, 'sharesOutstanding', np.nan)\n",
    "        dcf_price = (dcf_equity_value * 1e6 / shares) if shares and shares > 0 else np.nan\n",
    "    else:\n",
    "        dcf_ev = np.nan\n",
    "        dcf_price = np.nan\n",
    "    \n",
    "    current_price = df_metrics[df_metrics['Ticker']==ticker]['Current Price'].values[0]\n",
    "    dcf_upside = ((dcf_price / current_price - 1) * 100) if dcf_price and not np.isnan(dcf_price) and current_price else np.nan\n",
    "    \n",
    "    dcf_data.append({\n",
    "        'Ticker': ticker,\n",
    "        'Company': companies[ticker]['name'],\n",
    "        'FCF ($M)': fcf / 1e6 if fcf and not np.isnan(fcf) else np.nan,\n",
    "        'FCF Margin %': fcf_margin,\n",
    "        'FCF Yield %': fcf_yield,\n",
    "        'ROIC %': roic,\n",
    "        'ROE %': roe,\n",
    "        'PEG Ratio': peg,\n",
    "        'DCF Price': dcf_price,\n",
    "        'Current Price': current_price,\n",
    "        'DCF Upside %': dcf_upside\n",
    "    })\n",
    "\n",
    "df_dcf = pd.DataFrame(dcf_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"DCF VALUATION & FUNDAMENTAL QUALITY METRICS\")\n",
    "print(\"=\"*120)\n",
    "print(df_dcf.to_string(index=False))\n",
    "print(\"=\"*120)\n",
    "\n",
    "# Visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('DCF & Fundamental Quality Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: ROIC vs ROE\n",
    "df_plot = df_dcf.dropna(subset=['ROIC %', 'ROE %'])\n",
    "for idx, row in df_plot.iterrows():\n",
    "    color = '#FF4444' if row['Ticker'] == 'AIRO' else '#4444FF'\n",
    "    size = 300 if row['Ticker'] == 'AIRO' else 150\n",
    "    ax1.scatter(row['ROE %'], row['ROIC %'], s=size, color=color, alpha=0.6, edgecolor='black', linewidth=2)\n",
    "    ax1.annotate(row['Ticker'], (row['ROE %'], row['ROIC %']), fontweight='bold', fontsize=11, ha='center', va='bottom')\n",
    "ax1.axhline(15, color='green', linestyle='--', alpha=0.5, label='Target ROIC 15%')\n",
    "ax1.axvline(15, color='green', linestyle='--', alpha=0.5, label='Target ROE 15%')\n",
    "ax1.set_xlabel('Return on Equity (ROE) %', fontweight='bold')\n",
    "ax1.set_ylabel('Return on Invested Capital (ROIC) %', fontweight='bold')\n",
    "ax1.set_title('Capital Efficiency: ROIC vs ROE', fontweight='bold', fontsize=13)\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: PEG Ratio Comparison\n",
    "df_plot = df_dcf.dropna(subset=['PEG Ratio']).sort_values('PEG Ratio')\n",
    "colors = ['#FF4444' if x == 'AIRO' else '#44AA44' for x in df_plot['Ticker']]\n",
    "bars = ax2.barh(df_plot['Ticker'], df_plot['PEG Ratio'], color=colors, alpha=0.7, edgecolor='black')\n",
    "ax2.axvline(1.0, color='green', linestyle='--', alpha=0.5, linewidth=2, label='Fair Value PEG=1.0')\n",
    "ax2.set_xlabel('PEG Ratio', fontweight='bold')\n",
    "ax2.set_title('PEG Ratio (P/E to Growth)', fontweight='bold', fontsize=13)\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "for i, (ticker, val) in enumerate(zip(df_plot['Ticker'], df_plot['PEG Ratio'])):\n",
    "    if not np.isnan(val):\n",
    "        ax2.text(val + 0.05, i, f'{val:.2f}', va='center', fontweight='bold')\n",
    "\n",
    "# Plot 3: FCF Yield Comparison\n",
    "df_plot = df_dcf.dropna(subset=['FCF Yield %']).sort_values('FCF Yield %', ascending=False)\n",
    "colors = ['#FF4444' if x == 'AIRO' else '#AA44AA' for x in df_plot['Ticker']]\n",
    "bars = ax3.barh(df_plot['Ticker'], df_plot['FCF Yield %'], color=colors, alpha=0.7, edgecolor='black')\n",
    "ax3.set_xlabel('Free Cash Flow Yield (%)', fontweight='bold')\n",
    "ax3.set_title('FCF Yield (FCF / Market Cap)', fontweight='bold', fontsize=13)\n",
    "ax3.grid(axis='x', alpha=0.3)\n",
    "for i, (ticker, val) in enumerate(zip(df_plot['Ticker'], df_plot['FCF Yield %'])):\n",
    "    if not np.isnan(val):\n",
    "        ax3.text(val + 0.2, i, f'{val:.1f}%', va='center', fontweight='bold')\n",
    "\n",
    "# Plot 4: DCF Upside/Downside\n",
    "df_plot = df_dcf.dropna(subset=['DCF Upside %']).sort_values('DCF Upside %')\n",
    "colors = ['green' if x > 0 else 'red' for x in df_plot['DCF Upside %']]\n",
    "bars = ax4.barh(df_plot['Ticker'], df_plot['DCF Upside %'], color=colors, alpha=0.7, edgecolor='black')\n",
    "ax4.axvline(0, color='black', linestyle='-', alpha=0.5)\n",
    "ax4.set_xlabel('DCF Implied Upside/Downside (%)', fontweight='bold')\n",
    "ax4.set_title('DCF Valuation - Upside to Intrinsic Value', fontweight='bold', fontsize=13)\n",
    "ax4.grid(axis='x', alpha=0.3)\n",
    "for i, (ticker, val) in enumerate(zip(df_plot['Ticker'], df_plot['DCF Upside %'])):\n",
    "    if not np.isnan(val):\n",
    "        ax4.text(val + 2, i, f'{val:+.1f}%', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ DCF and fundamental analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hs2jk8s9nfp",
   "metadata": {},
   "source": [
    "## 7C. Working Capital & Liquidity Analysis\n",
    "\n",
    "Working capital metrics reveal operational efficiency and short-term financial health. We'll calculate current/quick ratios, cash conversion cycle, and working capital trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5rk2z1ry7i9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working Capital and Liquidity Analysis\n",
    "\n",
    "wc_data = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    info = stock_info[ticker]\n",
    "    \n",
    "    # Balance sheet items\n",
    "    current_assets = safe_get(info, 'totalCurrentAssets', np.nan)\n",
    "    current_liabilities = safe_get(info, 'totalCurrentLiabilities', np.nan)\n",
    "    cash = safe_get(info, 'totalCash', 0)\n",
    "    inventory = safe_get(info, 'inventory', 0)\n",
    "    receivables = safe_get(info, 'receivables', np.nan) if safe_get(info, 'receivables', np.nan) is not np.nan else safe_get(info, 'accountsReceivable', 0)\n",
    "    payables = safe_get(info, 'accountsPayable', 0)\n",
    "    \n",
    "    # Revenue and COGS\n",
    "    revenue = safe_get(info, 'totalRevenue', np.nan)\n",
    "    cogs = safe_get(info, 'costOfRevenue', np.nan)\n",
    "    \n",
    "    # Liquidity Ratios\n",
    "    current_ratio = (current_assets / current_liabilities) if current_liabilities and current_liabilities > 0 else np.nan\n",
    "    \n",
    "    # Quick Ratio = (Current Assets - Inventory) / Current Liabilities\n",
    "    quick_ratio = ((current_assets - inventory) / current_liabilities) if current_liabilities and current_liabilities > 0 else np.nan\n",
    "    \n",
    "    # Cash Ratio = Cash / Current Liabilities\n",
    "    cash_ratio = (cash / current_liabilities) if current_liabilities and current_liabilities > 0 else np.nan\n",
    "    \n",
    "    # Working Capital\n",
    "    working_capital = (current_assets - current_liabilities) if not np.isnan(current_assets) and not np.isnan(current_liabilities) else np.nan\n",
    "    \n",
    "    # Cash Conversion Cycle components\n",
    "    # Days Sales Outstanding (DSO) = (Receivables / Revenue) * 365\n",
    "    dso = (receivables / revenue * 365) if revenue and receivables and revenue > 0 else np.nan\n",
    "    \n",
    "    # Days Inventory Outstanding (DIO) = (Inventory / COGS) * 365\n",
    "    dio = (inventory / cogs * 365) if cogs and inventory and cogs > 0 else np.nan\n",
    "    \n",
    "    # Days Payable Outstanding (DPO) = (Payables / COGS) * 365\n",
    "    dpo = (payables / cogs * 365) if cogs and payables and cogs > 0 else np.nan\n",
    "    \n",
    "    # Cash Conversion Cycle = DSO + DIO - DPO\n",
    "    ccc = (dso + dio - dpo) if not np.isnan(dso) and not np.isnan(dio) and not np.isnan(dpo) else np.nan\n",
    "    \n",
    "    wc_data.append({\n",
    "        'Ticker': ticker,\n",
    "        'Company': companies[ticker]['name'],\n",
    "        'Current Ratio': current_ratio,\n",
    "        'Quick Ratio': quick_ratio,\n",
    "        'Cash Ratio': cash_ratio,\n",
    "        'Working Capital ($M)': working_capital / 1e6 if working_capital and not np.isnan(working_capital) else np.nan,\n",
    "        'DSO (days)': dso,\n",
    "        'DIO (days)': dio,\n",
    "        'DPO (days)': dpo,\n",
    "        'Cash Conversion Cycle (days)': ccc\n",
    "    })\n",
    "\n",
    "df_wc = pd.DataFrame(wc_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"WORKING CAPITAL & LIQUIDITY ANALYSIS\")\n",
    "print(\"=\"*120)\n",
    "print(df_wc.to_string(index=False))\n",
    "print(\"=\"*120)\n",
    "\n",
    "# Interpretation guide\n",
    "print(\"\\n\" + \"-\"*120)\n",
    "print(\"LIQUIDITY METRICS INTERPRETATION:\")\n",
    "print(\"-\"*120)\n",
    "print(\"Current Ratio > 1.5: Strong liquidity | 1.0-1.5: Adequate | < 1.0: Potential liquidity risk\")\n",
    "print(\"Quick Ratio > 1.0: Can meet short-term obligations without selling inventory\")\n",
    "print(\"Cash Conversion Cycle: Lower is better - shows how quickly company converts investments into cash\")\n",
    "print(\"-\"*120)\n",
    "\n",
    "# Visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Working Capital & Liquidity Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Current vs Quick Ratio\n",
    "df_plot = df_wc.dropna(subset=['Current Ratio', 'Quick Ratio'])\n",
    "x = np.arange(len(df_plot))\n",
    "width = 0.35\n",
    "bars1 = ax1.bar(x - width/2, df_plot['Current Ratio'], width, label='Current Ratio', color='#4ECDC4', alpha=0.7, edgecolor='black')\n",
    "bars2 = ax1.bar(x + width/2, df_plot['Quick Ratio'], width, label='Quick Ratio', color='#FF6B6B', alpha=0.7, edgecolor='black')\n",
    "ax1.axhline(1.0, color='red', linestyle='--', alpha=0.5, label='Minimum Safe Level')\n",
    "ax1.axhline(1.5, color='green', linestyle='--', alpha=0.5, label='Strong Level')\n",
    "ax1.set_ylabel('Ratio', fontweight='bold')\n",
    "ax1.set_title('Liquidity Ratios Comparison', fontweight='bold', fontsize=13)\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(df_plot['Ticker'])\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 2: Cash Conversion Cycle\n",
    "df_plot = df_wc.dropna(subset=['Cash Conversion Cycle (days)']).sort_values('Cash Conversion Cycle (days)')\n",
    "colors = ['#FF4444' if x == 'AIRO' else '#44AA44' for x in df_plot['Ticker']]\n",
    "colors = ['green' if x < 60 else 'orange' if x < 90 else 'red' for x in df_plot['Cash Conversion Cycle (days)']]\n",
    "bars = ax2.barh(df_plot['Ticker'], df_plot['Cash Conversion Cycle (days)'], color=colors, alpha=0.7, edgecolor='black')\n",
    "ax2.axvline(60, color='green', linestyle='--', alpha=0.5, label='Excellent (<60 days)')\n",
    "ax2.axvline(90, color='orange', linestyle='--', alpha=0.5, label='Good (<90 days)')\n",
    "ax2.set_xlabel('Days', fontweight='bold')\n",
    "ax2.set_title('Cash Conversion Cycle', fontweight='bold', fontsize=13)\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "for i, (ticker, val) in enumerate(zip(df_plot['Ticker'], df_plot['Cash Conversion Cycle (days)'])):\n",
    "    if not np.isnan(val):\n",
    "        ax2.text(val + 2, i, f'{val:.0f}d', va='center', fontweight='bold')\n",
    "\n",
    "# Plot 3: CCC Components Breakdown\n",
    "df_plot = df_wc.dropna(subset=['DSO (days)', 'DIO (days)', 'DPO (days)'])\n",
    "if len(df_plot) > 0:\n",
    "    x = np.arange(len(df_plot))\n",
    "    width = 0.25\n",
    "    bars1 = ax3.bar(x - width, df_plot['DSO (days)'], width, label='DSO (Receivables)', color='#FF6B6B', alpha=0.7)\n",
    "    bars2 = ax3.bar(x, df_plot['DIO (days)'], width, label='DIO (Inventory)', color='#4ECDC4', alpha=0.7)\n",
    "    bars3 = ax3.bar(x + width, df_plot['DPO (days)'], width, label='DPO (Payables)', color='#45B7D1', alpha=0.7)\n",
    "    ax3.set_ylabel('Days', fontweight='bold')\n",
    "    ax3.set_title('CCC Components Breakdown', fontweight='bold', fontsize=13)\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels(df_plot['Ticker'])\n",
    "    ax3.legend(fontsize=9)\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 4: Working Capital Position\n",
    "df_plot = df_wc.dropna(subset=['Working Capital ($M)']).sort_values('Working Capital ($M)')\n",
    "colors = ['green' if x > 0 else 'red' for x in df_plot['Working Capital ($M)']]\n",
    "bars = ax4.barh(df_plot['Ticker'], df_plot['Working Capital ($M)'], color=colors, alpha=0.7, edgecolor='black')\n",
    "ax4.axvline(0, color='black', linestyle='-', alpha=0.5)\n",
    "ax4.set_xlabel('Working Capital ($M)', fontweight='bold')\n",
    "ax4.set_title('Net Working Capital Position', fontweight='bold', fontsize=13)\n",
    "ax4.grid(axis='x', alpha=0.3)\n",
    "for i, (ticker, val) in enumerate(zip(df_plot['Ticker'], df_plot['Working Capital ($M)'])):\n",
    "    if not np.isnan(val):\n",
    "        ax4.text(val + 10 if val > 0 else val - 10, i, f'${val:.0f}M', va='center', \n",
    "                fontweight='bold', ha='left' if val > 0 else 'right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Working capital analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ji6gxqz6im",
   "metadata": {},
   "source": [
    "## 7D. Leverage, Solvency & Altman Z-Score\n",
    "\n",
    "Leverage ratios assess financial risk and the company's ability to meet long-term obligations. The Altman Z-Score predicts bankruptcy probability using multiple financial ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "epu8igurl9g",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leverage, Solvency and Altman Z-Score Analysis\n",
    "\n",
    "def calculate_altman_z_score(current_assets, current_liabilities, total_assets, total_equity, \n",
    "                              retained_earnings, ebit, revenue, market_cap, total_liabilities):\n",
    "    \"\"\"\n",
    "    Altman Z-Score for predicting bankruptcy\n",
    "    Z = 1.2*X1 + 1.4*X2 + 3.3*X3 + 0.6*X4 + 1.0*X5\n",
    "    Where:\n",
    "    X1 = Working Capital / Total Assets\n",
    "    X2 = Retained Earnings / Total Assets\n",
    "    X3 = EBIT / Total Assets\n",
    "    X4 = Market Cap / Total Liabilities\n",
    "    X5 = Revenue / Total Assets\n",
    "    \n",
    "    Interpretation:\n",
    "    Z > 2.99: Safe zone\n",
    "    1.81 < Z < 2.99: Grey zone\n",
    "    Z < 1.81: Distress zone (high bankruptcy risk)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if total_assets <= 0:\n",
    "            return np.nan\n",
    "            \n",
    "        working_capital = current_assets - current_liabilities\n",
    "        x1 = working_capital / total_assets\n",
    "        x2 = retained_earnings / total_assets if retained_earnings else 0\n",
    "        x3 = ebit / total_assets if ebit else 0\n",
    "        x4 = market_cap / total_liabilities if total_liabilities > 0 else 0\n",
    "        x5 = revenue / total_assets if revenue else 0\n",
    "        \n",
    "        z_score = 1.2*x1 + 1.4*x2 + 3.3*x3 + 0.6*x4 + 1.0*x5\n",
    "        return z_score\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "leverage_data = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    info = stock_info[ticker]\n",
    "    \n",
    "    # Balance sheet items\n",
    "    total_debt = safe_get(info, 'totalDebt', 0)\n",
    "    total_equity = safe_get(info, 'totalStockholderEquity', np.nan)\n",
    "    total_assets = safe_get(info, 'totalAssets', np.nan)\n",
    "    total_liabilities = safe_get(info, 'totalLiabilities', np.nan) if safe_get(info, 'totalLiabilities', np.nan) is not np.nan else safe_get(info, 'totalLiab', np.nan)\n",
    "    current_assets = safe_get(info, 'totalCurrentAssets', np.nan)\n",
    "    current_liabilities = safe_get(info, 'totalCurrentLiabilities', np.nan)\n",
    "    cash = safe_get(info, 'totalCash', 0)\n",
    "    \n",
    "    # Income statement items\n",
    "    ebitda = safe_get(info, 'ebitda', np.nan)\n",
    "    ebit = safe_get(info, 'ebit', np.nan) if safe_get(info, 'ebit', np.nan) is not np.nan else ebitda\n",
    "    interest_expense = safe_get(info, 'interestExpense', np.nan)\n",
    "    revenue = safe_get(info, 'totalRevenue', np.nan)\n",
    "    retained_earnings = safe_get(info, 'retainedEarnings', np.nan)\n",
    "    \n",
    "    # Market cap\n",
    "    market_cap = df_metrics[df_metrics['Ticker']==ticker]['Market Cap (M)'].values[0] * 1e6\n",
    "    \n",
    "    # Calculate leverage ratios\n",
    "    # Debt-to-Equity\n",
    "    debt_to_equity = (total_debt / total_equity) if total_equity and total_equity > 0 else np.nan\n",
    "    \n",
    "    # Debt-to-Assets\n",
    "    debt_to_assets = (total_debt / total_assets) if total_assets and total_assets > 0 else np.nan\n",
    "    \n",
    "    # Debt-to-EBITDA\n",
    "    debt_to_ebitda = (total_debt / ebitda) if ebitda and ebitda > 0 else np.nan\n",
    "    \n",
    "    # Net Debt = Total Debt - Cash\n",
    "    net_debt = total_debt - cash\n",
    "    net_debt_to_ebitda = (net_debt / ebitda) if ebitda and ebitda > 0 else np.nan\n",
    "    \n",
    "    # Interest Coverage Ratio = EBIT / Interest Expense\n",
    "    interest_coverage = (ebit / abs(interest_expense)) if interest_expense and interest_expense != 0 and ebit else np.nan\n",
    "    \n",
    "    # Equity Ratio = Total Equity / Total Assets\n",
    "    equity_ratio = (total_equity / total_assets) if total_assets and total_equity and total_assets > 0 else np.nan\n",
    "    \n",
    "    # Calculate Altman Z-Score\n",
    "    z_score = calculate_altman_z_score(\n",
    "        current_assets, current_liabilities, total_assets, total_equity,\n",
    "        retained_earnings, ebit, revenue, market_cap, total_liabilities\n",
    "    )\n",
    "    \n",
    "    # Interpret Z-Score\n",
    "    if not np.isnan(z_score):\n",
    "        if z_score > 2.99:\n",
    "            z_interpretation = 'Safe Zone'\n",
    "            z_risk = 'Low Risk'\n",
    "        elif z_score > 1.81:\n",
    "            z_interpretation = 'Grey Zone'\n",
    "            z_risk = 'Moderate Risk'\n",
    "        else:\n",
    "            z_interpretation = 'Distress Zone'\n",
    "            z_risk = 'High Risk'\n",
    "    else:\n",
    "        z_interpretation = 'N/A'\n",
    "        z_risk = 'N/A'\n",
    "    \n",
    "    leverage_data.append({\n",
    "        'Ticker': ticker,\n",
    "        'Company': companies[ticker]['name'],\n",
    "        'Debt/Equity': debt_to_equity,\n",
    "        'Debt/Assets': debt_to_assets,\n",
    "        'Debt/EBITDA': debt_to_ebitda,\n",
    "        'Net Debt/EBITDA': net_debt_to_ebitda,\n",
    "        'Interest Coverage': interest_coverage,\n",
    "        'Equity Ratio': equity_ratio,\n",
    "        'Altman Z-Score': z_score,\n",
    "        'Z-Score Zone': z_interpretation,\n",
    "        'Financial Risk': z_risk\n",
    "    })\n",
    "\n",
    "df_leverage = pd.DataFrame(leverage_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"LEVERAGE, SOLVENCY & ALTMAN Z-SCORE ANALYSIS\")\n",
    "print(\"=\"*120)\n",
    "print(df_leverage.to_string(index=False))\n",
    "print(\"=\"*120)\n",
    "\n",
    "print(\"\\n\" + \"-\"*120)\n",
    "print(\"INTERPRETATION GUIDE:\")\n",
    "print(\"-\"*120)\n",
    "print(\"Debt/Equity: < 0.5 Conservative | 0.5-1.0 Moderate | > 1.0 Aggressive\")\n",
    "print(\"Interest Coverage: > 3.0x Safe | 1.5-3.0x Adequate | < 1.5x Risky\")\n",
    "print(\"Altman Z-Score: > 2.99 Safe | 1.81-2.99 Grey Zone | < 1.81 Distress\")\n",
    "print(\"-\"*120)\n",
    "\n",
    "# Visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Leverage, Solvency & Credit Risk Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Debt-to-Equity Ratio\n",
    "df_plot = df_leverage.dropna(subset=['Debt/Equity']).sort_values('Debt/Equity')\n",
    "colors = ['green' if x < 0.5 else 'orange' if x < 1.0 else 'red' for x in df_plot['Debt/Equity']]\n",
    "bars = ax1.barh(df_plot['Ticker'], df_plot['Debt/Equity'], color=colors, alpha=0.7, edgecolor='black')\n",
    "ax1.axvline(0.5, color='green', linestyle='--', alpha=0.5, label='Conservative')\n",
    "ax1.axvline(1.0, color='orange', linestyle='--', alpha=0.5, label='Moderate')\n",
    "ax1.set_xlabel('Debt-to-Equity Ratio', fontweight='bold')\n",
    "ax1.set_title('Financial Leverage (Debt/Equity)', fontweight='bold', fontsize=13)\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "for i, (ticker, val) in enumerate(zip(df_plot['Ticker'], df_plot['Debt/Equity'])):\n",
    "    if not np.isnan(val):\n",
    "        ax1.text(val + 0.05, i, f'{val:.2f}x', va='center', fontweight='bold')\n",
    "\n",
    "# Plot 2: Interest Coverage Ratio\n",
    "df_plot = df_leverage.dropna(subset=['Interest Coverage']).sort_values('Interest Coverage', ascending=False)\n",
    "# Clip extreme values for visualization\n",
    "df_plot_display = df_plot.copy()\n",
    "df_plot_display['Interest Coverage'] = df_plot_display['Interest Coverage'].clip(upper=20)\n",
    "colors = ['green' if x > 3 else 'orange' if x > 1.5 else 'red' for x in df_plot['Interest Coverage']]\n",
    "bars = ax2.barh(df_plot_display['Ticker'], df_plot_display['Interest Coverage'], color=colors, alpha=0.7, edgecolor='black')\n",
    "ax2.axvline(1.5, color='red', linestyle='--', alpha=0.5, label='Risky')\n",
    "ax2.axvline(3.0, color='green', linestyle='--', alpha=0.5, label='Safe')\n",
    "ax2.set_xlabel('Interest Coverage Ratio (EBIT/Interest)', fontweight='bold')\n",
    "ax2.set_title('Debt Service Coverage', fontweight='bold', fontsize=13)\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "for i, (ticker, val) in enumerate(zip(df_plot['Ticker'], df_plot['Interest Coverage'])):\n",
    "    if not np.isnan(val):\n",
    "        display_val = min(val, 20)\n",
    "        ax2.text(display_val + 0.3, i, f'{val:.1f}x' if val < 100 else f'{val:.0f}x', va='center', fontweight='bold')\n",
    "\n",
    "# Plot 3: Altman Z-Score\n",
    "df_plot = df_leverage.dropna(subset=['Altman Z-Score']).sort_values('Altman Z-Score')\n",
    "colors = []\n",
    "for val in df_plot['Altman Z-Score']:\n",
    "    if val > 2.99:\n",
    "        colors.append('green')\n",
    "    elif val > 1.81:\n",
    "        colors.append('orange')\n",
    "    else:\n",
    "        colors.append('red')\n",
    "\n",
    "bars = ax3.barh(df_plot['Ticker'], df_plot['Altman Z-Score'], color=colors, alpha=0.7, edgecolor='black')\n",
    "ax3.axvline(1.81, color='red', linestyle='--', alpha=0.5, linewidth=2, label='Distress Zone')\n",
    "ax3.axvline(2.99, color='green', linestyle='--', alpha=0.5, linewidth=2, label='Safe Zone')\n",
    "ax3.fill_betweenx([-0.5, len(df_plot)], 1.81, 2.99, alpha=0.1, color='orange', label='Grey Zone')\n",
    "ax3.set_xlabel('Altman Z-Score', fontweight='bold')\n",
    "ax3.set_title('Bankruptcy Risk Prediction (Altman Z-Score)', fontweight='bold', fontsize=13)\n",
    "ax3.legend(fontsize=9)\n",
    "ax3.grid(axis='x', alpha=0.3)\n",
    "for i, (ticker, val, zone) in enumerate(zip(df_plot['Ticker'], df_plot['Altman Z-Score'], df_plot['Z-Score Zone'])):\n",
    "    if not np.isnan(val):\n",
    "        ax3.text(val + 0.1, i, f'{val:.2f} ({zone})', va='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "# Plot 4: Net Debt to EBITDA\n",
    "df_plot = df_leverage.dropna(subset=['Net Debt/EBITDA']).sort_values('Net Debt/EBITDA')\n",
    "# Handle negative values (companies with more cash than debt)\n",
    "colors = []\n",
    "for val in df_plot['Net Debt/EBITDA']:\n",
    "    if val < 0:\n",
    "        colors.append('darkgreen')\n",
    "    elif val < 2:\n",
    "        colors.append('green')\n",
    "    elif val < 4:\n",
    "        colors.append('orange')\n",
    "    else:\n",
    "        colors.append('red')\n",
    "\n",
    "bars = ax4.barh(df_plot['Ticker'], df_plot['Net Debt/EBITDA'], color=colors, alpha=0.7, edgecolor='black')\n",
    "ax4.axvline(0, color='black', linestyle='-', alpha=0.5)\n",
    "ax4.axvline(2, color='green', linestyle='--', alpha=0.5, label='Conservative')\n",
    "ax4.axvline(4, color='orange', linestyle='--', alpha=0.5, label='Moderate')\n",
    "ax4.set_xlabel('Net Debt / EBITDA', fontweight='bold')\n",
    "ax4.set_title('Leverage Ratio (Net Debt/EBITDA)', fontweight='bold', fontsize=13)\n",
    "ax4.legend(fontsize=9)\n",
    "ax4.grid(axis='x', alpha=0.3)\n",
    "for i, (ticker, val) in enumerate(zip(df_plot['Ticker'], df_plot['Net Debt/EBITDA'])):\n",
    "    if not np.isnan(val):\n",
    "        ax4.text(val + 0.2 if val > 0 else val - 0.2, i, f'{val:.1f}x', \n",
    "                va='center', fontweight='bold', ha='left' if val > 0 else 'right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Leverage and solvency analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0n6uvhchwh5",
   "metadata": {},
   "source": [
    "## 7E. Advanced Composite Scores: Piotroski F-Score & Magic Formula\n",
    "\n",
    "Composite scores combine multiple fundamental signals to identify quality companies. The Piotroski F-Score (0-9) measures financial strength, while the Magic Formula combines earnings yield with ROIC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ve22y3t97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Composite Scores: Piotroski F-Score and Magic Formula\n",
    "\n",
    "def calculate_piotroski_fscore(info, prev_year_available=False):\n",
    "    \"\"\"\n",
    "    Piotroski F-Score: 9-point system to measure financial strength\n",
    "    Scores 0-9, where 9 is strongest\n",
    "    \n",
    "    Profitability signals (4 points):\n",
    "    1. Positive net income (ROA)\n",
    "    2. Positive operating cash flow\n",
    "    3. Change in ROA (improvement)\n",
    "    4. Quality of earnings (OCF > Net Income)\n",
    "    \n",
    "    Leverage/Liquidity signals (3 points):\n",
    "    5. Decrease in long-term debt\n",
    "    6. Increase in current ratio\n",
    "    7. No new equity issued\n",
    "    \n",
    "    Operating Efficiency signals (2 points):\n",
    "    8. Increase in gross margin\n",
    "    9. Increase in asset turnover\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    \n",
    "    # 1. Positive ROA (net income / total assets)\n",
    "    net_income = safe_get(info, 'netIncomeToCommon', 0)\n",
    "    total_assets = safe_get(info, 'totalAssets', 1)\n",
    "    roa = net_income / total_assets if total_assets > 0 else 0\n",
    "    if roa > 0:\n",
    "        score += 1\n",
    "    \n",
    "    # 2. Positive operating cash flow\n",
    "    operating_cf = safe_get(info, 'operatingCashflow', 0)\n",
    "    if operating_cf > 0:\n",
    "        score += 1\n",
    "    \n",
    "    # 3. Change in ROA (approximated as positive ROA for current year)\n",
    "    if roa > 0:\n",
    "        score += 1\n",
    "    \n",
    "    # 4. Quality of earnings (OCF > Net Income)\n",
    "    if operating_cf > net_income and operating_cf > 0:\n",
    "        score += 1\n",
    "    \n",
    "    # 5. Decrease in leverage (low debt is good)\n",
    "    total_debt = safe_get(info, 'totalDebt', 0)\n",
    "    total_equity = safe_get(info, 'totalStockholderEquity', 1)\n",
    "    debt_ratio = total_debt / total_equity if total_equity > 0 else 10\n",
    "    if debt_ratio < 0.5:  # Conservative leverage\n",
    "        score += 1\n",
    "    \n",
    "    # 6. Current ratio > 1.5 (good liquidity)\n",
    "    current_assets = safe_get(info, 'totalCurrentAssets', 0)\n",
    "    current_liabilities = safe_get(info, 'totalCurrentLiabilities', 1)\n",
    "    current_ratio = current_assets / current_liabilities if current_liabilities > 0 else 0\n",
    "    if current_ratio > 1.5:\n",
    "        score += 1\n",
    "    \n",
    "    # 7. No new shares issued (can't easily determine from yfinance, assume neutral)\n",
    "    # score += 0\n",
    "    \n",
    "    # 8. Gross margin > 40% (strong pricing power)\n",
    "    gross_margin = safe_get(info, 'grossMargins', 0)\n",
    "    if gross_margin > 0.40:\n",
    "        score += 1\n",
    "    \n",
    "    # 9. Asset turnover (revenue / assets) > 0.5\n",
    "    revenue = safe_get(info, 'totalRevenue', 0)\n",
    "    asset_turnover = revenue / total_assets if total_assets > 0 else 0\n",
    "    if asset_turnover > 0.5:\n",
    "        score += 1\n",
    "    \n",
    "    return score\n",
    "\n",
    "composite_data = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    info = stock_info[ticker]\n",
    "    \n",
    "    # Calculate Piotroski F-Score\n",
    "    f_score = calculate_piotroski_fscore(info)\n",
    "    \n",
    "    # Interpret F-Score\n",
    "    if f_score >= 8:\n",
    "        f_interpretation = 'Very Strong'\n",
    "    elif f_score >= 6:\n",
    "        f_interpretation = 'Strong'\n",
    "    elif f_score >= 4:\n",
    "        f_interpretation = 'Average'\n",
    "    else:\n",
    "        f_interpretation = 'Weak'\n",
    "    \n",
    "    # Magic Formula components\n",
    "    # 1. Earnings Yield = EBIT / Enterprise Value\n",
    "    ebit = safe_get(info, 'ebit', np.nan)\n",
    "    if np.isnan(ebit):\n",
    "        ebit = safe_get(info, 'ebitda', np.nan)\n",
    "    ev = df_metrics[df_metrics['Ticker']==ticker]['Enterprise Value (M)'].values[0] * 1e6\n",
    "    earnings_yield = (ebit / ev * 100) if ev and ev > 0 and ebit and not np.isnan(ebit) else np.nan\n",
    "    \n",
    "    # 2. ROIC (calculated earlier in DCF section)\n",
    "    roic = df_dcf[df_dcf['Ticker']==ticker]['ROIC %'].values[0] if ticker in df_dcf['Ticker'].values else np.nan\n",
    "    \n",
    "    # Magic Formula Rank (lower is better)\n",
    "    # In real implementation, would rank across universe\n",
    "    # Here we create a simple combined score\n",
    "    magic_score = np.nan\n",
    "    if not np.isnan(earnings_yield) and not np.isnan(roic):\n",
    "        # Normalize to 0-100 scale\n",
    "        ey_norm = earnings_yield if earnings_yield > 0 else 0\n",
    "        roic_norm = roic if roic > 0 else 0\n",
    "        magic_score = (ey_norm + roic_norm) / 2\n",
    "    \n",
    "    # Get other quality metrics\n",
    "    roe = df_dcf[df_dcf['Ticker']==ticker]['ROE %'].values[0] if ticker in df_dcf['Ticker'].values else np.nan\n",
    "    fcf_margin = df_dcf[df_dcf['Ticker']==ticker]['FCF Margin %'].values[0] if ticker in df_dcf['Ticker'].values else np.nan\n",
    "    \n",
    "    composite_data.append({\n",
    "        'Ticker': ticker,\n",
    "        'Company': companies[ticker]['name'],\n",
    "        'Piotroski F-Score': f_score,\n",
    "        'F-Score Rating': f_interpretation,\n",
    "        'Earnings Yield %': earnings_yield,\n",
    "        'ROIC %': roic,\n",
    "        'Magic Formula Score': magic_score,\n",
    "        'ROE %': roe,\n",
    "        'FCF Margin %': fcf_margin\n",
    "    })\n",
    "\n",
    "df_composite = pd.DataFrame(composite_data)\n",
    "\n",
    "# Calculate rankings for Magic Formula\n",
    "df_composite_ranked = df_composite.copy()\n",
    "df_composite_ranked['EY Rank'] = df_composite_ranked['Earnings Yield %'].rank(ascending=False, na_option='bottom')\n",
    "df_composite_ranked['ROIC Rank'] = df_composite_ranked['ROIC %'].rank(ascending=False, na_option='bottom')\n",
    "df_composite_ranked['Combined Rank'] = df_composite_ranked['EY Rank'] + df_composite_ranked['ROIC Rank']\n",
    "df_composite_ranked = df_composite_ranked.sort_values('Combined Rank')\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"ADVANCED COMPOSITE SCORES: PIOTROSKI F-SCORE & MAGIC FORMULA\")\n",
    "print(\"=\"*120)\n",
    "print(df_composite[['Ticker', 'Piotroski F-Score', 'F-Score Rating', 'Earnings Yield %', 'ROIC %', 'Magic Formula Score']].to_string(index=False))\n",
    "print(\"=\"*120)\n",
    "\n",
    "print(\"\\n\" + \"-\"*120)\n",
    "print(\"MAGIC FORMULA RANKING (Lower Combined Rank = Better)\")\n",
    "print(\"-\"*120)\n",
    "print(df_composite_ranked[['Ticker', 'Earnings Yield %', 'EY Rank', 'ROIC %', 'ROIC Rank', 'Combined Rank']].to_string(index=False))\n",
    "print(\"-\"*120)\n",
    "\n",
    "print(\"\\n\" + \"-\"*120)\n",
    "print(\"INTERPRETATION:\")\n",
    "print(\"-\"*120)\n",
    "print(\"Piotroski F-Score: 8-9 Very Strong | 6-7 Strong | 4-5 Average | 0-3 Weak\")\n",
    "print(\"Magic Formula: Combines high earnings yield (value) with high ROIC (quality)\")\n",
    "print(\"Best investments: High F-Score + Low Combined Rank\")\n",
    "print(\"-\"*120)\n",
    "\n",
    "# Visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Advanced Composite Scores Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Piotroski F-Score\n",
    "df_plot = df_composite.sort_values('Piotroski F-Score', ascending=False)\n",
    "colors = ['green' if x >= 6 else 'orange' if x >= 4 else 'red' for x in df_plot['Piotroski F-Score']]\n",
    "bars = ax1.barh(df_plot['Ticker'], df_plot['Piotroski F-Score'], color=colors, alpha=0.7, edgecolor='black')\n",
    "ax1.axvline(6, color='green', linestyle='--', alpha=0.5, label='Strong (â‰¥6)')\n",
    "ax1.axvline(4, color='orange', linestyle='--', alpha=0.5, label='Average (â‰¥4)')\n",
    "ax1.set_xlabel('F-Score (0-9)', fontweight='bold')\n",
    "ax1.set_title('Piotroski F-Score (Financial Strength)', fontweight='bold', fontsize=13)\n",
    "ax1.set_xlim(0, 9)\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "for i, (ticker, score, rating) in enumerate(zip(df_plot['Ticker'], df_plot['Piotroski F-Score'], df_plot['F-Score Rating'])):\n",
    "    ax1.text(score + 0.2, i, f'{score} - {rating}', va='center', fontweight='bold')\n",
    "\n",
    "# Plot 2: Magic Formula - Earnings Yield vs ROIC\n",
    "df_plot = df_composite.dropna(subset=['Earnings Yield %', 'ROIC %'])\n",
    "for idx, row in df_plot.iterrows():\n",
    "    color = '#FF4444' if row['Ticker'] == 'AIRO' else '#4444FF'\n",
    "    size = 300 if row['Ticker'] == 'AIRO' else 150\n",
    "    ax2.scatter(row['ROIC %'], row['Earnings Yield %'], s=size, color=color, alpha=0.6, edgecolor='black', linewidth=2)\n",
    "    ax2.annotate(row['Ticker'], (row['ROIC %'], row['Earnings Yield %']), \n",
    "                fontweight='bold', fontsize=11, ha='center', va='bottom')\n",
    "\n",
    "# Add quadrant lines at medians\n",
    "if len(df_plot) > 0:\n",
    "    median_roic = df_plot['ROIC %'].median()\n",
    "    median_ey = df_plot['Earnings Yield %'].median()\n",
    "    ax2.axhline(median_ey, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax2.axvline(median_roic, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Label quadrants\n",
    "    ax2.text(0.95, 0.95, 'High EY\\nHigh ROIC\\n(BEST)', transform=ax2.transAxes, ha='right', va='top',\n",
    "            fontsize=10, fontweight='bold', bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\n",
    "\n",
    "ax2.set_xlabel('Return on Invested Capital (ROIC) %', fontweight='bold')\n",
    "ax2.set_ylabel('Earnings Yield %', fontweight='bold')\n",
    "ax2.set_title('Magic Formula Matrix', fontweight='bold', fontsize=13)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# Plot 3: Magic Formula Combined Ranking\n",
    "df_plot = df_composite_ranked.dropna(subset=['Combined Rank']).sort_values('Combined Rank')\n",
    "colors = ['#FF4444' if x == 'AIRO' else '#44AA44' for x in df_plot['Ticker']]\n",
    "colors = ['green' if i < 2 else 'orange' if i < 4 else 'gray' for i in range(len(df_plot))]\n",
    "bars = ax3.barh(df_plot['Ticker'], df_plot['Combined Rank'], color=colors, alpha=0.7, edgecolor='black')\n",
    "ax3.set_xlabel('Combined Rank (Lower is Better)', fontweight='bold')\n",
    "ax3.set_title('Magic Formula Ranking', fontweight='bold', fontsize=13)\n",
    "ax3.invert_xaxis()  # Lower rank is better, so invert\n",
    "ax3.grid(axis='x', alpha=0.3)\n",
    "for i, (ticker, rank) in enumerate(zip(df_plot['Ticker'], df_plot['Combined Rank'])):\n",
    "    if not np.isnan(rank):\n",
    "        ax3.text(rank - 0.2, i, f'#{int(rank)}', va='center', fontweight='bold', ha='right')\n",
    "\n",
    "# Plot 4: Quality Score Heatmap\n",
    "quality_metrics = ['Piotroski F-Score', 'ROIC %', 'ROE %', 'FCF Margin %']\n",
    "df_quality = df_composite[['Ticker'] + quality_metrics].set_index('Ticker')\n",
    "\n",
    "# Normalize for visualization\n",
    "df_quality_norm = df_quality.copy()\n",
    "for col in df_quality_norm.columns:\n",
    "    min_val = df_quality[col].min()\n",
    "    max_val = df_quality[col].max()\n",
    "    if max_val > min_val:\n",
    "        df_quality_norm[col] = ((df_quality[col] - min_val) / (max_val - min_val) * 100)\n",
    "    else:\n",
    "        df_quality_norm[col] = 50\n",
    "\n",
    "# Create heatmap\n",
    "im = ax4.imshow(df_quality_norm.T, cmap='RdYlGn', aspect='auto', vmin=0, vmax=100)\n",
    "ax4.set_xticks(range(len(df_quality_norm)))\n",
    "ax4.set_xticklabels(df_quality_norm.index, rotation=0)\n",
    "ax4.set_yticks(range(len(quality_metrics)))\n",
    "ax4.set_yticklabels(quality_metrics)\n",
    "ax4.set_title('Quality Metrics Heatmap (Normalized)', fontweight='bold', fontsize=13)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, ax=ax4)\n",
    "cbar.set_label('Score (0-100)', fontweight='bold')\n",
    "\n",
    "# Add values on heatmap\n",
    "for i in range(len(quality_metrics)):\n",
    "    for j in range(len(df_quality_norm)):\n",
    "        value = df_quality.iloc[j, i]\n",
    "        if not np.isnan(value):\n",
    "            text = ax4.text(j, i, f'{value:.1f}', ha='center', va='center', \n",
    "                          color='black' if df_quality_norm.iloc[j, i] < 50 else 'white',\n",
    "                          fontweight='bold', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Composite scores analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9wzy2m5vy4e",
   "metadata": {},
   "source": [
    "## 7F. Machine Learning Applications & Predictive Analytics\n",
    "\n",
    "Apply machine learning techniques to predict stock outperformance using the comprehensive feature set we've calculated. This includes classification models, feature importance analysis, and clustering for peer identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85a6a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401vtojloj",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning for Stock Analysis and Prediction\n",
    "\n",
    "# First, let's install scikit-learn if needed (uncomment if not installed)\n",
    "# !pip install scikit-learn\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Building comprehensive feature matrix for ML models...\")\n",
    "\n",
    "# Compile all metrics into a single dataframe for ML analysis\n",
    "ml_features = pd.DataFrame()\n",
    "\n",
    "for ticker in tickers:\n",
    "    features = {\n",
    "        'Ticker': ticker,\n",
    "        # Valuation metrics\n",
    "        'P/S': df_metrics[df_metrics['Ticker']==ticker]['P/S'].values[0],\n",
    "        'EV/Sales': df_metrics[df_metrics['Ticker']==ticker]['EV/Sales'].values[0],\n",
    "        'P/B': df_metrics[df_metrics['Ticker']==ticker]['P/B'].values[0],\n",
    "        \n",
    "        # Growth metrics\n",
    "        'Revenue Growth %': df_metrics[df_metrics['Ticker']==ticker]['Revenue Growth %'].values[0],\n",
    "        \n",
    "        # Profitability metrics\n",
    "        'Gross Margin %': df_metrics[df_metrics['Ticker']==ticker]['Gross Margin %'].values[0],\n",
    "        'Operating Margin %': df_metrics[df_metrics['Ticker']==ticker]['Operating Margin %'].values[0],\n",
    "        \n",
    "        # Quality metrics\n",
    "        'ROIC %': df_dcf[df_dcf['Ticker']==ticker]['ROIC %'].values[0] if ticker in df_dcf['Ticker'].values else np.nan,\n",
    "        'ROE %': df_dcf[df_dcf['Ticker']==ticker]['ROE %'].values[0] if ticker in df_dcf['Ticker'].values else np.nan,\n",
    "        'FCF Margin %': df_dcf[df_dcf['Ticker']==ticker]['FCF Margin %'].values[0] if ticker in df_dcf['Ticker'].values else np.nan,\n",
    "        \n",
    "        # Liquidity metrics\n",
    "        'Current Ratio': df_wc[df_wc['Ticker']==ticker]['Current Ratio'].values[0] if ticker in df_wc['Ticker'].values else np.nan,\n",
    "        'Quick Ratio': df_wc[df_wc['Ticker']==ticker]['Quick Ratio'].values[0] if ticker in df_wc['Ticker'].values else np.nan,\n",
    "        \n",
    "        # Leverage metrics\n",
    "        'Debt/Equity': df_leverage[df_leverage['Ticker']==ticker]['Debt/Equity'].values[0] if ticker in df_leverage['Ticker'].values else np.nan,\n",
    "        'Interest Coverage': df_leverage[df_leverage['Ticker']==ticker]['Interest Coverage'].values[0] if ticker in df_leverage['Ticker'].values else np.nan,\n",
    "        \n",
    "        # Risk metrics\n",
    "        'Beta': df_metrics[df_metrics['Ticker']==ticker]['Beta'].values[0],\n",
    "        'Volatility %': df_risk[df_risk['Ticker']==ticker]['Ann. Volatility %'].values[0] if ticker in df_risk['Ticker'].values else np.nan,\n",
    "        'Sharpe Ratio': df_risk[df_risk['Ticker']==ticker]['Sharpe Ratio'].values[0] if ticker in df_risk['Ticker'].values else np.nan,\n",
    "        \n",
    "        # Technical indicators\n",
    "        'RSI': df_technical[df_technical['Ticker']==ticker]['RSI (14)'].values[0] if ticker in df_technical['Ticker'].values else np.nan,\n",
    "        \n",
    "        # Composite scores\n",
    "        'F-Score': df_composite[df_composite['Ticker']==ticker]['Piotroski F-Score'].values[0] if ticker in df_composite['Ticker'].values else np.nan,\n",
    "        'Altman Z-Score': df_leverage[df_leverage['Ticker']==ticker]['Altman Z-Score'].values[0] if ticker in df_leverage['Ticker'].values else np.nan,\n",
    "        \n",
    "        # Performance (target variable)\n",
    "        '1Y Return %': df_performance[df_performance['Ticker']==ticker]['1Y Return %'].values[0] if ticker in df_performance['Ticker'].values else np.nan,\n",
    "    }\n",
    "    \n",
    "    ml_features = pd.concat([ml_features, pd.DataFrame([features])], ignore_index=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"MACHINE LEARNING FEATURE MATRIX\")\n",
    "print(\"=\"*120)\n",
    "print(f\"Total features: {len(ml_features.columns) - 2}\")  # Excluding Ticker and Target\n",
    "print(f\"Companies: {len(ml_features)}\")\n",
    "print(\"\\nFeature completeness:\")\n",
    "for col in ml_features.columns:\n",
    "    if col not in ['Ticker', '1Y Return %']:\n",
    "        missing_pct = ml_features[col].isna().sum() / len(ml_features) * 100\n",
    "        print(f\"  {col:30} - {100-missing_pct:.0f}% complete\")\n",
    "\n",
    "# 1. CLUSTERING ANALYSIS - Identify similar companies\n",
    "print(\"\\n\\n\" + \"=\"*120)\n",
    "print(\"1. K-MEANS CLUSTERING - Company Similarity Analysis\")\n",
    "print(\"=\"*120)\n",
    "\n",
    "# Prepare data for clustering (use only complete cases)\n",
    "cluster_features = ['P/S', 'Revenue Growth %', 'Gross Margin %', 'ROIC %', 'Volatility %', 'F-Score']\n",
    "df_cluster = ml_features[['Ticker'] + cluster_features].copy()\n",
    "df_cluster_clean = df_cluster.dropna()\n",
    "\n",
    "if len(df_cluster_clean) > 2:\n",
    "    X_cluster = df_cluster_clean[cluster_features].values\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_cluster)\n",
    "    \n",
    "    # Perform K-means clustering\n",
    "    n_clusters = min(3, len(df_cluster_clean))\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    clusters = kmeans.fit_predict(X_scaled)\n",
    "    df_cluster_clean['Cluster'] = clusters\n",
    "    \n",
    "    print(f\"\\nIdentified {n_clusters} company clusters:\")\n",
    "    for i in range(n_clusters):\n",
    "        companies_in_cluster = df_cluster_clean[df_cluster_clean['Cluster'] == i]['Ticker'].tolist()\n",
    "        print(f\"\\nCluster {i+1}: {', '.join(companies_in_cluster)}\")\n",
    "        cluster_means = df_cluster_clean[df_cluster_clean['Cluster'] == i][cluster_features].mean()\n",
    "        print(\"  Characteristics:\")\n",
    "        for feature, mean_val in cluster_means.items():\n",
    "            print(f\"    {feature}: {mean_val:.2f}\")\n",
    "    \n",
    "    # PCA for visualization\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    print(f\"\\nPCA Explained Variance: {pca.explained_variance_ratio_.sum()*100:.1f}% (2 components)\")\n",
    "else:\n",
    "    print(\"\\nInsufficient complete data for clustering analysis\")\n",
    "    X_pca = None\n",
    "\n",
    "# 2. FEATURE IMPORTANCE ANALYSIS\n",
    "print(\"\\n\\n\" + \"=\"*120)\n",
    "print(\"2. FEATURE IMPORTANCE - What Predicts Outperformance?\")\n",
    "print(\"=\"*120)\n",
    "\n",
    "# Create binary target: outperformed vs underperformed (above vs below median return)\n",
    "ml_features_complete = ml_features.dropna(subset=['1Y Return %'])\n",
    "\n",
    "if len(ml_features_complete) >= 3:\n",
    "    median_return = ml_features_complete['1Y Return %'].median()\n",
    "    ml_features_complete['Outperformed'] = (ml_features_complete['1Y Return %'] > median_return).astype(int)\n",
    "    \n",
    "    # Select features for modeling\n",
    "    model_features = ['P/S', 'EV/Sales', 'Revenue Growth %', 'Gross Margin %', 'ROIC %', \n",
    "                     'ROE %', 'Current Ratio', 'Debt/Equity', 'Volatility %', 'Sharpe Ratio', \n",
    "                     'F-Score', 'Altman Z-Score']\n",
    "    \n",
    "    # Filter to available features\n",
    "    available_features = [f for f in model_features if f in ml_features_complete.columns]\n",
    "    df_model = ml_features_complete[['Ticker', 'Outperformed'] + available_features].dropna()\n",
    "    \n",
    "    if len(df_model) >= 3 and len(available_features) > 0:\n",
    "        X = df_model[available_features].values\n",
    "        y = df_model['Outperformed'].values\n",
    "        \n",
    "        # Train Random Forest\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=3)\n",
    "        rf.fit(X, y)\n",
    "        \n",
    "        # Get feature importances\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': available_features,\n",
    "            'Importance': rf.feature_importances_\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        print(\"\\nFeature Importance Rankings:\")\n",
    "        print(feature_importance.to_string(index=False))\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = rf.predict(X)\n",
    "        df_model['Predicted'] = predictions\n",
    "        df_model['Prediction'] = df_model['Predicted'].map({1: 'Outperform', 0: 'Underperform'})\n",
    "        \n",
    "        print(\"\\n\\nModel Predictions:\")\n",
    "        print(df_model[['Ticker', 'Prediction']].to_string(index=False))\n",
    "        \n",
    "        feature_imp_available = True\n",
    "    else:\n",
    "        print(\"\\nInsufficient complete data for feature importance analysis\")\n",
    "        feature_imp_available = False\n",
    "else:\n",
    "    print(\"\\nInsufficient data for outperformance prediction model\")\n",
    "    feature_imp_available = False\n",
    "\n",
    "# 3. VISUALIZATION\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Machine Learning Analysis Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Cluster visualization (PCA)\n",
    "if X_pca is not None and len(df_cluster_clean) > 0:\n",
    "    for i in range(n_clusters):\n",
    "        mask = df_cluster_clean['Cluster'] == i\n",
    "        cluster_points = X_pca[mask]\n",
    "        ax1.scatter(cluster_points[:, 0], cluster_points[:, 1], \n",
    "                   s=200, alpha=0.6, label=f'Cluster {i+1}', edgecolor='black', linewidth=2)\n",
    "        \n",
    "        # Add labels\n",
    "        for idx, ticker in enumerate(df_cluster_clean[mask]['Ticker']):\n",
    "            point_idx = df_cluster_clean[mask].index[idx]\n",
    "            actual_idx = list(df_cluster_clean.index).index(point_idx)\n",
    "            ax1.annotate(ticker, (X_pca[actual_idx, 0], X_pca[actual_idx, 1]),\n",
    "                        fontweight='bold', fontsize=10, ha='center', va='bottom')\n",
    "    \n",
    "    ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% var)', fontweight='bold')\n",
    "    ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% var)', fontweight='bold')\n",
    "    ax1.set_title('Company Clustering (K-Means + PCA)', fontweight='bold', fontsize=13)\n",
    "    ax1.legend(fontsize=9)\n",
    "    ax1.grid(alpha=0.3)\n",
    "else:\n",
    "    ax1.text(0.5, 0.5, 'Insufficient data\\nfor clustering', ha='center', va='center',\n",
    "            transform=ax1.transAxes, fontsize=12)\n",
    "    ax1.axis('off')\n",
    "\n",
    "# Plot 2: Feature Importance\n",
    "if feature_imp_available:\n",
    "    top_features = feature_importance.head(10)\n",
    "    colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(top_features)))\n",
    "    bars = ax2.barh(top_features['Feature'], top_features['Importance'], color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax2.set_xlabel('Importance Score', fontweight='bold')\n",
    "    ax2.set_title('Top Predictive Features (Random Forest)', fontweight='bold', fontsize=13)\n",
    "    ax2.grid(axis='x', alpha=0.3)\n",
    "    for i, (feature, importance) in enumerate(zip(top_features['Feature'], top_features['Importance'])):\n",
    "        ax2.text(importance + 0.01, i, f'{importance:.3f}', va='center', fontweight='bold', fontsize=9)\n",
    "else:\n",
    "    ax2.text(0.5, 0.5, 'Insufficient data\\nfor feature importance', ha='center', va='center',\n",
    "            transform=ax2.transAxes, fontsize=12)\n",
    "    ax2.axis('off')\n",
    "\n",
    "# Plot 3: Correlation Heatmap\n",
    "correlation_features = ['P/S', 'Revenue Growth %', 'ROIC %', 'Volatility %', 'F-Score', '1Y Return %']\n",
    "df_corr = ml_features[correlation_features].dropna()\n",
    "\n",
    "if len(df_corr) > 2:\n",
    "    corr_matrix = df_corr.corr()\n",
    "    im = ax3.imshow(corr_matrix, cmap='RdBu_r', aspect='auto', vmin=-1, vmax=1)\n",
    "    ax3.set_xticks(range(len(correlation_features)))\n",
    "    ax3.set_xticklabels(correlation_features, rotation=45, ha='right')\n",
    "    ax3.set_yticks(range(len(correlation_features)))\n",
    "    ax3.set_yticklabels(correlation_features)\n",
    "    ax3.set_title('Feature Correlation Matrix', fontweight='bold', fontsize=13)\n",
    "    \n",
    "    # Add correlation values\n",
    "    for i in range(len(correlation_features)):\n",
    "        for j in range(len(correlation_features)):\n",
    "            text = ax3.text(j, i, f'{corr_matrix.iloc[i, j]:.2f}',\n",
    "                          ha='center', va='center',\n",
    "                          color='white' if abs(corr_matrix.iloc[i, j]) > 0.5 else 'black',\n",
    "                          fontweight='bold', fontsize=9)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax3)\n",
    "    cbar.set_label('Correlation', fontweight='bold')\n",
    "else:\n",
    "    ax3.text(0.5, 0.5, 'Insufficient data\\nfor correlation analysis', ha='center', va='center',\n",
    "            transform=ax3.transAxes, fontsize=12)\n",
    "    ax3.axis('off')\n",
    "\n",
    "# Plot 4: Model Performance Summary\n",
    "if feature_imp_available and len(df_model) > 0:\n",
    "    # Create summary statistics\n",
    "    summary_data = {\n",
    "        'Metric': ['Total Companies', 'Training Samples', 'Features Used', 'Predicted Outperform', \n",
    "                  'Predicted Underperform', 'Top Feature Importance'],\n",
    "        'Value': [\n",
    "            len(ml_features),\n",
    "            len(df_model),\n",
    "            len(available_features),\n",
    "            (df_model['Predicted'] == 1).sum(),\n",
    "            (df_model['Predicted'] == 0).sum(),\n",
    "            f\"{feature_importance.iloc[0]['Feature']}\\n({feature_importance.iloc[0]['Importance']:.3f})\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    ax4.axis('off')\n",
    "    table = ax4.table(cellText=[[k, v] for k, v in zip(summary_data['Metric'], summary_data['Value'])],\n",
    "                     colLabels=['Metric', 'Value'],\n",
    "                     cellLoc='left',\n",
    "                     loc='center',\n",
    "                     colWidths=[0.6, 0.4])\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1, 2)\n",
    "    \n",
    "    # Style header\n",
    "    for i in range(2):\n",
    "        table[(0, i)].set_facecolor('#4ECDC4')\n",
    "        table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "    \n",
    "    # Alternate row colors\n",
    "    for i in range(1, len(summary_data['Metric']) + 1):\n",
    "        if i % 2 == 0:\n",
    "            table[(i, 0)].set_facecolor('#f0f0f0')\n",
    "            table[(i, 1)].set_facecolor('#f0f0f0')\n",
    "    \n",
    "    ax4.set_title('ML Model Summary', fontweight='bold', fontsize=13, pad=20)\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'Insufficient data\\nfor model summary', ha='center', va='center',\n",
    "            transform=ax4.transAxes, fontsize=12)\n",
    "    ax4.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Machine learning analysis complete\")\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"KEY INSIGHTS FROM ML ANALYSIS:\")\n",
    "print(\"=\"*120)\n",
    "if feature_imp_available:\n",
    "    print(f\"â€¢ Most predictive feature: {feature_importance.iloc[0]['Feature']}\")\n",
    "    print(f\"â€¢ Companies predicted to outperform: {', '.join(df_model[df_model['Predicted']==1]['Ticker'].tolist())}\")\n",
    "    print(f\"â€¢ Model uses {len(available_features)} fundamental & technical features\")\n",
    "else:\n",
    "    print(\"â€¢ Insufficient data for predictive modeling with current dataset\")\n",
    "print(\"â€¢ Clustering reveals natural groupings based on fundamental characteristics\")\n",
    "print(\"â€¢ Feature correlations identify redundant and complementary metrics\")\n",
    "print(\"=\"*120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755953cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T17:46:20.908868Z",
     "iopub.status.busy": "2026-01-08T17:46:20.907798Z",
     "iopub.status.idle": "2026-01-08T17:46:20.931753Z",
     "shell.execute_reply": "2026-01-08T17:46:20.930350Z"
    },
    "papermill": {
     "duration": 0.048784,
     "end_time": "2026-01-08T17:46:20.934250",
     "exception": false,
     "start_time": "2026-01-08T17:46:20.885466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create master comparison dataframe combining all metrics\n",
    "df_comparison = df_metrics.merge(df_performance[['Ticker', '1Y Return %', 'Volatility % (Ann.)']], on='Ticker')\n",
    "df_comparison = df_comparison.merge(df_risk[['Ticker', 'Max Drawdown %', 'Sharpe Ratio']], on='Ticker')\n",
    "\n",
    "# Select key metrics for dashboard\n",
    "dashboard_cols = [\n",
    "    'Ticker', 'Company', 'Category',\n",
    "    'Current Price', 'Market Cap (M)',\n",
    "    'Revenue TTM (M)', 'Revenue Growth %',\n",
    "    'Gross Margin %', 'Operating Margin %',\n",
    "    'EV/Sales', 'P/S',\n",
    "    '1Y Return %', 'Volatility % (Ann.)',\n",
    "    'Max Drawdown %', 'Sharpe Ratio',\n",
    "    'Analyst Target', 'Upside to Target %'\n",
    "]\n",
    "\n",
    "df_dashboard = df_comparison[dashboard_cols].copy()\n",
    "\n",
    "print(\"\\n\" + \"=\"*140)\n",
    "print(\"COMPREHENSIVE COMPARATIVE ANALYSIS DASHBOARD\")\n",
    "print(\"=\"*140)\n",
    "print(df_dashboard.to_string(index=False))\n",
    "print(\"=\"*140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce6a44a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T17:46:20.978271Z",
     "iopub.status.busy": "2026-01-08T17:46:20.977836Z",
     "iopub.status.idle": "2026-01-08T17:46:21.328005Z",
     "shell.execute_reply": "2026-01-08T17:46:21.326886Z"
    },
    "papermill": {
     "duration": 0.376061,
     "end_time": "2026-01-08T17:46:21.330907",
     "exception": false,
     "start_time": "2026-01-08T17:46:20.954846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualization 6: Comprehensive Comparison Heatmap\n",
    "\n",
    "# Prepare data for heatmap - normalize metrics to 0-1 scale for comparison\n",
    "heatmap_metrics = ['Revenue Growth %', 'Gross Margin %', '1Y Return %', 'EV/Sales', \n",
    "                   'Sharpe Ratio', 'Max Drawdown %']\n",
    "\n",
    "df_heatmap = df_comparison[['Ticker'] + heatmap_metrics].set_index('Ticker')\n",
    "\n",
    "# Normalize each column to 0-100 scale (higher is better)\n",
    "# For metrics where lower is better (EV/Sales, Max Drawdown), invert the scale\n",
    "df_normalized = df_heatmap.copy()\n",
    "\n",
    "for col in df_normalized.columns:\n",
    "    if col in ['Max Drawdown %']:  # Lower is better\n",
    "        df_normalized[col] = 100 - ((df_heatmap[col] - df_heatmap[col].min()) / \n",
    "                                    (df_heatmap[col].max() - df_heatmap[col].min()) * 100)\n",
    "    elif col in ['EV/Sales']:  # Lower is better for valuation\n",
    "        df_normalized[col] = 100 - ((df_heatmap[col] - df_heatmap[col].min()) / \n",
    "                                    (df_heatmap[col].max() - df_heatmap[col].min()) * 100)\n",
    "    else:  # Higher is better\n",
    "        df_normalized[col] = ((df_heatmap[col] - df_heatmap[col].min()) / \n",
    "                             (df_heatmap[col].max() - df_heatmap[col].min()) * 100)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(df_normalized.T, annot=True, fmt='.0f', cmap='RdYlGn', center=50,\n",
    "           cbar_kws={'label': 'Relative Score (0-100)'}, linewidths=0.5, ax=ax)\n",
    "\n",
    "ax.set_title('Comparative Performance Heatmap (Normalized Scores)', \n",
    "            fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_xlabel('Companies', fontweight='bold', fontsize=12)\n",
    "ax.set_ylabel('Metrics', fontweight='bold', fontsize=12)\n",
    "\n",
    "# Highlight AIRO column\n",
    "airo_idx = list(df_normalized.index).index('AIRO')\n",
    "for i in range(len(df_normalized.columns)):\n",
    "    rect = plt.Rectangle((airo_idx, i), 1, 1, fill=False, edgecolor='red', linewidth=3)\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Comparative heatmap generated\")\n",
    "print(\"\\nNote: Heatmap shows normalized scores (0-100) where green = better, red = worse\")\n",
    "print(\"AIRO column is highlighted in red border for easy identification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7828fdfa",
   "metadata": {
    "papermill": {
     "duration": 0.021437,
     "end_time": "2026-01-08T17:46:21.376313",
     "exception": false,
     "start_time": "2026-01-08T17:46:21.354876",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8. Investment Recommendation Framework\n",
    "\n",
    "Final synthesis of all quantitative analysis to provide actionable investment recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ff5519",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T17:46:21.421627Z",
     "iopub.status.busy": "2026-01-08T17:46:21.421256Z",
     "iopub.status.idle": "2026-01-08T17:46:21.449518Z",
     "shell.execute_reply": "2026-01-08T17:46:21.448186Z"
    },
    "papermill": {
     "duration": 0.054167,
     "end_time": "2026-01-08T17:46:21.452466",
     "exception": false,
     "start_time": "2026-01-08T17:46:21.398299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scoring framework for investment decision\n",
    "# Each factor weighted by importance (total = 100%)\n",
    "\n",
    "def calculate_investment_score(ticker_data):\n",
    "    \"\"\"\n",
    "    Calculate composite investment score (0-100) based on weighted factors:\n",
    "    - Valuation Attractiveness (25%)\n",
    "    - Growth Potential (25%)\n",
    "    - Profitability (20%)\n",
    "    - Risk-Adjusted Returns (20%)\n",
    "    - Momentum (10%)\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    \n",
    "    # Valuation (25%) - Lower EV/Sales is better\n",
    "    ev_sales = ticker_data.get('EV/Sales', np.nan)\n",
    "    if not np.isnan(ev_sales):\n",
    "        # Score: 100 if EV/Sales < 2, declining to 0 at EV/Sales > 10\n",
    "        val_score = max(0, min(100, (10 - ev_sales) / 8 * 100))\n",
    "        score += val_score * 0.25\n",
    "    \n",
    "    # Growth (25%) - Higher revenue growth is better\n",
    "    growth = ticker_data.get('Revenue Growth %', np.nan)\n",
    "    if not np.isnan(growth):\n",
    "        # Score: 100 if growth > 50%, declining to 0 at growth < -20%\n",
    "        growth_score = max(0, min(100, (growth + 20) / 70 * 100))\n",
    "        score += growth_score * 0.25\n",
    "    \n",
    "    # Profitability (20%) - Higher gross margin is better\n",
    "    margin = ticker_data.get('Gross Margin %', np.nan)\n",
    "    if not np.isnan(margin):\n",
    "        # Score: 100 if margin > 60%, declining to 0 at margin < 0%\n",
    "        margin_score = max(0, min(100, margin / 60 * 100))\n",
    "        score += margin_score * 0.20\n",
    "    \n",
    "    # Risk-Adjusted Returns (20%) - Higher Sharpe is better\n",
    "    sharpe = ticker_data.get('Sharpe Ratio', np.nan)\n",
    "    if not np.isnan(sharpe):\n",
    "        # Score: 100 if Sharpe > 2, declining to 0 at Sharpe < -1\n",
    "        sharpe_score = max(0, min(100, (sharpe + 1) / 3 * 100))\n",
    "        score += sharpe_score * 0.20\n",
    "    \n",
    "    # Momentum (10%) - Positive recent returns\n",
    "    return_1y = ticker_data.get('1Y Return %', np.nan)\n",
    "    if not np.isnan(return_1y):\n",
    "        # Score: 100 if return > 100%, declining to 0 at return < -50%\n",
    "        momentum_score = max(0, min(100, (return_1y + 50) / 150 * 100))\n",
    "        score += momentum_score * 0.10\n",
    "    \n",
    "    return score\n",
    "\n",
    "# Calculate scores for all companies\n",
    "scores = []\n",
    "for ticker in tickers:\n",
    "    ticker_data = df_comparison[df_comparison['Ticker'] == ticker].iloc[0].to_dict()\n",
    "    score = calculate_investment_score(ticker_data)\n",
    "    scores.append({\n",
    "        'Ticker': ticker,\n",
    "        'Company': companies[ticker]['name'],\n",
    "        'Investment Score': score,\n",
    "        'Rating': 'STRONG BUY' if score >= 75 else 'BUY' if score >= 60 else 'HOLD' if score >= 40 else 'SELL'\n",
    "    })\n",
    "\n",
    "df_scores = pd.DataFrame(scores).sort_values('Investment Score', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"QUANTITATIVE INVESTMENT SCORING & RECOMMENDATIONS\")\n",
    "print(\"=\"*100)\n",
    "print(df_scores.to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "\n",
    "# AIRO-specific recommendation\n",
    "airo_score = df_scores[df_scores['Ticker'] == 'AIRO'].iloc[0]\n",
    "airo_rank = df_scores[df_scores['Ticker'] == 'AIRO'].index[0] + 1\n",
    "\n",
    "print(\"\\n\" + \"-\"*100)\n",
    "print(\"AIRO GROUP HOLDINGS - FINAL INVESTMENT RECOMMENDATION\")\n",
    "print(\"-\"*100)\n",
    "print(f\"Quantitative Score:     {airo_score['Investment Score']:.1f}/100\")\n",
    "print(f\"Relative Rank:          #{airo_rank} of {len(tickers)} companies analyzed\")\n",
    "print(f\"Quantitative Rating:    {airo_score['Rating']}\")\n",
    "print(\"\\nKey Findings:\")\n",
    "print(f\"  â€¢ Current Price:      ${df_metrics[df_metrics['Ticker']=='AIRO']['Current Price'].values[0]:.2f}\")\n",
    "print(f\"  â€¢ SOTP Target Price:  ${implied_price_sotp:.2f} ({upside:+.1f}% upside)\")\n",
    "print(f\"  â€¢ Analyst Target:     ${df_metrics[df_metrics['Ticker']=='AIRO']['Analyst Target'].values[0]:.2f}\")\n",
    "print(f\"  â€¢ EV/Sales Multiple:  {df_metrics[df_metrics['Ticker']=='AIRO']['EV/Sales'].values[0]:.2f}x\")\n",
    "print(f\"  â€¢ Gross Margin:       {df_metrics[df_metrics['Ticker']=='AIRO']['Gross Margin %'].values[0]:.1f}%\")\n",
    "print(f\"  â€¢ 1Y Performance:     {df_performance[df_performance['Ticker']=='AIRO']['1Y Return %'].values[0]:+.1f}%\")\n",
    "print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d76a54a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T17:46:21.498277Z",
     "iopub.status.busy": "2026-01-08T17:46:21.497238Z",
     "iopub.status.idle": "2026-01-08T17:46:22.640777Z",
     "shell.execute_reply": "2026-01-08T17:46:22.639649Z"
    },
    "papermill": {
     "duration": 1.17163,
     "end_time": "2026-01-08T17:46:22.645402",
     "exception": false,
     "start_time": "2026-01-08T17:46:21.473772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualization 7: Investment Scoring Dashboard\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Investment Scoring & Recommendation Dashboard', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Overall Investment Scores\n",
    "df_plot = df_scores.sort_values('Investment Score', ascending=True)\n",
    "colors = ['#FF4444' if x == 'AIRO' else '#4444FF' for x in df_plot['Ticker']]\n",
    "bars = ax1.barh(df_plot['Ticker'], df_plot['Investment Score'], color=colors, alpha=0.7, edgecolor='black')\n",
    "ax1.set_xlabel('Investment Score (0-100)', fontweight='bold')\n",
    "ax1.set_title('Quantitative Investment Score Ranking', fontweight='bold', fontsize=13)\n",
    "ax1.axvline(60, color='orange', linestyle='--', alpha=0.5, label='BUY Threshold (60)')\n",
    "ax1.axvline(75, color='green', linestyle='--', alpha=0.5, label='STRONG BUY Threshold (75)')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "for i, (ticker, score, rating) in enumerate(zip(df_plot['Ticker'], df_plot['Investment Score'], df_plot['Rating'])):\n",
    "    ax1.text(score + 2, i, f'{score:.0f} - {rating}', va='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "# Plot 2: Valuation vs Growth Matrix\n",
    "for idx, row in df_comparison.iterrows():\n",
    "    color = '#FF4444' if row['Ticker'] == 'AIRO' else '#4444FF'\n",
    "    size = 300 if row['Ticker'] == 'AIRO' else 150\n",
    "    ax2.scatter(row['EV/Sales'], row['Revenue Growth %'], s=size, color=color,\n",
    "               alpha=0.6, edgecolor='black', linewidth=2)\n",
    "    ax2.annotate(row['Ticker'], (row['EV/Sales'], row['Revenue Growth %']),\n",
    "                fontweight='bold', fontsize=11, ha='center', va='bottom')\n",
    "\n",
    "# Add quadrant lines\n",
    "median_ev = df_comparison['EV/Sales'].median()\n",
    "median_growth = df_comparison['Revenue Growth %'].median()\n",
    "ax2.axhline(median_growth, color='gray', linestyle='--', alpha=0.5)\n",
    "ax2.axvline(median_ev, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Label quadrants\n",
    "ax2.text(0.95, 0.95, 'High Growth\\nExpensive', transform=ax2.transAxes, ha='right', va='top',\n",
    "        fontsize=9, style='italic', alpha=0.5)\n",
    "ax2.text(0.05, 0.95, 'High Growth\\nCheap (BEST)', transform=ax2.transAxes, ha='left', va='top',\n",
    "        fontsize=9, fontweight='bold', alpha=0.7)\n",
    "ax2.text(0.05, 0.05, 'Low Growth\\nCheap', transform=ax2.transAxes, ha='left', va='bottom',\n",
    "        fontsize=9, style='italic', alpha=0.5)\n",
    "ax2.text(0.95, 0.05, 'Low Growth\\nExpensive (WORST)', transform=ax2.transAxes, ha='right', va='bottom',\n",
    "        fontsize=9, fontweight='bold', alpha=0.7)\n",
    "\n",
    "ax2.set_xlabel('EV/Sales Multiple (Lower is Cheaper)', fontweight='bold')\n",
    "ax2.set_ylabel('Revenue Growth % (Higher is Better)', fontweight='bold')\n",
    "ax2.set_title('Valuation vs Growth Matrix', fontweight='bold', fontsize=13)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "# Plot 3: Risk-Reward Efficiency\n",
    "for idx, row in df_risk.iterrows():\n",
    "    color = '#FF4444' if row['Ticker'] == 'AIRO' else '#44AA44'\n",
    "    size = 300 if row['Ticker'] == 'AIRO' else 150\n",
    "    # Plot return/volatility (higher is better)\n",
    "    return_vol_ratio = row['Ann. Return %'] / row['Ann. Volatility %'] if row['Ann. Volatility %'] != 0 else 0\n",
    "    ax3.scatter(row['Ann. Volatility %'], row['Ann. Return %'], s=size, color=color,\n",
    "               alpha=0.6, edgecolor='black', linewidth=2)\n",
    "    ax3.annotate(row['Ticker'], (row['Ann. Volatility %'], row['Ann. Return %']),\n",
    "                fontweight='bold', fontsize=11, ha='center', va='bottom')\n",
    "\n",
    "ax3.axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax3.set_xlabel('Risk (Annualized Volatility %)', fontweight='bold')\n",
    "ax3.set_ylabel('Reward (Annualized Return %)', fontweight='bold')\n",
    "ax3.set_title('Risk-Reward Efficiency', fontweight='bold', fontsize=13)\n",
    "ax3.grid(alpha=0.3)\n",
    "\n",
    "# Plot 4: Position Sizing Recommendation\n",
    "# Based on score and risk profile\n",
    "position_recs = []\n",
    "for ticker in tickers:\n",
    "    score = df_scores[df_scores['Ticker'] == ticker]['Investment Score'].values[0]\n",
    "    vol = df_risk[df_risk['Ticker'] == ticker]['Ann. Volatility %'].values[0]\n",
    "    \n",
    "    # Base position on score, adjusted down for high volatility\n",
    "    base_position = score / 100 * 10  # Max 10% position\n",
    "    vol_adjustment = 1 - (vol / 100)  # Reduce for high volatility\n",
    "    recommended_position = base_position * vol_adjustment\n",
    "    \n",
    "    position_recs.append({\n",
    "        'Ticker': ticker,\n",
    "        'Recommended Position %': recommended_position\n",
    "    })\n",
    "\n",
    "df_positions = pd.DataFrame(position_recs).sort_values('Recommended Position %', ascending=True)\n",
    "colors = ['#FF4444' if x == 'AIRO' else '#AA44AA' for x in df_positions['Ticker']]\n",
    "bars = ax4.barh(df_positions['Ticker'], df_positions['Recommended Position %'], \n",
    "               color=colors, alpha=0.7, edgecolor='black')\n",
    "ax4.set_xlabel('Recommended Portfolio Weight (%)', fontweight='bold')\n",
    "ax4.set_title('Position Sizing Recommendation (10% Max)', fontweight='bold', fontsize=13)\n",
    "ax4.grid(axis='x', alpha=0.3)\n",
    "for i, (ticker, pos) in enumerate(zip(df_positions['Ticker'], df_positions['Recommended Position %'])):\n",
    "    ax4.text(pos + 0.1, i, f'{pos:.1f}%', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Investment recommendation dashboard generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa29a11",
   "metadata": {
    "papermill": {
     "duration": 0.024788,
     "end_time": "2026-01-08T17:46:22.696936",
     "exception": false,
     "start_time": "2026-01-08T17:46:22.672148",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 9. Export Results\n",
    "\n",
    "Save all analysis results to CSV files for further analysis or integration into portfolio management systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c6b27b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-08T17:46:22.747175Z",
     "iopub.status.busy": "2026-01-08T17:46:22.746745Z",
     "iopub.status.idle": "2026-01-08T17:46:22.763439Z",
     "shell.execute_reply": "2026-01-08T17:46:22.762089Z"
    },
    "papermill": {
     "duration": 0.045681,
     "end_time": "2026-01-08T17:46:22.765700",
     "exception": false,
     "start_time": "2026-01-08T17:46:22.720019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Export comprehensive results\n",
    "output_dir = '/mnt/user-data/outputs'\n",
    "\n",
    "try:\n",
    "    # Master comparison export\n",
    "    df_comparison.to_csv(f'{output_dir}/AIRO_Competitor_Comparison.csv', index=False)\n",
    "    print(f\"âœ“ Exported: AIRO_Competitor_Comparison.csv\")\n",
    "    \n",
    "    # Performance metrics\n",
    "    df_performance.to_csv(f'{output_dir}/AIRO_Performance_Metrics.csv', index=False)\n",
    "    print(f\"âœ“ Exported: AIRO_Performance_Metrics.csv\")\n",
    "    \n",
    "    # Risk analysis\n",
    "    df_risk.to_csv(f'{output_dir}/AIRO_Risk_Analysis.csv', index=False)\n",
    "    print(f\"âœ“ Exported: AIRO_Risk_Analysis.csv\")\n",
    "    \n",
    "    # SOTP valuation\n",
    "    df_sotp.to_csv(f'{output_dir}/AIRO_SOTP_Valuation.csv', index=False)\n",
    "    print(f\"âœ“ Exported: AIRO_SOTP_Valuation.csv\")\n",
    "    \n",
    "    # Investment scores\n",
    "    df_scores.to_csv(f'{output_dir}/AIRO_Investment_Scores.csv', index=False)\n",
    "    print(f\"âœ“ Exported: AIRO_Investment_Scores.csv\")\n",
    "    \n",
    "    print(\"\\nâœ“ All analysis results exported successfully\")\n",
    "    print(f\"\\nFiles saved to: {output_dir}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâœ— Export error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7859d18",
   "metadata": {
    "papermill": {
     "duration": 0.02334,
     "end_time": "2026-01-08T17:46:22.814030",
     "exception": false,
     "start_time": "2026-01-08T17:46:22.790690",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Summary & Key Takeaways\n",
    "\n",
    "This comprehensive quantitative analysis of AIRO Group Holdings and its competitors provides multiple valuation perspectives:\n",
    "\n",
    "### Valuation Conclusions:\n",
    "1. **Trading Comparables:** AIRO trades at 2.5x EV/Sales vs peer averages of 3-6x\n",
    "2. **Sum-of-Parts:** Implies fair value around $12-15/share based on segment analysis\n",
    "3. **Risk-Adjusted Returns:** Volatility and drawdown metrics suggest higher risk profile\n",
    "4. **Growth-Value Matrix:** Positioned as moderate growth at reasonable valuation\n",
    "\n",
    "### Investment Recommendation:\n",
    "**Rating:** HOLD (Quantitative Score: See above)  \n",
    "**Position Sizing:** 1-2% of portfolio maximum (high risk)\n",
    "\n",
    "### Key Strengths:\n",
    "- Diversified revenue streams across drones, avionics, training, and eVTOL\n",
    "- Strong gross margins (58%+) demonstrating pricing power\n",
    "- Nordic JV provides production scale opportunity\n",
    "- Trump administration's pro-drone policies create regulatory tailwind\n",
    "\n",
    "### Key Risks:\n",
    "- Extreme revenue lumpiness creates forecasting difficulty\n",
    "- Margin erosion trend (68% â†’ 44%) requires investigation\n",
    "- 3-5 years behind eVTOL leaders (Joby, Archer)\n",
    "- Significant execution risk across four disparate businesses\n",
    "- Likely requires additional capital raise (dilution)\n",
    "\n",
    "### Next Steps:\n",
    "1. Monitor Q4 2025 results (critical for validating $24.5M booked revenue)\n",
    "2. Assess Nordic JV integration progress (production scaling timeline)\n",
    "3. Track gross margin trends (return to 60%+ would be bullish signal)\n",
    "4. Watch for strategic decision on eVTOL segment (sell/spin/scale back)\n",
    "5. Set price alerts: Buy signal at $8-9, Sell signal above $15-16\n",
    "\n",
    "---\n",
    "\n",
    "**Analysis Completed:** January 8, 2026  \n",
    "**Methodology:** Goldman Sachs-style equity research with quantitative scoring  \n",
    "**Data Sources:** Yahoo Finance, company filings, analyst reports  \n",
    "\n",
    "*This analysis is for informational purposes only and does not constitute investment advice.*"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31239,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 30.398373,
   "end_time": "2026-01-08T17:46:23.961005",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-08T17:45:53.562632",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
