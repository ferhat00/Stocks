{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Data Incremental Updater for Kaggle\n",
    "\n",
    "**Features:**\n",
    "- Loads Google Drive credentials from Kaggle dataset\n",
    "- Downloads existing CSV files from Google Drive\n",
    "- Appends only new trading days since last update\n",
    "- Uploads updated files back to Google Drive\n",
    "\n",
    "**Setup Requirements:**\n",
    "1. Add `google-drive-token` dataset to this notebook (contains token.pickle)\n",
    "2. Run once - subsequent runs will be incremental\n",
    "\n",
    "**Google Drive Folder Structure:**\n",
    "```\n",
    "StockData/\n",
    "‚îú‚îÄ‚îÄ SP500/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ sp500_adj_close.csv\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ sp500_close.csv\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ sp500_open.csv\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ sp500_high.csv\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ sp500_low.csv\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ sp500_volume.csv\n",
    "‚îî‚îÄ‚îÄ NASDAQ/\n",
    "    ‚îú‚îÄ‚îÄ nasdaq_adj_close.csv\n",
    "    ‚îî‚îÄ‚îÄ ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install yfinance pandas numpy google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client tqdm -q\n",
    "\n",
    "print(\"‚úÖ All packages installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pickle\n",
    "import io\n",
    "import os\n",
    "from pathlib import Path\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaFileUpload, MediaIoBaseDownload, MediaIoBaseUpload\n",
    "from google.auth.transport.requests import Request\n",
    "\n",
    "print(\"‚úÖ Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Google Drive folder name where your stock data is stored\n",
    "# The script will create this folder if it doesn't exist\n",
    "DRIVE_FOLDER_NAME = 'StockData'\n",
    "\n",
    "# Path to token.pickle from Kaggle dataset\n",
    "# Add your google-drive-token dataset to this notebook\n",
    "TOKEN_PATH = '/kaggle/input/google-drive-token/token.pickle'\n",
    "\n",
    "# Date configuration\n",
    "START_DATE = '2020-01-01'  # Used only if no existing data found\n",
    "END_DATE = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# File types to update\n",
    "PRICE_TYPES = ['adj_close', 'close', 'open', 'high', 'low', 'volume']\n",
    "\n",
    "print(f\"üìÅ Google Drive folder: {DRIVE_FOLDER_NAME}\")\n",
    "print(f\"üìÖ End date: {END_DATE}\")\n",
    "print(f\"üîë Token path: {TOKEN_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Google Drive Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LOAD CREDENTIALS FROM KAGGLE DATASET\n",
    "# ============================================================================\n",
    "\n",
    "def load_credentials():\n",
    "    \"\"\"Load Google Drive credentials from Kaggle dataset\"\"\"\n",
    "    \n",
    "    if not os.path.exists(TOKEN_PATH):\n",
    "        print(\"‚ùå ERROR: token.pickle not found!\")\n",
    "        print(\"\\nüìã Setup Instructions:\")\n",
    "        print(\"1. Make sure you've added the 'google-drive-token' dataset to this notebook\")\n",
    "        print(\"2. Click 'Add Data' ‚Üí Search for your dataset ‚Üí Add\")\n",
    "        print(\"3. Re-run this cell\")\n",
    "        raise FileNotFoundError(f\"Token file not found at {TOKEN_PATH}\")\n",
    "    \n",
    "    print(f\"üîë Loading credentials from {TOKEN_PATH}...\")\n",
    "    \n",
    "    with open(TOKEN_PATH, 'rb') as token:\n",
    "        creds = pickle.load(token)\n",
    "    \n",
    "    # Refresh token if expired\n",
    "    if creds and creds.expired and creds.refresh_token:\n",
    "        print(\"üîÑ Refreshing expired token...\")\n",
    "        creds.refresh(Request())\n",
    "        print(\"‚úÖ Token refreshed\")\n",
    "    \n",
    "    print(\"‚úÖ Credentials loaded successfully\")\n",
    "    return creds\n",
    "\n",
    "def get_drive_service():\n",
    "    \"\"\"Initialize Google Drive API service\"\"\"\n",
    "    creds = load_credentials()\n",
    "    service = build('drive', 'v3', credentials=creds)\n",
    "    print(\"‚úÖ Google Drive service initialized\")\n",
    "    return service\n",
    "\n",
    "# Initialize service\n",
    "drive_service = get_drive_service()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Drive Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GOOGLE DRIVE HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def find_or_create_folder(service, folder_name, parent_id=None):\n",
    "    \"\"\"Find existing folder or create new one in Google Drive\"\"\"\n",
    "    \n",
    "    # Search for folder\n",
    "    query = f\"name='{folder_name}' and mimeType='application/vnd.google-apps.folder' and trashed=false\"\n",
    "    if parent_id:\n",
    "        query += f\" and '{parent_id}' in parents\"\n",
    "    \n",
    "    results = service.files().list(\n",
    "        q=query,\n",
    "        spaces='drive',\n",
    "        fields='files(id, name)'\n",
    "    ).execute()\n",
    "    \n",
    "    folders = results.get('files', [])\n",
    "    \n",
    "    if folders:\n",
    "        print(f\"‚úÖ Found existing folder: {folder_name} (ID: {folders[0]['id']})\")\n",
    "        return folders[0]['id']\n",
    "    \n",
    "    # Create folder if not found\n",
    "    file_metadata = {\n",
    "        'name': folder_name,\n",
    "        'mimeType': 'application/vnd.google-apps.folder'\n",
    "    }\n",
    "    \n",
    "    if parent_id:\n",
    "        file_metadata['parents'] = [parent_id]\n",
    "    \n",
    "    folder = service.files().create(\n",
    "        body=file_metadata,\n",
    "        fields='id'\n",
    "    ).execute()\n",
    "    \n",
    "    print(f\"üìÅ Created folder: {folder_name} (ID: {folder['id']})\")\n",
    "    return folder['id']\n",
    "\n",
    "def find_file_in_folder(service, filename, folder_id):\n",
    "    \"\"\"Find a file by name in specific folder\"\"\"\n",
    "    \n",
    "    query = f\"name='{filename}' and '{folder_id}' in parents and trashed=false\"\n",
    "    \n",
    "    results = service.files().list(\n",
    "        q=query,\n",
    "        spaces='drive',\n",
    "        fields='files(id, name, modifiedTime)'\n",
    "    ).execute()\n",
    "    \n",
    "    files = results.get('files', [])\n",
    "    return files[0] if files else None\n",
    "\n",
    "def download_csv_from_drive(service, file_id, filename):\n",
    "    \"\"\"Download CSV file from Google Drive and return as DataFrame\"\"\"\n",
    "    \n",
    "    try:\n",
    "        request = service.files().get_media(fileId=file_id)\n",
    "        fh = io.BytesIO()\n",
    "        downloader = MediaIoBaseDownload(fh, request)\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            status, done = downloader.next_chunk()\n",
    "        \n",
    "        fh.seek(0)\n",
    "        df = pd.read_csv(fh, index_col=0, parse_dates=True)\n",
    "        print(f\"‚úÖ Downloaded {filename}: {df.shape}\")\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error downloading {filename}: {e}\")\n",
    "        return None\n",
    "\n",
    "def upload_csv_to_drive(service, df, filename, folder_id, file_id=None):\n",
    "    \"\"\"Upload DataFrame as CSV to Google Drive (update if exists, create if new)\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Convert DataFrame to CSV bytes\n",
    "        csv_buffer = io.BytesIO()\n",
    "        df.to_csv(csv_buffer)\n",
    "        csv_buffer.seek(0)\n",
    "        \n",
    "        media = MediaIoBaseUpload(\n",
    "            csv_buffer,\n",
    "            mimetype='text/csv',\n",
    "            resumable=True\n",
    "        )\n",
    "        \n",
    "        if file_id:\n",
    "            # Update existing file\n",
    "            updated_file = service.files().update(\n",
    "                fileId=file_id,\n",
    "                media_body=media\n",
    "            ).execute()\n",
    "            print(f\"‚úÖ Updated {filename} in Drive\")\n",
    "        else:\n",
    "            # Create new file\n",
    "            file_metadata = {\n",
    "                'name': filename,\n",
    "                'parents': [folder_id]\n",
    "            }\n",
    "            new_file = service.files().create(\n",
    "                body=file_metadata,\n",
    "                media_body=media,\n",
    "                fields='id'\n",
    "            ).execute()\n",
    "            print(f\"‚úÖ Created {filename} in Drive (ID: {new_file['id']})\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error uploading {filename}: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"‚úÖ Google Drive helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ticker Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TICKER LISTS (S&P 500 & NASDAQ-100)\n",
    "# ============================================================================\n",
    "\n",
    "def get_sp500_tickers():\n",
    "    \"\"\"S&P 500 ticker list\"\"\"\n",
    "    tickers = [\n",
    "        'AAPL', 'MSFT', 'NVDA', 'AMZN', 'GOOGL', 'META', 'GOOG', 'BRK-B', 'AVGO', 'LLY',\n",
    "        'JPM', 'TSLA', 'V', 'UNH', 'XOM', 'WMT', 'MA', 'COST', 'HD', 'PG',\n",
    "        'JNJ', 'NFLX', 'BAC', 'ABBV', 'CRM', 'ORCL', 'CVX', 'KO', 'AMD', 'MRK',\n",
    "        'ADBE', 'PEP', 'TMO', 'ACN', 'LIN', 'CSCO', 'MCD', 'ABT', 'WFC', 'PM',\n",
    "        'GE', 'QCOM', 'TXN', 'IBM', 'INTU', 'CAT', 'VZ', 'DHR', 'CMCSA', 'AMGN',\n",
    "        'ISRG', 'NOW', 'PFE', 'NEE', 'AMAT', 'DIS', 'SPGI', 'HON', 'UBER', 'RTX',\n",
    "        'UNP', 'LOW', 'AXP', 'T', 'COP', 'BKNG', 'ELV', 'SYK', 'MS', 'PLD',\n",
    "        'BLK', 'UPS', 'GILD', 'LRCX', 'BA', 'VRTX', 'PANW', 'ADI', 'DE', 'MDT',\n",
    "        'TJX', 'LMT', 'GS', 'SCHW', 'CI', 'MMC', 'ADP', 'CB', 'MDLZ', 'C',\n",
    "        'REGN', 'BMY', 'AMT', 'SO', 'BSX', 'PGR', 'SLB', 'BX', 'ETN', 'SBUX',\n",
    "        'KLAC', 'MU', 'ZTS', 'FI', 'EQIX', 'DUK', 'EOG', 'SNPS', 'ICE', 'CME',\n",
    "        'CDNS', 'WM', 'PH', 'NOC', 'INTC', 'GD', 'APD', 'MCK', 'ITW', 'PYPL',\n",
    "        'MAR', 'CL', 'CMG', 'MSI', 'EMR', 'CSX', 'MCO', 'APH', 'AON', 'USB',\n",
    "        'WELL', 'NXPI', 'TT', 'MMM', 'SHW', 'TDG', 'ADSK', 'COF', 'CVS', 'ECL',\n",
    "        'HCA', 'NSC', 'FCX', 'ROP', 'FDX', 'GM', 'PCAR', 'AFL', 'AJG', 'ABNB',\n",
    "        'MPC', 'PSX', 'TGT', 'CARR', 'ORLY', 'AZO', 'JCI', 'AIG', 'AMP', 'GIS',\n",
    "        'MET', 'SRE', 'D', 'NEM', 'TEL', 'TRV', 'O', 'KMB', 'HLT', 'SPG',\n",
    "        'VLO', 'CCI', 'CPRT', 'OXY', 'FICO', 'PAYX', 'MSCI', 'FTNT', 'AEP', 'MCHP',\n",
    "        'RSG', 'BK', 'ALL', 'PCG', 'SYY', 'PSA', 'KR', 'CTAS', 'CTVA', 'BKR',\n",
    "        'DHI', 'JBL', 'PRU', 'EA', 'DLR', 'TRGP', 'CMI', 'FAST', 'HES', 'HSY',\n",
    "        'EW', 'KMI', 'RCL', 'CHTR', 'IT', 'KVUE', 'PEG', 'COR', 'OTIS', 'URI',\n",
    "        'YUM', 'MNST', 'KDP', 'DD', 'DAL', 'GEHC', 'ODFL', 'EXC', 'LULU', 'GWW',\n",
    "        'PWR', 'ACGL', 'IDXX', 'NDAQ', 'VMC', 'AME', 'GLW', 'A', 'XEL', 'CTSH',\n",
    "        'FANG', 'ED', 'WAB', 'MLM', 'HWM', 'ROK', 'WMB', 'HIG', 'DXCM', 'LEN',\n",
    "        'HAL', 'IQV', 'EXR', 'DOW', 'IR', 'STZ', 'ANSS', 'ROST', 'RMD', 'CCEP',\n",
    "        'EIX', 'CBRE', 'BRO', 'ON', 'MTD', 'MPWR', 'WEC', 'PPG', 'ETR', 'GPN',\n",
    "        'VRSK', 'VICI', 'CDW', 'AWK', 'EBAY', 'KEYS', 'WTW', 'LYB', 'STT', 'GDDY',\n",
    "        'BLDR', 'HUM', 'TTWO', 'XYL', 'HPQ', 'FTV', 'AEE', 'AVB', 'DECK', 'AXON',\n",
    "        'LH', 'EQR', 'ZBH', 'HUBB', 'EFX', 'DTE', 'IFF', 'TYL', 'BIIB', 'PHM',\n",
    "        'VLTO', 'TSN', 'PPL', 'CHD', 'NUE', 'MTB', 'DFS', 'CAH', 'TSCO', 'WY',\n",
    "        'K', 'WBD', 'CNP', 'ES', 'FITB', 'APTV', 'NTAP', 'FE', 'STE', 'HOLX',\n",
    "        'BR', 'DG', 'TROW', 'STX', 'WST', 'RF', 'ZBRA', 'BALL', 'CCL', 'MOH',\n",
    "        'HBAN', 'LUV', 'MKC', 'CFG', 'VTR', 'IRM', 'BAX', 'STLD', 'WDC', 'TDY',\n",
    "        'ATO', 'FDS', 'GPC', 'EXPD', 'ESS', 'DOV', 'EL', 'DRI', 'UAL', 'MAA',\n",
    "        'EXPE', 'CBOE', 'CMS', 'WAT', 'INVH', 'SYF', 'ALGN', 'IP', 'TER', 'NVR',\n",
    "        'PTC', 'SWK', 'NTRS', 'LDOS', 'J', 'KEY', 'CLX', 'CINF', 'HPE', 'BBY',\n",
    "        'PODD', 'TXT', 'CAG', 'EPAM', 'PKG', 'LVS', 'CPT', 'IEX', 'OMC', 'LYV',\n",
    "        'JBHT', 'RVTY', 'CF', 'MOS', 'AKAM', 'HST', 'INCY', 'LNT', 'AMCR', 'POOL',\n",
    "        'EVRG', 'PKI', 'ULTA', 'SWKS', 'BXP', 'ALB', 'PAYC', 'COO', 'MTCH', 'TPR',\n",
    "        'TECH', 'JKHY', 'CTLT', 'NDSN', 'EMN', 'KIM', 'UDR', 'AES', 'CRL', 'NCLH',\n",
    "        'CPB', 'CE', 'HII', 'BG', 'REG', 'TFX', 'IPG', 'BEN', 'FRT', 'GL',\n",
    "        'APA', 'AIZ', 'FFIV', 'TAP', 'HSIC', 'WYNN', 'CHRW', 'RJF', 'VTRS', 'PNR',\n",
    "        'MGM', 'NI', 'MRO', 'NRG', 'BBWI', 'ALLE', 'BWA', 'SOLV', 'HAS', 'WHR',\n",
    "        'GNRC', 'PNW', 'HRL', 'FMC', 'IVZ', 'SJM', 'AOS', 'AAL', 'MKTX', 'KMX',\n",
    "        'ENPH', 'DVN', 'LKQ', 'UHS', 'ROL', 'L', 'JNPR', 'QRVO', 'CZR', 'PARA',\n",
    "        'NWSA', 'NWS', 'FOX', 'FOXA'\n",
    "    ]\n",
    "    print(f\"‚úÖ Retrieved {len(tickers)} S&P 500 tickers\")\n",
    "    return tickers\n",
    "\n",
    "def get_nasdaq_tickers():\n",
    "    \"\"\"NASDAQ-100 ticker list\"\"\"\n",
    "    tickers = [\n",
    "        'AAPL', 'MSFT', 'NVDA', 'AMZN', 'META', 'GOOGL', 'GOOG', 'AVGO', 'TSLA', 'COST',\n",
    "        'NFLX', 'AMD', 'ADBE', 'QCOM', 'CSCO', 'INTU', 'TXN', 'AMGN', 'CMCSA', 'ISRG',\n",
    "        'AMAT', 'HON', 'BKNG', 'UBER', 'PANW', 'LRCX', 'ADP', 'VRTX', 'GILD', 'ADI',\n",
    "        'SBUX', 'MU', 'KLAC', 'REGN', 'SNPS', 'INTC', 'CDNS', 'PYPL', 'NXPI', 'MELI',\n",
    "        'MDLZ', 'CME', 'ASML', 'CRWD', 'MAR', 'CTAS', 'ADSK', 'ABNB', 'CSX', 'ORLY',\n",
    "        'PCAR', 'FTNT', 'CHTR', 'ROP', 'MNST', 'DASH', 'TTD', 'WDAY', 'CPRT', 'ODFL',\n",
    "        'AEP', 'ROST', 'MRVL', 'FAST', 'PAYX', 'EA', 'DXCM', 'KDP', 'CTSH', 'IDXX',\n",
    "        'KHC', 'VRSK', 'EXC', 'LULU', 'GEHC', 'CSGP', 'CCEP', 'BKR', 'ZS', 'DDOG',\n",
    "        'AZN', 'TEAM', 'XEL', 'FANG', 'ANSS', 'ON', 'BIIB', 'MCHP', 'CDW', 'TTWO',\n",
    "        'GFS', 'MDB', 'ILMN', 'WBD', 'ARM', 'WBA', 'MRNA', 'DLTR', 'SMCI'\n",
    "    ]\n",
    "    print(f\"‚úÖ Retrieved {len(tickers)} NASDAQ-100 tickers\")\n",
    "    return tickers\n",
    "\n",
    "# Load tickers\n",
    "sp500_tickers = get_sp500_tickers()\n",
    "nasdaq_tickers = get_nasdaq_tickers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Download Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA DOWNLOAD FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def get_last_date_from_df(df):\n",
    "    \"\"\"Get the last date from a DataFrame\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        return None\n",
    "    return pd.Timestamp(df.index.max().date())\n",
    "\n",
    "def download_incremental_data(tickers, start_date, end_date):\n",
    "    \"\"\"Download new data since start_date\"\"\"\n",
    "    \n",
    "    all_data = {}\n",
    "    failed = []\n",
    "    \n",
    "    for ticker in tqdm(tickers, desc=\"Downloading\"):\n",
    "        try:\n",
    "            data = yf.download(\n",
    "                ticker,\n",
    "                start=start_date,\n",
    "                end=end_date,\n",
    "                progress=False,\n",
    "                show_errors=False\n",
    "            )\n",
    "            \n",
    "            if not data.empty:\n",
    "                all_data[ticker] = data\n",
    "            else:\n",
    "                failed.append(ticker)\n",
    "            \n",
    "            time.sleep(0.2)  # Rate limiting\n",
    "            \n",
    "        except Exception as e:\n",
    "            failed.append(ticker)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Downloaded: {len(all_data)}/{len(tickers)} tickers\")\n",
    "    if failed:\n",
    "        print(f\"‚ö†Ô∏è  Failed: {len(failed)} tickers\")\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "def create_price_dataframes(all_data):\n",
    "    \"\"\"Create separate DataFrames for each price type\"\"\"\n",
    "    \n",
    "    price_dfs = {}\n",
    "    \n",
    "    for price_type in ['Close', 'Adj Close', 'Open', 'High', 'Low', 'Volume']:\n",
    "        df = pd.DataFrame({\n",
    "            ticker: data[price_type]\n",
    "            for ticker, data in all_data.items()\n",
    "            if price_type in data.columns\n",
    "        })\n",
    "        \n",
    "        # Map to our naming convention\n",
    "        key_map = {\n",
    "            'Close': 'close',\n",
    "            'Adj Close': 'adj_close',\n",
    "            'Open': 'open',\n",
    "            'High': 'high',\n",
    "            'Low': 'low',\n",
    "            'Volume': 'volume'\n",
    "        }\n",
    "        \n",
    "        price_dfs[key_map[price_type]] = df\n",
    "    \n",
    "    return price_dfs\n",
    "\n",
    "def merge_dataframes(existing_df, new_df):\n",
    "    \"\"\"Merge existing and new data, removing duplicates\"\"\"\n",
    "    \n",
    "    if existing_df is None or existing_df.empty:\n",
    "        return new_df\n",
    "    \n",
    "    if new_df is None or new_df.empty:\n",
    "        return existing_df\n",
    "    \n",
    "    # Concatenate\n",
    "    combined = pd.concat([existing_df, new_df])\n",
    "    \n",
    "    # Remove duplicate dates (keep last)\n",
    "    combined = combined[~combined.index.duplicated(keep='last')]\n",
    "    \n",
    "    # Sort by date\n",
    "    combined = combined.sort_index()\n",
    "    \n",
    "    return combined\n",
    "\n",
    "print(\"‚úÖ Data download functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Update Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MAIN UPDATE FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def update_index_data(service, index_name, tickers, folder_name):\n",
    "    \"\"\"\n",
    "    Complete update workflow for an index:\n",
    "    1. Download existing CSVs from Google Drive\n",
    "    2. Check last date\n",
    "    3. Download incremental data\n",
    "    4. Merge and upload back to Drive\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"UPDATING {index_name}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Get or create folder structure\n",
    "    base_folder_id = find_or_create_folder(service, DRIVE_FOLDER_NAME)\n",
    "    index_folder_id = find_or_create_folder(service, folder_name, base_folder_id)\n",
    "    \n",
    "    # Check for existing adj_close file to determine last date\n",
    "    adj_close_filename = f\"{folder_name.lower()}_adj_close.csv\"\n",
    "    existing_file = find_file_in_folder(service, adj_close_filename, index_folder_id)\n",
    "    \n",
    "    last_date = None\n",
    "    \n",
    "    if existing_file:\n",
    "        print(f\"üìÅ Found existing data: {adj_close_filename}\")\n",
    "        existing_df = download_csv_from_drive(\n",
    "            service,\n",
    "            existing_file['id'],\n",
    "            adj_close_filename\n",
    "        )\n",
    "        last_date = get_last_date_from_df(existing_df)\n",
    "        print(f\"üìÖ Last date in data: {last_date.strftime('%Y-%m-%d')}\")\n",
    "    else:\n",
    "        print(f\"üÜï No existing data found - will download full history\")\n",
    "        last_date = pd.Timestamp(START_DATE)\n",
    "    \n",
    "    # Check if update needed\n",
    "    today = pd.Timestamp(datetime.today().date())\n",
    "    \n",
    "    if last_date >= today:\n",
    "        print(f\"‚úÖ Data is current - no update needed!\")\n",
    "        return\n",
    "    \n",
    "    days_behind = (today - last_date).days\n",
    "    print(f\"\\nüìä Need to update {days_behind} days of data\")\n",
    "    \n",
    "    # Download incremental data\n",
    "    start_date = (last_date + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "    end_date = today.strftime('%Y-%m-%d')\n",
    "    \n",
    "    print(f\"\\n‚è¨ Downloading data from {start_date} to {end_date}...\")\n",
    "    new_data = download_incremental_data(tickers, start_date, end_date)\n",
    "    \n",
    "    if not new_data:\n",
    "        print(\"‚ö†Ô∏è  No new data available\")\n",
    "        return\n",
    "    \n",
    "    # Create price DataFrames\n",
    "    print(f\"\\nüìä Processing new data...\")\n",
    "    new_price_dfs = create_price_dataframes(new_data)\n",
    "    \n",
    "    # Update each file type\n",
    "    print(f\"\\n‚è´ Updating files in Google Drive...\\n\")\n",
    "    \n",
    "    for price_type in PRICE_TYPES:\n",
    "        filename = f\"{folder_name.lower()}_{price_type}.csv\"\n",
    "        \n",
    "        # Find existing file\n",
    "        existing_file = find_file_in_folder(service, filename, index_folder_id)\n",
    "        \n",
    "        # Download existing data if it exists\n",
    "        existing_df = None\n",
    "        if existing_file:\n",
    "            existing_df = download_csv_from_drive(\n",
    "                service,\n",
    "                existing_file['id'],\n",
    "                filename\n",
    "            )\n",
    "        \n",
    "        # Merge with new data\n",
    "        if price_type in new_price_dfs:\n",
    "            updated_df = merge_dataframes(existing_df, new_price_dfs[price_type])\n",
    "            \n",
    "            # Upload to Drive\n",
    "            file_id = existing_file['id'] if existing_file else None\n",
    "            upload_csv_to_drive(\n",
    "                service,\n",
    "                updated_df,\n",
    "                filename,\n",
    "                index_folder_id,\n",
    "                file_id\n",
    "            )\n",
    "            \n",
    "            print(f\"   Shape: {updated_df.shape}, Date range: {updated_df.index.min().date()} to {updated_df.index.max().date()}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ {index_name} update complete!\")\n",
    "\n",
    "print(\"‚úÖ Main update function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RUN UPDATES FOR BOTH INDICES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING INCREMENTAL UPDATE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Update S&P 500\n",
    "update_index_data(\n",
    "    drive_service,\n",
    "    \"S&P 500\",\n",
    "    sp500_tickers,\n",
    "    \"SP500\"\n",
    ")\n",
    "\n",
    "# Update NASDAQ-100\n",
    "update_index_data(\n",
    "    drive_service,\n",
    "    \"NASDAQ-100\",\n",
    "    nasdaq_tickers,\n",
    "    \"NASDAQ\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ ALL UPDATES COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìÅ Files updated in Google Drive: {DRIVE_FOLDER_NAME}/\")\n",
    "print(\"\\nüéØ Next run will only download data since last update!\")\n",
    "print(\"‚è±Ô∏è  Estimated incremental runtime: 2-5 minutes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
