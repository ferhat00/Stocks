{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qzDa_wuUWJG1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nComplete Stock Data Downloader for S&P 500 & NASDAQ\\nSaves organized CSV files to Google Drive\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Complete Stock Data Downloader for S&P 500 & NASDAQ\n",
    "Saves organized CSV files to Google Drive\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Detected environment: LOCAL\n",
      "âœ… Using local directory\n",
      "ğŸ“ Output directory: c:\\Users\\Ferhat\\Documents\\GitHub\\Stocks\\output\n"
     ]
    }
   ],
   "source": [
    "# Environment Detection and Setup\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def detect_environment():\n",
    "    \"\"\"\n",
    "    Detects whether the code is running locally, on Kaggle, or on Google Colab.\n",
    "    Returns: 'local', 'kaggle', or 'colab'\n",
    "    \"\"\"\n",
    "    # Check for Google Colab\n",
    "    try:\n",
    "        import google.colab\n",
    "        return 'colab'\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    # Check for Kaggle\n",
    "    if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
    "        return 'kaggle'\n",
    "    \n",
    "    # Default to local\n",
    "    return 'local'\n",
    "\n",
    "def setup_environment():\n",
    "    \"\"\"\n",
    "    Sets up the environment based on where the code is running.\n",
    "    Returns the output directory path.\n",
    "    \"\"\"\n",
    "    env = detect_environment()\n",
    "    print(f\"ğŸ” Detected environment: {env.upper()}\")\n",
    "    \n",
    "    if env == 'colab':\n",
    "        # Mount Google Drive\n",
    "        print(\"ğŸ“‚ Mounting Google Drive...\")\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        \n",
    "        # Set output path to Google Drive\n",
    "        output_dir = Path('/content/drive/MyDrive/Stocks_Data')\n",
    "        print(f\"âœ… Google Drive mounted successfully\")\n",
    "        \n",
    "    elif env == 'kaggle':\n",
    "        # Kaggle output directory\n",
    "        output_dir = Path('/kaggle/working')\n",
    "        print(f\"âœ… Using Kaggle working directory\")\n",
    "        \n",
    "    else:  # local\n",
    "        # Local output directory (same as notebook location)\n",
    "        output_dir = Path.cwd() / 'output'\n",
    "        print(f\"âœ… Using local directory\")\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"ğŸ“ Output directory: {output_dir}\")\n",
    "    \n",
    "    return env, output_dir\n",
    "\n",
    "# Run setup\n",
    "ENVIRONMENT, OUTPUT_DIR = setup_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ yfinance already installed\n",
      "âœ“ pandas already installed\n",
      "âœ“ numpy already installed\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (if not already installed)\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Install a package using pip if not already installed.\"\"\"\n",
    "    try:\n",
    "        __import__(package.split('[')[0])\n",
    "        print(f\"âœ“ {package} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"ğŸ“¦ Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "        print(f\"âœ“ {package} installed successfully\")\n",
    "\n",
    "# Install required packages\n",
    "packages = ['yfinance', 'pandas', 'numpy']\n",
    "\n",
    "for package in packages:\n",
    "    install_package(package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "h1xdNel2WKxq"
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3DvJwPXGWcX6",
    "outputId": "131e518e-3c60-48bd-daa8-1b7522047f03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Data will be saved to: c:\\Users\\Ferhat\\Documents\\GitHub\\Stocks\\output\n",
      "ğŸ“… Date range: 2020-01-01 to 2026-02-02\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Configuration - using OUTPUT_DIR from environment setup\n",
    "BASE_PATH = str(OUTPUT_DIR)\n",
    "START_DATE = '2020-01-01'\n",
    "END_DATE = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# Create directory structure\n",
    "BASE_PATH_PATH = Path(BASE_PATH)\n",
    "(BASE_PATH_PATH / 'SP500').mkdir(parents=True, exist_ok=True)\n",
    "(BASE_PATH_PATH / 'NASDAQ').mkdir(parents=True, exist_ok=True)\n",
    "(BASE_PATH_PATH / 'Combined').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\nğŸ“ Data will be saved to: {BASE_PATH}\")\n",
    "print(f\"ğŸ“… Date range: {START_DATE} to {END_DATE}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "yPvM1fd-WqqL"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FUNCTION: Get S&P 500 Tickers\n",
    "# ============================================================================\n",
    "def get_sp500_tickers():\n",
    "    \"\"\"Scrape S&P 500 tickers from Wikipedia with User-Agent headers\"\"\"\n",
    "    try:\n",
    "        url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "\n",
    "        # Add a headers dictionary to mimic a real browser\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        table = soup.find('table', {'id': 'constituents'})\n",
    "\n",
    "        if table is None:\n",
    "            raise ValueError(\"Could not find the 'constituents' table on the page.\")\n",
    "\n",
    "        # Pass the HTML string to read_html\n",
    "        df = pd.read_html(str(table))[0]\n",
    "\n",
    "        # Clean tickers (Wikipedia uses '.' for some classes, Yahoo Finance uses '-')\n",
    "        tickers = [ticker.replace('.', '-') for ticker in df['Symbol'].tolist()]\n",
    "\n",
    "        print(f\"âœ… Retrieved {len(tickers)} S&P 500 tickers\")\n",
    "        return tickers, df[['Symbol', 'Security', 'GICS Sector', 'GICS Sub-Industry']]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error getting S&P 500 tickers: {e}\")\n",
    "        return [], pd.DataFrame()\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCTION: Get NASDAQ 100 Tickers (as proxy for NASDAQ)\n",
    "# ============================================================================\n",
    "def get_nasdaq_tickers():\n",
    "    \"\"\"Scrape NASDAQ-100 tickers from Wikipedia\"\"\"\n",
    "    try:\n",
    "        url = 'https://en.wikipedia.org/wiki/Nasdaq-100'\n",
    "\n",
    "        # Adding the same User-Agent header to bypass blocks\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # The NASDAQ-100 table ID is usually 'constituents', but if that fails,\n",
    "        # we target the first wikitable on the page.\n",
    "        table = soup.find('table', {'id': 'constituents'})\n",
    "        if table is None:\n",
    "            table = soup.find('table', {'class': 'wikitable'})\n",
    "\n",
    "        df = pd.read_html(str(table))[0]\n",
    "\n",
    "        # Clean the tickers\n",
    "        tickers = [ticker.replace('.', '-') for ticker in df['Ticker'].tolist()]\n",
    "\n",
    "        print(f\"âœ… Retrieved {len(tickers)} NASDAQ-100 tickers\")\n",
    "\n",
    "        # Note: NASDAQ table uses 'Company' instead of 'Security'\n",
    "        return tickers, df[['Ticker', 'Company', 'GICS Sector', 'GICS Sub-Industry']]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error getting NASDAQ tickers: {e}\")\n",
    "        return [], pd.DataFrame()\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCTION: Download Data with Error Handling\n",
    "# ============================================================================\n",
    "def download_ticker_data(ticker, start_date, end_date, retry=3):\n",
    "    \"\"\"Download data for a single ticker with retry logic\"\"\"\n",
    "    for attempt in range(retry):\n",
    "        try:\n",
    "            data = yf.download(ticker, start=start_date, end=end_date,\n",
    "                             progress=False)\n",
    "            if not data.empty:\n",
    "                # Flatten multi-index columns if present\n",
    "                if isinstance(data.columns, pd.MultiIndex):\n",
    "                    data.columns = data.columns.get_level_values(0)\n",
    "                return data\n",
    "            time.sleep(0.5)  # Rate limiting\n",
    "        except Exception as e:\n",
    "            if attempt == retry - 1:\n",
    "                return pd.DataFrame()\n",
    "            time.sleep(1)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCTION: Batch Download with Progress Tracking\n",
    "# ============================================================================\n",
    "def batch_download(tickers, index_name, start_date, end_date):\n",
    "    \"\"\"Download data for multiple tickers with progress tracking\"\"\"\n",
    "\n",
    "    all_data = {}\n",
    "    failed_tickers = []\n",
    "\n",
    "    print(f\"\\nğŸ“Š Downloading {index_name} data for {len(tickers)} tickers...\")\n",
    "\n",
    "    for ticker in tqdm(tickers, desc=f\"{index_name} Progress\"):\n",
    "        data = download_ticker_data(ticker, start_date, end_date)\n",
    "\n",
    "        if not data.empty:\n",
    "            all_data[ticker] = data\n",
    "        else:\n",
    "            failed_tickers.append(ticker)\n",
    "\n",
    "        # Rate limiting - be nice to yfinance\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    print(f\"âœ… Successfully downloaded: {len(all_data)}/{len(tickers)}\")\n",
    "    if failed_tickers:\n",
    "        print(f\"âš ï¸  Failed tickers: {failed_tickers[:10]}{'...' if len(failed_tickers) > 10 else ''}\")\n",
    "\n",
    "    return all_data, failed_tickers\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCTION: Create Combined DataFrames\n",
    "# ============================================================================\n",
    "def create_combined_dataframes(all_data):\n",
    "    \"\"\"Create combined DataFrames for different price types\"\"\"\n",
    "\n",
    "    if not all_data:\n",
    "        return {}\n",
    "\n",
    "    combined = {}\n",
    "    tickers = list(all_data.keys())\n",
    "    \n",
    "    # Get a sample to check if data exists\n",
    "    sample_ticker = tickers[0]\n",
    "    sample_data = all_data[sample_ticker]\n",
    "    \n",
    "    # Define column mappings - handle both regular and adjusted columns\n",
    "    column_types = {\n",
    "        'Close': 'Close',\n",
    "        'Adj_Close': 'Adj Close',\n",
    "        'Volume': 'Volume',\n",
    "        'Open': 'Open',\n",
    "        'High': 'High',\n",
    "        'Low': 'Low'\n",
    "    }\n",
    "    \n",
    "    for key, col_name in column_types.items():\n",
    "        # Check if column exists in the sample data\n",
    "        if col_name in sample_data.columns:\n",
    "            try:\n",
    "                # Create a dictionary of series for each ticker\n",
    "                data_dict = {}\n",
    "                for ticker in tickers:\n",
    "                    if col_name in all_data[ticker].columns:\n",
    "                        data_dict[ticker] = all_data[ticker][col_name]\n",
    "                \n",
    "                # Create DataFrame from dict - pandas will align indices automatically\n",
    "                if data_dict:\n",
    "                    df = pd.DataFrame(data_dict)\n",
    "                    combined[key] = df\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸  Warning: Could not create {key} dataframe: {e}\")\n",
    "                continue\n",
    "\n",
    "    return combined\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCTION: Calculate Returns and Additional Metrics\n",
    "# ============================================================================\n",
    "def calculate_metrics(price_df):\n",
    "    \"\"\"Calculate daily and cumulative returns\"\"\"\n",
    "\n",
    "    # Daily returns\n",
    "    daily_returns = price_df.pct_change()\n",
    "\n",
    "    # Cumulative returns\n",
    "    cumulative_returns = (1 + daily_returns).cumprod() - 1\n",
    "\n",
    "    return daily_returns, cumulative_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NWV_5mA4WK0O",
    "outputId": "0fd6f4b9-c16b-4665-d16e-25b12ef77941"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STARTING DATA DOWNLOAD\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ferhat\\AppData\\Local\\Temp\\ipykernel_344\\3470799733.py:26: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Retrieved 503 S&P 500 tickers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ferhat\\AppData\\Local\\Temp\\ipykernel_344\\3470799733.py:62: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Retrieved 101 NASDAQ-100 tickers\n",
      "âŒ Error getting NASDAQ tickers: \"['GICS Sector', 'GICS Sub-Industry'] not in index\"\n",
      "ğŸ’¾ Saved S&P 500 constituent info\n",
      "\n",
      "ğŸ“Š Downloading S&P 500 data for 503 tickers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S&P 500 Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 503/503 [03:45<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Successfully downloaded: 503/503\n",
      "\n",
      "ğŸ“Š Processing S&P 500 data...\n",
      "ğŸ’¾ Saved S&P 500 Close: (1528, 503) -> c:\\Users\\Ferhat\\Documents\\GitHub\\Stocks\\output/SP500/sp500_close.csv\n",
      "ğŸ’¾ Saved S&P 500 Volume: (1528, 503) -> c:\\Users\\Ferhat\\Documents\\GitHub\\Stocks\\output/SP500/sp500_volume.csv\n",
      "ğŸ’¾ Saved S&P 500 Open: (1528, 503) -> c:\\Users\\Ferhat\\Documents\\GitHub\\Stocks\\output/SP500/sp500_open.csv\n",
      "ğŸ’¾ Saved S&P 500 High: (1528, 503) -> c:\\Users\\Ferhat\\Documents\\GitHub\\Stocks\\output/SP500/sp500_high.csv\n",
      "ğŸ’¾ Saved S&P 500 Low: (1528, 503) -> c:\\Users\\Ferhat\\Documents\\GitHub\\Stocks\\output/SP500/sp500_low.csv\n",
      "\n",
      "ğŸ“ˆ Calculating S&P 500 returns...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Adj_Close'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Calculate and save returns with explicit messages\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mğŸ“ˆ Calculating S&P 500 returns...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m sp500_daily_returns, sp500_cumulative_returns = calculate_metrics(\u001b[43msp500_combined\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mAdj_Close\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Save daily returns\u001b[39;00m\n\u001b[32m     43\u001b[39m daily_returns_file = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBASE_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/SP500/sp500_daily_returns.csv\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'Adj_Close'"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STARTING DATA DOWNLOAD\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get tickers\n",
    "sp500_tickers, sp500_info = get_sp500_tickers()\n",
    "nasdaq_tickers, nasdaq_info = get_nasdaq_tickers()\n",
    "\n",
    "# Save ticker lists\n",
    "if not sp500_info.empty:\n",
    "    sp500_info.to_csv(f'{BASE_PATH}/SP500/sp500_constituents.csv', index=False)\n",
    "    print(f\"ğŸ’¾ Saved S&P 500 constituent info\")\n",
    "\n",
    "if not nasdaq_info.empty:\n",
    "    nasdaq_info.to_csv(f'{BASE_PATH}/NASDAQ/nasdaq100_constituents.csv', index=False)\n",
    "    print(f\"ğŸ’¾ Saved NASDAQ-100 constituent info\")\n",
    "\n",
    "# Download S&P 500 data\n",
    "if sp500_tickers:\n",
    "    sp500_data, sp500_failed = batch_download(sp500_tickers, \"S&P 500\", START_DATE, END_DATE)\n",
    "\n",
    "    if sp500_data:\n",
    "        print(f\"\\nğŸ“Š Processing S&P 500 data...\")\n",
    "        \n",
    "        # Create combined DataFrames\n",
    "        sp500_combined = create_combined_dataframes(sp500_data)\n",
    "\n",
    "        # Save each type explicitly\n",
    "        for data_type, df in sp500_combined.items():\n",
    "            filename = f'{BASE_PATH}/SP500/sp500_{data_type.lower()}.csv'\n",
    "            df.to_csv(filename)\n",
    "            print(f\"ğŸ’¾ Saved S&P 500 {data_type}: {df.shape} -> {filename}\")\n",
    "\n",
    "        # Calculate and save returns with explicit messages\n",
    "        print(f\"\\nğŸ“ˆ Calculating S&P 500 returns...\")\n",
    "        sp500_daily_returns, sp500_cumulative_returns = calculate_metrics(sp500_combined['Adj_Close'])\n",
    "        \n",
    "        # Save daily returns\n",
    "        daily_returns_file = f'{BASE_PATH}/SP500/sp500_daily_returns.csv'\n",
    "        sp500_daily_returns.to_csv(daily_returns_file)\n",
    "        print(f\"ğŸ’¾ Saved S&P 500 Daily Returns: {sp500_daily_returns.shape} -> {daily_returns_file}\")\n",
    "        \n",
    "        # Save cumulative returns\n",
    "        cum_returns_file = f'{BASE_PATH}/SP500/sp500_cumulative_returns.csv'\n",
    "        sp500_cumulative_returns.to_csv(cum_returns_file)\n",
    "        print(f\"ğŸ’¾ Saved S&P 500 Cumulative Returns: {sp500_cumulative_returns.shape} -> {cum_returns_file}\")\n",
    "    else:\n",
    "        print(\"âš ï¸  No S&P 500 data to save - all downloads failed\")\n",
    "\n",
    "# Download NASDAQ data\n",
    "if nasdaq_tickers:\n",
    "    nasdaq_data, nasdaq_failed = batch_download(nasdaq_tickers, \"NASDAQ-100\", START_DATE, END_DATE)\n",
    "\n",
    "    if nasdaq_data:\n",
    "        print(f\"\\nğŸ“Š Processing NASDAQ-100 data...\")\n",
    "        \n",
    "        # Create combined DataFrames\n",
    "        nasdaq_combined = create_combined_dataframes(nasdaq_data)\n",
    "\n",
    "        # Save each type explicitly\n",
    "        for data_type, df in nasdaq_combined.items():\n",
    "            filename = f'{BASE_PATH}/NASDAQ/nasdaq_{data_type.lower()}.csv'\n",
    "            df.to_csv(filename)\n",
    "            print(f\"ğŸ’¾ Saved NASDAQ-100 {data_type}: {df.shape} -> {filename}\")\n",
    "\n",
    "        # Calculate and save returns with explicit messages\n",
    "        print(f\"\\nğŸ“ˆ Calculating NASDAQ-100 returns...\")\n",
    "        nasdaq_daily_returns, nasdaq_cumulative_returns = calculate_metrics(nasdaq_combined['Adj_Close'])\n",
    "        \n",
    "        # Save daily returns\n",
    "        daily_returns_file = f'{BASE_PATH}/NASDAQ/nasdaq_daily_returns.csv'\n",
    "        nasdaq_daily_returns.to_csv(daily_returns_file)\n",
    "        print(f\"ğŸ’¾ Saved NASDAQ-100 Daily Returns: {nasdaq_daily_returns.shape} -> {daily_returns_file}\")\n",
    "        \n",
    "        # Save cumulative returns\n",
    "        cum_returns_file = f'{BASE_PATH}/NASDAQ/nasdaq_cumulative_returns.csv'\n",
    "        nasdaq_cumulative_returns.to_csv(cum_returns_file)\n",
    "        print(f\"ğŸ’¾ Saved NASDAQ-100 Cumulative Returns: {nasdaq_cumulative_returns.shape} -> {cum_returns_file}\")\n",
    "    else:\n",
    "        print(\"âš ï¸  No NASDAQ-100 data to save - all downloads failed\")\n",
    "\n",
    "# Create combined universe (unique tickers from both)\n",
    "all_tickers = list(set(sp500_tickers + nasdaq_tickers))\n",
    "print(f\"\\nğŸ“Š Total unique tickers: {len(all_tickers)}\")\n",
    "\n",
    "# Save combined ticker list\n",
    "pd.DataFrame({\n",
    "    'Ticker': all_tickers,\n",
    "    'In_SP500': [t in sp500_tickers for t in all_tickers],\n",
    "    'In_NASDAQ100': [t in nasdaq_tickers for t in all_tickers]\n",
    "}).to_csv(f'{BASE_PATH}/Combined/all_tickers.csv', index=False)\n",
    "print(f\"ğŸ’¾ Saved combined ticker list\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… DOWNLOAD COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nFiles saved to: {BASE_PATH}\")\n",
    "print(\"\\nExpected file structure:\")\n",
    "print(\"â”œâ”€â”€ SP500/\")\n",
    "print(\"â”‚   â”œâ”€â”€ sp500_constituents.csv\")\n",
    "print(\"â”‚   â”œâ”€â”€ sp500_close.csv\")\n",
    "print(\"â”‚   â”œâ”€â”€ sp500_adj_close.csv\")\n",
    "print(\"â”‚   â”œâ”€â”€ sp500_open.csv\")\n",
    "print(\"â”‚   â”œâ”€â”€ sp500_high.csv\")\n",
    "print(\"â”‚   â”œâ”€â”€ sp500_low.csv\")\n",
    "print(\"â”‚   â”œâ”€â”€ sp500_volume.csv\")\n",
    "print(\"â”‚   â”œâ”€â”€ sp500_daily_returns.csv\")\n",
    "print(\"â”‚   â””â”€â”€ sp500_cumulative_returns.csv\")\n",
    "print(\"â”œâ”€â”€ NASDAQ/\")\n",
    "print(\"â”‚   â”œâ”€â”€ nasdaq100_constituents.csv\")\n",
    "print(\"â”‚   â”œâ”€â”€ nasdaq_close.csv\")\n",
    "print(\"â”‚   â”œâ”€â”€ nasdaq_adj_close.csv\")\n",
    "print(\"â”‚   â”œâ”€â”€ nasdaq_open.csv\")\n",
    "print(\"â”‚   â”œâ”€â”€ nasdaq_high.csv\")\n",
    "print(\"â”‚   â”œâ”€â”€ nasdaq_low.csv\")\n",
    "print(\"â”‚   â”œâ”€â”€ nasdaq_volume.csv\")\n",
    "print(\"â”‚   â”œâ”€â”€ nasdaq_daily_returns.csv\")\n",
    "print(\"â”‚   â””â”€â”€ nasdaq_cumulative_returns.csv\")\n",
    "print(\"â””â”€â”€ Combined/\")\n",
    "print(\"    â””â”€â”€ all_tickers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SElxNLKL6pfW"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DEBUG: Test single ticker download\n",
    "# ============================================================================\n",
    "print(\"ğŸ” Testing single ticker download...\")\n",
    "\n",
    "test_ticker = 'AAPL'\n",
    "print(f\"\\nAttempting to download {test_ticker}...\")\n",
    "\n",
    "try:\n",
    "    # Test ticker download\n",
    "    test_data = yf.download(test_ticker, start=START_DATE, end=END_DATE, \n",
    "                           progress=True)\n",
    "    \n",
    "    if test_data.empty:\n",
    "        print(f\"âŒ Downloaded data is empty for {test_ticker}\")\n",
    "    else:\n",
    "        print(f\"âœ… Successfully downloaded {test_ticker}\")\n",
    "        print(f\"   Shape: {test_data.shape}\")\n",
    "        print(f\"   Columns: {test_data.columns.tolist()}\")\n",
    "        print(f\"\\nFirst few rows:\")\n",
    "        print(test_data.head())\n",
    "        print(f\"\\nLast few rows:\")\n",
    "        print(test_data.tail())\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error downloading {test_ticker}: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Check yfinance version\n",
    "print(f\"\\nğŸ“¦ yfinance version: {yf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPROVED: Download Data with Better Error Handling\n",
    "# ============================================================================\n",
    "def download_ticker_data_debug(ticker, start_date, end_date, retry=3):\n",
    "    \"\"\"Download data for a single ticker with retry logic and detailed error reporting\"\"\"\n",
    "    for attempt in range(retry):\n",
    "        try:\n",
    "            data = yf.download(ticker, start=start_date, end=end_date,\n",
    "                             progress=False)\n",
    "            if not data.empty:\n",
    "                return data, None  # Return data and no error\n",
    "            else:\n",
    "                error_msg = f\"Empty data returned for {ticker}\"\n",
    "                time.sleep(0.5)\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Exception for {ticker}: {str(e)}\"\n",
    "            if attempt < retry - 1:\n",
    "                time.sleep(1)\n",
    "            \n",
    "    return pd.DataFrame(), error_msg  # Return empty dataframe and error message\n",
    "\n",
    "# Test with a few tickers\n",
    "print(\"ğŸ” Testing download with error reporting...\")\n",
    "test_tickers = ['AAPL', 'MSFT', 'GOOGL', 'INVALID_TICKER']\n",
    "\n",
    "for ticker in test_tickers:\n",
    "    data, error = download_ticker_data_debug(ticker, START_DATE, END_DATE)\n",
    "    if not data.empty:\n",
    "        print(f\"âœ… {ticker}: Downloaded {data.shape[0]} rows\")\n",
    "    else:\n",
    "        print(f\"âŒ {ticker}: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
