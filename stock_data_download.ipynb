{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qzDa_wuUWJG1"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Complete Stock Data Downloader for S&P 500 & NASDAQ\n",
    "Saves organized CSV files to Google Drive\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Detected environment: LOCAL\n",
      "‚úÖ Using local directory\n",
      "üìÅ Output directory: c:\\Users\\Ferhat\\Documents\\GitHub\\Stocks\\output\n"
     ]
    }
   ],
   "source": [
    "# Environment Detection and Setup\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def detect_environment():\n",
    "    \"\"\"\n",
    "    Detects whether the code is running locally, on Kaggle, or on Google Colab.\n",
    "    Returns: 'local', 'kaggle', or 'colab'\n",
    "    \"\"\"\n",
    "    # Check for Google Colab\n",
    "    try:\n",
    "        import google.colab\n",
    "        return 'colab'\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    # Check for Kaggle\n",
    "    if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
    "        return 'kaggle'\n",
    "    \n",
    "    # Default to local\n",
    "    return 'local'\n",
    "\n",
    "def setup_environment():\n",
    "    \"\"\"\n",
    "    Sets up the environment based on where the code is running.\n",
    "    Returns the output directory path.\n",
    "    \"\"\"\n",
    "    env = detect_environment()\n",
    "    print(f\"üîç Detected environment: {env.upper()}\")\n",
    "    \n",
    "    if env == 'colab':\n",
    "        # Mount Google Drive\n",
    "        print(\"üìÇ Mounting Google Drive...\")\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        \n",
    "        # Set output path to Google Drive\n",
    "        output_dir = Path('/content/drive/MyDrive/Stocks_Data')\n",
    "        print(f\"‚úÖ Google Drive mounted successfully\")\n",
    "        \n",
    "    elif env == 'kaggle':\n",
    "        # Kaggle output directory\n",
    "        output_dir = Path('/kaggle/working')\n",
    "        print(f\"‚úÖ Using Kaggle working directory\")\n",
    "        \n",
    "    else:  # local\n",
    "        # Local output directory (same as notebook location)\n",
    "        output_dir = Path.cwd() / 'output'\n",
    "        print(f\"‚úÖ Using local directory\")\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"üìÅ Output directory: {output_dir}\")\n",
    "    \n",
    "    return env, output_dir\n",
    "\n",
    "# Run setup\n",
    "ENVIRONMENT, OUTPUT_DIR = setup_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì yfinance already installed\n",
      "‚úì pandas already installed\n",
      "‚úì numpy already installed\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (if not already installed)\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Install a package using pip if not already installed.\"\"\"\n",
    "    try:\n",
    "        __import__(package.split('[')[0])\n",
    "        print(f\"‚úì {package} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"üì¶ Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "        print(f\"‚úì {package} installed successfully\")\n",
    "\n",
    "# Install required packages\n",
    "packages = ['yfinance', 'pandas', 'numpy']\n",
    "\n",
    "for package in packages:\n",
    "    install_package(package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "h1xdNel2WKxq"
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3DvJwPXGWcX6",
    "outputId": "131e518e-3c60-48bd-daa8-1b7522047f03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÅ Data will be saved to: c:\\Users\\Ferhat\\Documents\\GitHub\\Stocks\\output\n",
      "üìÖ Date range: 2020-01-01 to 2026-02-02\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Configuration - using OUTPUT_DIR from environment setup\n",
    "BASE_PATH = str(OUTPUT_DIR)\n",
    "START_DATE = '2020-01-01'\n",
    "END_DATE = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# Create directory structure\n",
    "BASE_PATH_PATH = Path(BASE_PATH)\n",
    "(BASE_PATH_PATH / 'SP500').mkdir(parents=True, exist_ok=True)\n",
    "(BASE_PATH_PATH / 'NASDAQ').mkdir(parents=True, exist_ok=True)\n",
    "(BASE_PATH_PATH / 'Combined').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\nüìÅ Data will be saved to: {BASE_PATH}\")\n",
    "print(f\"üìÖ Date range: {START_DATE} to {END_DATE}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yPvM1fd-WqqL"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FUNCTION: Get S&P 500 Tickers\n",
    "# ============================================================================\n",
    "def get_sp500_tickers():\n",
    "    \"\"\"Scrape S&P 500 tickers from Wikipedia with User-Agent headers\"\"\"\n",
    "    try:\n",
    "        url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "\n",
    "        # Add a headers dictionary to mimic a real browser\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        table = soup.find('table', {'id': 'constituents'})\n",
    "\n",
    "        if table is None:\n",
    "            raise ValueError(\"Could not find the 'constituents' table on the page.\")\n",
    "\n",
    "        # Pass the HTML string to read_html\n",
    "        df = pd.read_html(str(table))[0]\n",
    "\n",
    "        # Clean tickers (Wikipedia uses '.' for some classes, Yahoo Finance uses '-')\n",
    "        tickers = [ticker.replace('.', '-') for ticker in df['Symbol'].tolist()]\n",
    "\n",
    "        print(f\"‚úÖ Retrieved {len(tickers)} S&P 500 tickers\")\n",
    "        return tickers, df[['Symbol', 'Security', 'GICS Sector', 'GICS Sub-Industry']]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error getting S&P 500 tickers: {e}\")\n",
    "        return [], pd.DataFrame()\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCTION: Get NASDAQ 100 Tickers (as proxy for NASDAQ)\n",
    "# ============================================================================\n",
    "def get_nasdaq_tickers():\n",
    "    \"\"\"Scrape NASDAQ-100 tickers from Wikipedia\"\"\"\n",
    "    try:\n",
    "        url = 'https://en.wikipedia.org/wiki/Nasdaq-100'\n",
    "\n",
    "        # Adding the same User-Agent header to bypass blocks\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # The NASDAQ-100 table ID is usually 'constituents', but if that fails,\n",
    "        # we target the first wikitable on the page.\n",
    "        table = soup.find('table', {'id': 'constituents'})\n",
    "        if table is None:\n",
    "            table = soup.find('table', {'class': 'wikitable'})\n",
    "\n",
    "        df = pd.read_html(str(table))[0]\n",
    "\n",
    "        # Clean the tickers\n",
    "        tickers = [ticker.replace('.', '-') for ticker in df['Ticker'].tolist()]\n",
    "\n",
    "        print(f\"‚úÖ Retrieved {len(tickers)} NASDAQ-100 tickers\")\n",
    "\n",
    "        # Note: NASDAQ table uses 'Company' instead of 'Security'\n",
    "        return tickers, df[['Ticker', 'Company', 'GICS Sector', 'GICS Sub-Industry']]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error getting NASDAQ tickers: {e}\")\n",
    "        return [], pd.DataFrame()\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCTION: Download Data with Error Handling\n",
    "# ============================================================================\n",
    "def download_ticker_data(ticker, start_date, end_date, retry=3):\n",
    "    \"\"\"Download data for a single ticker with retry logic\"\"\"\n",
    "    for attempt in range(retry):\n",
    "        try:\n",
    "            data = yf.download(ticker, start=start_date, end=end_date,\n",
    "                             progress=False, show_errors=False)\n",
    "            if not data.empty:\n",
    "                return data\n",
    "            time.sleep(0.5)  # Rate limiting\n",
    "        except Exception as e:\n",
    "            if attempt == retry - 1:\n",
    "                return pd.DataFrame()\n",
    "            time.sleep(1)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCTION: Batch Download with Progress Tracking\n",
    "# ============================================================================\n",
    "def batch_download(tickers, index_name, start_date, end_date):\n",
    "    \"\"\"Download data for multiple tickers with progress tracking\"\"\"\n",
    "\n",
    "    all_data = {}\n",
    "    failed_tickers = []\n",
    "\n",
    "    print(f\"\\nüìä Downloading {index_name} data for {len(tickers)} tickers...\")\n",
    "\n",
    "    for ticker in tqdm(tickers, desc=f\"{index_name} Progress\"):\n",
    "        data = download_ticker_data(ticker, start_date, end_date)\n",
    "\n",
    "        if not data.empty:\n",
    "            all_data[ticker] = data\n",
    "        else:\n",
    "            failed_tickers.append(ticker)\n",
    "\n",
    "        # Rate limiting - be nice to yfinance\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    print(f\"‚úÖ Successfully downloaded: {len(all_data)}/{len(tickers)}\")\n",
    "    if failed_tickers:\n",
    "        print(f\"‚ö†Ô∏è  Failed tickers: {failed_tickers[:10]}{'...' if len(failed_tickers) > 10 else ''}\")\n",
    "\n",
    "    return all_data, failed_tickers\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCTION: Create Combined DataFrames\n",
    "# ============================================================================\n",
    "def create_combined_dataframes(all_data):\n",
    "    \"\"\"Create combined DataFrames for different price types\"\"\"\n",
    "\n",
    "    if not all_data:\n",
    "        return {}\n",
    "\n",
    "    combined = {}\n",
    "\n",
    "    # Get all tickers\n",
    "    tickers = list(all_data.keys())\n",
    "\n",
    "    # Extract Close prices\n",
    "    close_df = pd.DataFrame({ticker: all_data[ticker]['Close']\n",
    "                            for ticker in tickers if 'Close' in all_data[ticker].columns})\n",
    "\n",
    "    # Extract Adjusted Close\n",
    "    adj_close_df = pd.DataFrame({ticker: all_data[ticker]['Adj Close']\n",
    "                                for ticker in tickers if 'Adj Close' in all_data[ticker].columns})\n",
    "\n",
    "    # Extract Volume\n",
    "    volume_df = pd.DataFrame({ticker: all_data[ticker]['Volume']\n",
    "                             for ticker in tickers if 'Volume' in all_data[ticker].columns})\n",
    "\n",
    "    # Extract Open, High, Low\n",
    "    open_df = pd.DataFrame({ticker: all_data[ticker]['Open']\n",
    "                           for ticker in tickers if 'Open' in all_data[ticker].columns})\n",
    "\n",
    "    high_df = pd.DataFrame({ticker: all_data[ticker]['High']\n",
    "                           for ticker in tickers if 'High' in all_data[ticker].columns})\n",
    "\n",
    "    low_df = pd.DataFrame({ticker: all_data[ticker]['Low']\n",
    "                          for ticker in tickers if 'Low' in all_data[ticker].columns})\n",
    "\n",
    "    combined['Close'] = close_df\n",
    "    combined['Adj_Close'] = adj_close_df\n",
    "    combined['Volume'] = volume_df\n",
    "    combined['Open'] = open_df\n",
    "    combined['High'] = high_df\n",
    "    combined['Low'] = low_df\n",
    "\n",
    "    return combined\n",
    "\n",
    "# ============================================================================\n",
    "# FUNCTION: Calculate Returns and Additional Metrics\n",
    "# ============================================================================\n",
    "def calculate_metrics(price_df):\n",
    "    \"\"\"Calculate daily and cumulative returns\"\"\"\n",
    "\n",
    "    # Daily returns\n",
    "    daily_returns = price_df.pct_change()\n",
    "\n",
    "    # Cumulative returns\n",
    "    cumulative_returns = (1 + daily_returns).cumprod() - 1\n",
    "\n",
    "    return daily_returns, cumulative_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NWV_5mA4WK0O",
    "outputId": "0fd6f4b9-c16b-4665-d16e-25b12ef77941"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STARTING DATA DOWNLOAD\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ferhat\\AppData\\Local\\Temp\\ipykernel_13784\\3707474406.py:26: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Retrieved 503 S&P 500 tickers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ferhat\\AppData\\Local\\Temp\\ipykernel_13784\\3707474406.py:62: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Retrieved 101 NASDAQ-100 tickers\n",
      "‚ùå Error getting NASDAQ tickers: \"['GICS Sector', 'GICS Sub-Industry'] not in index\"\n",
      "üíæ Saved S&P 500 constituent info\n",
      "\n",
      "üìä Downloading S&P 500 data for 503 tickers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "S&P 500 Progress: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 503/503 [18:29<00:00,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully downloaded: 0/503\n",
      "‚ö†Ô∏è  Failed tickers: ['MMM', 'AOS', 'ABT', 'ABBV', 'ACN', 'ADBE', 'AMD', 'AES', 'AFL', 'A']...\n",
      "\n",
      "üìä Total unique tickers: 503\n",
      "\n",
      "======================================================================\n",
      "‚úÖ DOWNLOAD COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "Files saved to: c:\\Users\\Ferhat\\Documents\\GitHub\\Stocks\\output\n",
      "\n",
      "File structure:\n",
      "‚îú‚îÄ‚îÄ SP500/\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ sp500_constituents.csv\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ sp500_close.csv\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ sp500_adj_close.csv\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ sp500_volume.csv\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ sp500_daily_returns.csv\n",
      "‚îÇ   ‚îî‚îÄ‚îÄ sp500_cumulative_returns.csv\n",
      "‚îú‚îÄ‚îÄ NASDAQ/\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ nasdaq100_constituents.csv\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ nasdaq_close.csv\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ nasdaq_adj_close.csv\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ nasdaq_volume.csv\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ nasdaq_daily_returns.csv\n",
      "‚îÇ   ‚îî‚îÄ‚îÄ nasdaq_cumulative_returns.csv\n",
      "‚îî‚îÄ‚îÄ Combined/\n",
      "    ‚îî‚îÄ‚îÄ all_tickers.csv\n",
      "\n",
      "üîó You can now access these files through Claude via Google Drive!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STARTING DATA DOWNLOAD\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get tickers\n",
    "sp500_tickers, sp500_info = get_sp500_tickers()\n",
    "nasdaq_tickers, nasdaq_info = get_nasdaq_tickers()\n",
    "\n",
    "# Save ticker lists\n",
    "if not sp500_info.empty:\n",
    "    sp500_info.to_csv(f'{BASE_PATH}/SP500/sp500_constituents.csv', index=False)\n",
    "    print(f\"üíæ Saved S&P 500 constituent info\")\n",
    "\n",
    "if not nasdaq_info.empty:\n",
    "    nasdaq_info.to_csv(f'{BASE_PATH}/NASDAQ/nasdaq100_constituents.csv', index=False)\n",
    "    print(f\"üíæ Saved NASDAQ-100 constituent info\")\n",
    "\n",
    "# Download S&P 500 data\n",
    "if sp500_tickers:\n",
    "    sp500_data, sp500_failed = batch_download(sp500_tickers, \"S&P 500\", START_DATE, END_DATE)\n",
    "\n",
    "    if sp500_data:\n",
    "        # Create combined DataFrames\n",
    "        sp500_combined = create_combined_dataframes(sp500_data)\n",
    "\n",
    "        # Save each type\n",
    "        for data_type, df in sp500_combined.items():\n",
    "            df.to_csv(f'{BASE_PATH}/SP500/sp500_{data_type.lower()}.csv')\n",
    "            print(f\"üíæ Saved S&P 500 {data_type}: {df.shape}\")\n",
    "\n",
    "        # Calculate and save returns\n",
    "        sp500_returns, sp500_cum_returns = calculate_metrics(sp500_combined['Adj_Close'])\n",
    "        sp500_returns.to_csv(f'{BASE_PATH}/SP500/sp500_daily_returns.csv')\n",
    "        sp500_cum_returns.to_csv(f'{BASE_PATH}/SP500/sp500_cumulative_returns.csv')\n",
    "        print(f\"üíæ Saved S&P 500 returns data\")\n",
    "\n",
    "# Download NASDAQ data\n",
    "if nasdaq_tickers:\n",
    "    nasdaq_data, nasdaq_failed = batch_download(nasdaq_tickers, \"NASDAQ-100\", START_DATE, END_DATE)\n",
    "\n",
    "    if nasdaq_data:\n",
    "        # Create combined DataFrames\n",
    "        nasdaq_combined = create_combined_dataframes(nasdaq_data)\n",
    "\n",
    "        # Save each type\n",
    "        for data_type, df in nasdaq_combined.items():\n",
    "            df.to_csv(f'{BASE_PATH}/NASDAQ/nasdaq_{data_type.lower()}.csv')\n",
    "            print(f\"üíæ Saved NASDAQ-100 {data_type}: {df.shape}\")\n",
    "\n",
    "        # Calculate and save returns\n",
    "        nasdaq_returns, nasdaq_cum_returns = calculate_metrics(nasdaq_combined['Adj_Close'])\n",
    "        nasdaq_returns.to_csv(f'{BASE_PATH}/NASDAQ/nasdaq_daily_returns.csv')\n",
    "        nasdaq_cum_returns.to_csv(f'{BASE_PATH}/NASDAQ/nasdaq_cumulative_returns.csv')\n",
    "        print(f\"üíæ Saved NASDAQ-100 returns data\")\n",
    "\n",
    "# Create combined universe (unique tickers from both)\n",
    "all_tickers = list(set(sp500_tickers + nasdaq_tickers))\n",
    "print(f\"\\nüìä Total unique tickers: {len(all_tickers)}\")\n",
    "\n",
    "# Save combined ticker list\n",
    "pd.DataFrame({\n",
    "    'Ticker': all_tickers,\n",
    "    'In_SP500': [t in sp500_tickers for t in all_tickers],\n",
    "    'In_NASDAQ100': [t in nasdaq_tickers for t in all_tickers]\n",
    "}).to_csv(f'{BASE_PATH}/Combined/all_tickers.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ DOWNLOAD COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nFiles saved to: {BASE_PATH}\")\n",
    "print(\"\\nFile structure:\")\n",
    "print(\"‚îú‚îÄ‚îÄ SP500/\")\n",
    "print(\"‚îÇ   ‚îú‚îÄ‚îÄ sp500_constituents.csv\")\n",
    "print(\"‚îÇ   ‚îú‚îÄ‚îÄ sp500_close.csv\")\n",
    "print(\"‚îÇ   ‚îú‚îÄ‚îÄ sp500_adj_close.csv\")\n",
    "print(\"‚îÇ   ‚îú‚îÄ‚îÄ sp500_volume.csv\")\n",
    "print(\"‚îÇ   ‚îú‚îÄ‚îÄ sp500_daily_returns.csv\")\n",
    "print(\"‚îÇ   ‚îî‚îÄ‚îÄ sp500_cumulative_returns.csv\")\n",
    "print(\"‚îú‚îÄ‚îÄ NASDAQ/\")\n",
    "print(\"‚îÇ   ‚îú‚îÄ‚îÄ nasdaq100_constituents.csv\")\n",
    "print(\"‚îÇ   ‚îú‚îÄ‚îÄ nasdaq_close.csv\")\n",
    "print(\"‚îÇ   ‚îú‚îÄ‚îÄ nasdaq_adj_close.csv\")\n",
    "print(\"‚îÇ   ‚îú‚îÄ‚îÄ nasdaq_volume.csv\")\n",
    "print(\"‚îÇ   ‚îú‚îÄ‚îÄ nasdaq_daily_returns.csv\")\n",
    "print(\"‚îÇ   ‚îî‚îÄ‚îÄ nasdaq_cumulative_returns.csv\")\n",
    "print(\"‚îî‚îÄ‚îÄ Combined/\")\n",
    "print(\"    ‚îî‚îÄ‚îÄ all_tickers.csv\")\n",
    "print(\"\\nüîó You can now access these files through Claude via Google Drive!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SElxNLKL6pfW"
   },
   "outputs": [],
   "source": [
    "sp500_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nasdaq_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
